{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "print(\"TensorFlow has access to the following devices:\", tf.config.list_physical_devices())\n",
    "\n",
    "# See TensorFlow version\n",
    "print(\"TensorFlow version:\", tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "name of behaviour : 3DBall?team=0\n",
      "total reward for ep 0 is 1.1000000312924385\n",
      "total reward for ep 1 is 0.6000000238418579\n",
      "total reward for ep 2 is 0.40000002086162567\n",
      "total reward for ep 3 is 0.8000000268220901\n",
      "total reward for ep 4 is 1.500000037252903\n",
      "total reward for ep 5 is 2.0000000447034836\n",
      "total reward for ep 6 is 0.30000001937150955\n",
      "total reward for ep 7 is 1.500000037252903\n",
      "total reward for ep 8 is 1.4000000357627869\n",
      "total reward for ep 9 is 1.600000038743019\n"
     ]
    }
   ],
   "source": [
    "from mlagents_envs.environment import UnityEnvironment\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    env = UnityEnvironment(file_name = \"./Unity_practice/3DBall/3DBall_19\") # define env -> UnityEnvironment : main component that connect unity and python\n",
    "    env.reset() # initialization\n",
    "\n",
    "    # call behaviour\n",
    "    behaviour_name = list(env.behavior_specs.keys())[0]\n",
    "    print(f\"name of behaviour : {behaviour_name}\")\n",
    "    spec = env.behavior_specs[behaviour_name]\n",
    "\n",
    "    for ep in range(10):\n",
    "        env.reset()\n",
    "        # get_steps : return the information of agent in each step (reward, action ,state) \n",
    "        # decision_step : info about the step that request next action  \n",
    "        # terminal_step : info about the step end of the episode => identical to the first decision_steps next episode\n",
    "        decision_steps, terminal_steps = env.get_steps(behaviour_name)\n",
    "\n",
    "        # arbitrary target agent\n",
    "        tracked_agent = -1\n",
    "        done = False\n",
    "        ep_rewards = 0\n",
    "\n",
    "        while not done:\n",
    "            if tracked_agent == -1 and len(decision_steps)>=1:\n",
    "                tracked_agent = decision_steps.agent_id[0]\n",
    "\n",
    "            action = spec.action_spec.random_action(len(decision_steps)) # info of agent's action(randomly selected here)\n",
    "\n",
    "            env.set_actions(behaviour_name, action) # defince the behaviour of agent group whose behaviour name is same as action\n",
    "\n",
    "            env.step() # do one step\n",
    "\n",
    "            # after the step is done, get the agent's info\n",
    "            decision_steps, terminal_steps = env.get_steps(behaviour_name)\n",
    "\n",
    "            # store the reward\n",
    "            if tracked_agent in decision_steps:\n",
    "                ep_rewards += decision_steps[tracked_agent].reward\n",
    "            if tracked_agent in terminal_steps:\n",
    "                ep_rewards += terminal_steps[tracked_agent].reward\n",
    "                done = True\n",
    "\n",
    "        print(f\"total reward for ep {ep} is {ep_rewards}\")\n",
    "    env.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN Training\n",
    "======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeongminlee/miniforge3/envs/tf29_py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/jeongminlee/miniforge3/envs/tf29_py37/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Episode / Step: 299 / Score: -0.69 / Loss: nan / Epsilon: 1.0000\n",
      "20 Episode / Step: 513 / Score: -0.20 / Loss: nan / Epsilon: 1.0000\n",
      "30 Episode / Step: 609 / Score: -0.29 / Loss: nan / Epsilon: 1.0000\n",
      "40 Episode / Step: 941 / Score: 0.18 / Loss: nan / Epsilon: 1.0000\n",
      "50 Episode / Step: 1156 / Score: -0.20 / Loss: nan / Epsilon: 1.0000\n",
      "60 Episode / Step: 1467 / Score: 0.30 / Loss: nan / Epsilon: 1.0000\n",
      "70 Episode / Step: 1597 / Score: -0.32 / Loss: nan / Epsilon: 1.0000\n",
      "80 Episode / Step: 1829 / Score: -0.12 / Loss: nan / Epsilon: 1.0000\n",
      "90 Episode / Step: 2050 / Score: -0.41 / Loss: nan / Epsilon: 1.0000\n",
      "100 Episode / Step: 2341 / Score: 0.12 / Loss: nan / Epsilon: 1.0000\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "110 Episode / Step: 2566 / Score: -0.01 / Loss: nan / Epsilon: 1.0000\n",
      "120 Episode / Step: 2763 / Score: -0.39 / Loss: nan / Epsilon: 1.0000\n",
      "130 Episode / Step: 2983 / Score: -0.21 / Loss: nan / Epsilon: 1.0000\n",
      "140 Episode / Step: 3162 / Score: -0.17 / Loss: nan / Epsilon: 1.0000\n",
      "150 Episode / Step: 3263 / Score: -0.29 / Loss: nan / Epsilon: 1.0000\n",
      "160 Episode / Step: 3551 / Score: -1.08 / Loss: nan / Epsilon: 1.0000\n",
      "170 Episode / Step: 3820 / Score: -0.46 / Loss: nan / Epsilon: 1.0000\n",
      "180 Episode / Step: 4090 / Score: -0.26 / Loss: nan / Epsilon: 1.0000\n",
      "190 Episode / Step: 4259 / Score: -0.16 / Loss: nan / Epsilon: 1.0000\n",
      "200 Episode / Step: 4763 / Score: -0.19 / Loss: nan / Epsilon: 1.0000\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W NNPACK.cpp:80] Could not initialize NNPACK! Reason: Unsupported hardware.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210 Episode / Step: 5028 / Score: 0.15 / Loss: 0.0181 / Epsilon: 0.9994\n",
      "220 Episode / Step: 5494 / Score: -0.46 / Loss: 0.0187 / Epsilon: 0.9889\n",
      "230 Episode / Step: 5669 / Score: -0.16 / Loss: 0.0221 / Epsilon: 0.9849\n",
      "240 Episode / Step: 5829 / Score: -0.55 / Loss: 0.0195 / Epsilon: 0.9813\n",
      "250 Episode / Step: 6035 / Score: -0.40 / Loss: 0.0193 / Epsilon: 0.9767\n",
      "260 Episode / Step: 6335 / Score: -0.29 / Loss: 0.0202 / Epsilon: 0.9700\n",
      "270 Episode / Step: 6751 / Score: -0.11 / Loss: 0.0179 / Epsilon: 0.9606\n",
      "280 Episode / Step: 7028 / Score: -0.27 / Loss: 0.0182 / Epsilon: 0.9544\n",
      "290 Episode / Step: 7252 / Score: 0.19 / Loss: 0.0170 / Epsilon: 0.9493\n",
      "300 Episode / Step: 7618 / Score: -0.06 / Loss: 0.0154 / Epsilon: 0.9411\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "310 Episode / Step: 7953 / Score: 0.28 / Loss: 0.0116 / Epsilon: 0.9336\n",
      "320 Episode / Step: 8121 / Score: -0.16 / Loss: 0.0122 / Epsilon: 0.9298\n",
      "330 Episode / Step: 8280 / Score: -0.55 / Loss: 0.0124 / Epsilon: 0.9262\n",
      "340 Episode / Step: 8559 / Score: -0.27 / Loss: 0.0110 / Epsilon: 0.9199\n",
      "350 Episode / Step: 8732 / Score: 0.04 / Loss: 0.0101 / Epsilon: 0.9160\n",
      "360 Episode / Step: 9133 / Score: -1.19 / Loss: 0.0084 / Epsilon: 0.9070\n",
      "370 Episode / Step: 9476 / Score: -0.33 / Loss: 0.0077 / Epsilon: 0.8993\n",
      "380 Episode / Step: 9727 / Score: -0.14 / Loss: 0.0068 / Epsilon: 0.8936\n",
      "390 Episode / Step: 10102 / Score: -0.47 / Loss: 0.0061 / Epsilon: 0.8852\n",
      "400 Episode / Step: 10355 / Score: -0.44 / Loss: 0.0052 / Epsilon: 0.8795\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "410 Episode / Step: 10525 / Score: -0.36 / Loss: 0.0052 / Epsilon: 0.8757\n",
      "420 Episode / Step: 10787 / Score: -0.05 / Loss: 0.0056 / Epsilon: 0.8698\n",
      "430 Episode / Step: 11024 / Score: -0.13 / Loss: 0.0039 / Epsilon: 0.8645\n",
      "440 Episode / Step: 11280 / Score: 0.15 / Loss: 0.0043 / Epsilon: 0.8587\n",
      "450 Episode / Step: 11486 / Score: 0.10 / Loss: 0.0035 / Epsilon: 0.8541\n",
      "460 Episode / Step: 11683 / Score: 0.01 / Loss: 0.0043 / Epsilon: 0.8496\n",
      "470 Episode / Step: 12172 / Score: -0.28 / Loss: 0.0034 / Epsilon: 0.8386\n",
      "480 Episode / Step: 12395 / Score: -0.11 / Loss: 0.0029 / Epsilon: 0.8336\n",
      "490 Episode / Step: 12720 / Score: -0.12 / Loss: 0.0028 / Epsilon: 0.8263\n",
      "500 Episode / Step: 12849 / Score: 0.08 / Loss: 0.0021 / Epsilon: 0.8234\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "510 Episode / Step: 13071 / Score: 0.19 / Loss: 0.0031 / Epsilon: 0.8184\n",
      "520 Episode / Step: 13299 / Score: -0.02 / Loss: 0.0026 / Epsilon: 0.8133\n",
      "530 Episode / Step: 13497 / Score: 0.11 / Loss: 0.0021 / Epsilon: 0.8088\n",
      "540 Episode / Step: 13737 / Score: 0.37 / Loss: 0.0023 / Epsilon: 0.8034\n",
      "550 Episode / Step: 14068 / Score: 0.28 / Loss: 0.0018 / Epsilon: 0.7960\n",
      "560 Episode / Step: 14277 / Score: 0.40 / Loss: 0.0024 / Epsilon: 0.7913\n",
      "570 Episode / Step: 14526 / Score: 0.16 / Loss: 0.0019 / Epsilon: 0.7857\n",
      "580 Episode / Step: 14800 / Score: -0.36 / Loss: 0.0020 / Epsilon: 0.7795\n",
      "590 Episode / Step: 14956 / Score: 0.05 / Loss: 0.0015 / Epsilon: 0.7760\n",
      "600 Episode / Step: 15218 / Score: 0.05 / Loss: 0.0022 / Epsilon: 0.7701\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "610 Episode / Step: 15391 / Score: 0.34 / Loss: 0.0017 / Epsilon: 0.7662\n",
      "620 Episode / Step: 15642 / Score: 0.06 / Loss: 0.0019 / Epsilon: 0.7606\n",
      "630 Episode / Step: 15806 / Score: 0.05 / Loss: 0.0016 / Epsilon: 0.7569\n",
      "640 Episode / Step: 15965 / Score: -0.15 / Loss: 0.0021 / Epsilon: 0.7533\n",
      "650 Episode / Step: 16063 / Score: 0.31 / Loss: 0.0021 / Epsilon: 0.7511\n",
      "660 Episode / Step: 16195 / Score: 0.28 / Loss: 0.0019 / Epsilon: 0.7481\n",
      "670 Episode / Step: 16568 / Score: 0.34 / Loss: 0.0019 / Epsilon: 0.7397\n",
      "680 Episode / Step: 16875 / Score: 0.40 / Loss: 0.0017 / Epsilon: 0.7328\n",
      "690 Episode / Step: 17020 / Score: 0.67 / Loss: 0.0021 / Epsilon: 0.7295\n",
      "700 Episode / Step: 17213 / Score: 0.02 / Loss: 0.0019 / Epsilon: 0.7252\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "710 Episode / Step: 17401 / Score: 0.22 / Loss: 0.0016 / Epsilon: 0.7210\n",
      "720 Episode / Step: 17563 / Score: 0.25 / Loss: 0.0018 / Epsilon: 0.7173\n",
      "730 Episode / Step: 17642 / Score: 0.33 / Loss: 0.0021 / Epsilon: 0.7156\n",
      "740 Episode / Step: 17925 / Score: 0.53 / Loss: 0.0019 / Epsilon: 0.7092\n",
      "750 Episode / Step: 18152 / Score: 0.48 / Loss: 0.0018 / Epsilon: 0.7041\n",
      "760 Episode / Step: 18328 / Score: 0.43 / Loss: 0.0015 / Epsilon: 0.7001\n",
      "770 Episode / Step: 18447 / Score: 0.69 / Loss: 0.0014 / Epsilon: 0.6974\n",
      "780 Episode / Step: 18640 / Score: 0.32 / Loss: 0.0016 / Epsilon: 0.6931\n",
      "790 Episode / Step: 18734 / Score: 0.32 / Loss: 0.0018 / Epsilon: 0.6910\n",
      "800 Episode / Step: 18852 / Score: 0.69 / Loss: 0.0020 / Epsilon: 0.6883\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "810 Episode / Step: 18917 / Score: 0.75 / Loss: 0.0013 / Epsilon: 0.6869\n",
      "820 Episode / Step: 19143 / Score: 0.18 / Loss: 0.0015 / Epsilon: 0.6818\n",
      "830 Episode / Step: 19382 / Score: -0.03 / Loss: 0.0016 / Epsilon: 0.6764\n",
      "840 Episode / Step: 19547 / Score: 0.85 / Loss: 0.0012 / Epsilon: 0.6727\n",
      "850 Episode / Step: 19629 / Score: 0.53 / Loss: 0.0016 / Epsilon: 0.6708\n",
      "860 Episode / Step: 19967 / Score: 0.57 / Loss: 0.0013 / Epsilon: 0.6632\n",
      "870 Episode / Step: 20053 / Score: 0.12 / Loss: 0.0015 / Epsilon: 0.6613\n",
      "880 Episode / Step: 20186 / Score: 0.08 / Loss: 0.0018 / Epsilon: 0.6583\n",
      "890 Episode / Step: 20358 / Score: 0.44 / Loss: 0.0013 / Epsilon: 0.6544\n",
      "900 Episode / Step: 20524 / Score: 0.24 / Loss: 0.0013 / Epsilon: 0.6507\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "910 Episode / Step: 20746 / Score: 0.09 / Loss: 0.0015 / Epsilon: 0.6457\n",
      "920 Episode / Step: 20897 / Score: 0.46 / Loss: 0.0014 / Epsilon: 0.6423\n",
      "930 Episode / Step: 21050 / Score: 0.86 / Loss: 0.0013 / Epsilon: 0.6389\n",
      "940 Episode / Step: 21249 / Score: 0.01 / Loss: 0.0013 / Epsilon: 0.6344\n",
      "950 Episode / Step: 21327 / Score: 0.93 / Loss: 0.0012 / Epsilon: 0.6326\n",
      "960 Episode / Step: 21454 / Score: 0.48 / Loss: 0.0011 / Epsilon: 0.6298\n",
      "970 Episode / Step: 21535 / Score: 0.73 / Loss: 0.0017 / Epsilon: 0.6280\n",
      "980 Episode / Step: 21617 / Score: 0.53 / Loss: 0.0015 / Epsilon: 0.6261\n",
      "990 Episode / Step: 21735 / Score: 0.69 / Loss: 0.0016 / Epsilon: 0.6235\n",
      "1000 Episode / Step: 21837 / Score: 0.91 / Loss: 0.0011 / Epsilon: 0.6212\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "1010 Episode / Step: 21996 / Score: 0.25 / Loss: 0.0011 / Epsilon: 0.6176\n",
      "1020 Episode / Step: 22077 / Score: 0.73 / Loss: 0.0020 / Epsilon: 0.6158\n",
      "1030 Episode / Step: 22238 / Score: 0.65 / Loss: 0.0013 / Epsilon: 0.6121\n",
      "1040 Episode / Step: 22497 / Score: -0.15 / Loss: 0.0010 / Epsilon: 0.6063\n",
      "1050 Episode / Step: 22590 / Score: 0.72 / Loss: 0.0017 / Epsilon: 0.6042\n",
      "1060 Episode / Step: 22712 / Score: 0.29 / Loss: 0.0013 / Epsilon: 0.6015\n",
      "1070 Episode / Step: 22863 / Score: 0.26 / Loss: 0.0012 / Epsilon: 0.5981\n",
      "1080 Episode / Step: 23013 / Score: 0.06 / Loss: 0.0011 / Epsilon: 0.5947\n",
      "1090 Episode / Step: 23148 / Score: 0.68 / Loss: 0.0023 / Epsilon: 0.5917\n",
      "1100 Episode / Step: 23243 / Score: 0.92 / Loss: 0.0017 / Epsilon: 0.5895\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "1110 Episode / Step: 23316 / Score: 0.54 / Loss: 0.0012 / Epsilon: 0.5879\n",
      "1120 Episode / Step: 23519 / Score: 0.61 / Loss: 0.0009 / Epsilon: 0.5833\n",
      "1130 Episode / Step: 23595 / Score: 0.33 / Loss: 0.0013 / Epsilon: 0.5816\n",
      "1140 Episode / Step: 23662 / Score: 0.34 / Loss: 0.0011 / Epsilon: 0.5801\n",
      "1150 Episode / Step: 23773 / Score: 0.50 / Loss: 0.0025 / Epsilon: 0.5776\n",
      "1160 Episode / Step: 23922 / Score: 0.46 / Loss: 0.0012 / Epsilon: 0.5743\n",
      "1170 Episode / Step: 24008 / Score: 0.72 / Loss: 0.0013 / Epsilon: 0.5723\n",
      "1180 Episode / Step: 24108 / Score: 0.71 / Loss: 0.0018 / Epsilon: 0.5701\n",
      "1190 Episode / Step: 24241 / Score: 0.68 / Loss: 0.0017 / Epsilon: 0.5671\n",
      "1200 Episode / Step: 24355 / Score: 0.90 / Loss: 0.0012 / Epsilon: 0.5645\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "1210 Episode / Step: 24458 / Score: 0.31 / Loss: 0.0012 / Epsilon: 0.5622\n",
      "1220 Episode / Step: 24589 / Score: 0.68 / Loss: 0.0016 / Epsilon: 0.5592\n",
      "1230 Episode / Step: 24635 / Score: 0.56 / Loss: 0.0014 / Epsilon: 0.5582\n",
      "1240 Episode / Step: 24732 / Score: 0.71 / Loss: 0.0013 / Epsilon: 0.5560\n",
      "1250 Episode / Step: 24790 / Score: 0.75 / Loss: 0.0010 / Epsilon: 0.5547\n",
      "1260 Episode / Step: 24904 / Score: 0.30 / Loss: 0.0016 / Epsilon: 0.5522\n",
      "1270 Episode / Step: 25011 / Score: 0.70 / Loss: 0.0013 / Epsilon: 0.5498\n",
      "1280 Episode / Step: 25069 / Score: 0.75 / Loss: 0.0019 / Epsilon: 0.5484\n",
      "1290 Episode / Step: 25166 / Score: 0.51 / Loss: 0.0012 / Epsilon: 0.5463\n",
      "1300 Episode / Step: 25267 / Score: 0.51 / Loss: 0.0015 / Epsilon: 0.5440\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "1310 Episode / Step: 25367 / Score: 0.91 / Loss: 0.0015 / Epsilon: 0.5417\n",
      "1320 Episode / Step: 25432 / Score: 0.75 / Loss: 0.0011 / Epsilon: 0.5403\n",
      "1330 Episode / Step: 25546 / Score: 0.50 / Loss: 0.0013 / Epsilon: 0.5377\n",
      "1340 Episode / Step: 25608 / Score: 0.55 / Loss: 0.0017 / Epsilon: 0.5363\n",
      "1350 Episode / Step: 25731 / Score: 0.49 / Loss: 0.0017 / Epsilon: 0.5336\n",
      "1360 Episode / Step: 25792 / Score: 0.75 / Loss: 0.0012 / Epsilon: 0.5322\n",
      "1370 Episode / Step: 25971 / Score: 0.33 / Loss: 0.0011 / Epsilon: 0.5282\n",
      "1380 Episode / Step: 26049 / Score: 0.93 / Loss: 0.0012 / Epsilon: 0.5264\n",
      "1390 Episode / Step: 26126 / Score: -0.07 / Loss: 0.0010 / Epsilon: 0.5247\n",
      "1400 Episode / Step: 26227 / Score: 0.91 / Loss: 0.0013 / Epsilon: 0.5224\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "1410 Episode / Step: 26359 / Score: -0.12 / Loss: 0.0014 / Epsilon: 0.5194\n",
      "1420 Episode / Step: 26420 / Score: 0.15 / Loss: 0.0016 / Epsilon: 0.5180\n",
      "1430 Episode / Step: 26497 / Score: 0.93 / Loss: 0.0022 / Epsilon: 0.5163\n",
      "1440 Episode / Step: 26565 / Score: 0.34 / Loss: 0.0022 / Epsilon: 0.5148\n",
      "1450 Episode / Step: 26618 / Score: 0.76 / Loss: 0.0011 / Epsilon: 0.5136\n",
      "1460 Episode / Step: 26700 / Score: 0.53 / Loss: 0.0014 / Epsilon: 0.5117\n",
      "1470 Episode / Step: 26825 / Score: 0.69 / Loss: 0.0024 / Epsilon: 0.5089\n",
      "1480 Episode / Step: 26896 / Score: 0.54 / Loss: 0.0016 / Epsilon: 0.5073\n",
      "1490 Episode / Step: 27012 / Score: 0.69 / Loss: 0.0017 / Epsilon: 0.5047\n",
      "1500 Episode / Step: 27116 / Score: 0.91 / Loss: 0.0015 / Epsilon: 0.5024\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "1510 Episode / Step: 27173 / Score: 0.75 / Loss: 0.0014 / Epsilon: 0.5011\n",
      "1520 Episode / Step: 27222 / Score: 0.76 / Loss: 0.0009 / Epsilon: 0.5000\n",
      "1530 Episode / Step: 27335 / Score: 0.90 / Loss: 0.0011 / Epsilon: 0.4975\n",
      "1540 Episode / Step: 27401 / Score: 0.74 / Loss: 0.0014 / Epsilon: 0.4960\n",
      "1550 Episode / Step: 27444 / Score: 0.77 / Loss: 0.0015 / Epsilon: 0.4950\n",
      "1560 Episode / Step: 27522 / Score: 0.73 / Loss: 0.0011 / Epsilon: 0.4933\n",
      "1570 Episode / Step: 27644 / Score: 0.69 / Loss: 0.0015 / Epsilon: 0.4905\n",
      "1580 Episode / Step: 27693 / Score: 0.76 / Loss: 0.0012 / Epsilon: 0.4894\n",
      "1590 Episode / Step: 27758 / Score: 0.95 / Loss: 0.0014 / Epsilon: 0.4879\n",
      "1600 Episode / Step: 27890 / Score: 0.68 / Loss: 0.0015 / Epsilon: 0.4850\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "1610 Episode / Step: 27995 / Score: 0.51 / Loss: 0.0016 / Epsilon: 0.4826\n",
      "1620 Episode / Step: 28143 / Score: 0.36 / Loss: 0.0015 / Epsilon: 0.4793\n",
      "1630 Episode / Step: 28274 / Score: 0.48 / Loss: 0.0011 / Epsilon: 0.4763\n",
      "1640 Episode / Step: 28387 / Score: 0.70 / Loss: 0.0012 / Epsilon: 0.4738\n",
      "1650 Episode / Step: 28468 / Score: 0.53 / Loss: 0.0008 / Epsilon: 0.4720\n",
      "1660 Episode / Step: 28556 / Score: 0.32 / Loss: 0.0017 / Epsilon: 0.4700\n",
      "1670 Episode / Step: 28716 / Score: 0.25 / Loss: 0.0016 / Epsilon: 0.4664\n",
      "1680 Episode / Step: 28808 / Score: 0.92 / Loss: 0.0010 / Epsilon: 0.4643\n",
      "1690 Episode / Step: 28864 / Score: 0.55 / Loss: 0.0008 / Epsilon: 0.4631\n",
      "1700 Episode / Step: 28943 / Score: 0.73 / Loss: 0.0014 / Epsilon: 0.4613\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "1710 Episode / Step: 29024 / Score: 0.73 / Loss: 0.0008 / Epsilon: 0.4595\n",
      "1720 Episode / Step: 29078 / Score: 0.96 / Loss: 0.0014 / Epsilon: 0.4582\n",
      "1730 Episode / Step: 29137 / Score: 0.75 / Loss: 0.0010 / Epsilon: 0.4569\n",
      "1740 Episode / Step: 29207 / Score: 0.94 / Loss: 0.0011 / Epsilon: 0.4553\n",
      "1750 Episode / Step: 29336 / Score: 0.88 / Loss: 0.0010 / Epsilon: 0.4524\n",
      "1760 Episode / Step: 29422 / Score: 0.72 / Loss: 0.0012 / Epsilon: 0.4505\n",
      "1770 Episode / Step: 29493 / Score: 0.74 / Loss: 0.0007 / Epsilon: 0.4489\n",
      "1780 Episode / Step: 29605 / Score: 0.70 / Loss: 0.0010 / Epsilon: 0.4464\n",
      "1790 Episode / Step: 29706 / Score: 0.91 / Loss: 0.0010 / Epsilon: 0.4441\n",
      "1800 Episode / Step: 29777 / Score: 0.74 / Loss: 0.0012 / Epsilon: 0.4425\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "1810 Episode / Step: 29852 / Score: 0.74 / Loss: 0.0007 / Epsilon: 0.4408\n",
      "1820 Episode / Step: 29907 / Score: 0.96 / Loss: 0.0008 / Epsilon: 0.4396\n",
      "1830 Episode / Step: 30027 / Score: 0.49 / Loss: 0.0012 / Epsilon: 0.4369\n",
      "1840 Episode / Step: 30085 / Score: 0.75 / Loss: 0.0011 / Epsilon: 0.4356\n",
      "1850 Episode / Step: 30222 / Score: 0.67 / Loss: 0.0009 / Epsilon: 0.4325\n",
      "1860 Episode / Step: 30298 / Score: 0.73 / Loss: 0.0009 / Epsilon: 0.4308\n",
      "1870 Episode / Step: 30378 / Score: 0.93 / Loss: 0.0010 / Epsilon: 0.4290\n",
      "1880 Episode / Step: 30441 / Score: 0.95 / Loss: 0.0007 / Epsilon: 0.4276\n",
      "1890 Episode / Step: 30508 / Score: 0.74 / Loss: 0.0007 / Epsilon: 0.4261\n",
      "1900 Episode / Step: 30605 / Score: 0.71 / Loss: 0.0011 / Epsilon: 0.4239\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "1910 Episode / Step: 30673 / Score: 0.34 / Loss: 0.0010 / Epsilon: 0.4224\n",
      "1920 Episode / Step: 30739 / Score: 0.94 / Loss: 0.0011 / Epsilon: 0.4209\n",
      "1930 Episode / Step: 30804 / Score: 0.95 / Loss: 0.0010 / Epsilon: 0.4194\n",
      "1940 Episode / Step: 30919 / Score: 0.50 / Loss: 0.0008 / Epsilon: 0.4168\n",
      "1950 Episode / Step: 31047 / Score: 0.68 / Loss: 0.0017 / Epsilon: 0.4139\n",
      "1960 Episode / Step: 31087 / Score: 0.97 / Loss: 0.0011 / Epsilon: 0.4130\n",
      "1970 Episode / Step: 31120 / Score: 0.78 / Loss: 0.0015 / Epsilon: 0.4123\n",
      "1980 Episode / Step: 31181 / Score: 0.75 / Loss: 0.0010 / Epsilon: 0.4109\n",
      "1990 Episode / Step: 31229 / Score: 0.76 / Loss: 0.0007 / Epsilon: 0.4098\n",
      "2000 Episode / Step: 31299 / Score: 0.74 / Loss: 0.0011 / Epsilon: 0.4083\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "2010 Episode / Step: 31345 / Score: 0.96 / Loss: 0.0015 / Epsilon: 0.4072\n",
      "2020 Episode / Step: 31404 / Score: 0.55 / Loss: 0.0008 / Epsilon: 0.4059\n",
      "2030 Episode / Step: 31472 / Score: 0.74 / Loss: 0.0009 / Epsilon: 0.4044\n",
      "2040 Episode / Step: 31593 / Score: 0.89 / Loss: 0.0009 / Epsilon: 0.4017\n",
      "2050 Episode / Step: 31659 / Score: 0.94 / Loss: 0.0010 / Epsilon: 0.4002\n",
      "2060 Episode / Step: 31695 / Score: 0.97 / Loss: 0.0015 / Epsilon: 0.3994\n",
      "2070 Episode / Step: 31781 / Score: 0.72 / Loss: 0.0008 / Epsilon: 0.3974\n",
      "2080 Episode / Step: 31844 / Score: 0.75 / Loss: 0.0013 / Epsilon: 0.3960\n",
      "2090 Episode / Step: 31959 / Score: 0.90 / Loss: 0.0011 / Epsilon: 0.3934\n",
      "2100 Episode / Step: 32026 / Score: 0.74 / Loss: 0.0012 / Epsilon: 0.3919\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "2110 Episode / Step: 32077 / Score: 0.96 / Loss: 0.0010 / Epsilon: 0.3908\n",
      "2120 Episode / Step: 32163 / Score: 0.32 / Loss: 0.0014 / Epsilon: 0.3888\n",
      "2130 Episode / Step: 32241 / Score: 0.73 / Loss: 0.0008 / Epsilon: 0.3871\n",
      "2140 Episode / Step: 32336 / Score: 0.72 / Loss: 0.0012 / Epsilon: 0.3849\n",
      "2150 Episode / Step: 32434 / Score: 0.91 / Loss: 0.0008 / Epsilon: 0.3827\n",
      "2160 Episode / Step: 32504 / Score: 0.74 / Loss: 0.0007 / Epsilon: 0.3812\n",
      "2170 Episode / Step: 32549 / Score: 0.77 / Loss: 0.0010 / Epsilon: 0.3801\n",
      "2180 Episode / Step: 32591 / Score: 0.57 / Loss: 0.0008 / Epsilon: 0.3792\n",
      "2190 Episode / Step: 32662 / Score: 0.74 / Loss: 0.0017 / Epsilon: 0.3776\n",
      "2200 Episode / Step: 32735 / Score: 0.94 / Loss: 0.0009 / Epsilon: 0.3760\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "2210 Episode / Step: 32819 / Score: 0.73 / Loss: 0.0010 / Epsilon: 0.3741\n",
      "2220 Episode / Step: 32912 / Score: 0.92 / Loss: 0.0007 / Epsilon: 0.3720\n",
      "2230 Episode / Step: 32960 / Score: 0.36 / Loss: 0.0007 / Epsilon: 0.3709\n",
      "2240 Episode / Step: 33013 / Score: 0.76 / Loss: 0.0009 / Epsilon: 0.3697\n",
      "2250 Episode / Step: 33074 / Score: 0.55 / Loss: 0.0011 / Epsilon: 0.3683\n",
      "2260 Episode / Step: 33179 / Score: 0.51 / Loss: 0.0010 / Epsilon: 0.3660\n",
      "2270 Episode / Step: 33253 / Score: 0.74 / Loss: 0.0019 / Epsilon: 0.3643\n",
      "2280 Episode / Step: 33365 / Score: 0.90 / Loss: 0.0008 / Epsilon: 0.3618\n",
      "2290 Episode / Step: 33414 / Score: 0.96 / Loss: 0.0009 / Epsilon: 0.3607\n",
      "2300 Episode / Step: 33490 / Score: 0.93 / Loss: 0.0007 / Epsilon: 0.3590\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "2310 Episode / Step: 33550 / Score: 0.95 / Loss: 0.0009 / Epsilon: 0.3576\n",
      "2320 Episode / Step: 33619 / Score: 0.54 / Loss: 0.0008 / Epsilon: 0.3561\n",
      "2330 Episode / Step: 33660 / Score: 0.97 / Loss: 0.0007 / Epsilon: 0.3551\n",
      "2340 Episode / Step: 33707 / Score: 0.96 / Loss: 0.0007 / Epsilon: 0.3541\n",
      "2350 Episode / Step: 33793 / Score: 0.52 / Loss: 0.0008 / Epsilon: 0.3522\n",
      "2360 Episode / Step: 33847 / Score: 0.76 / Loss: 0.0007 / Epsilon: 0.3509\n",
      "2370 Episode / Step: 33903 / Score: 0.55 / Loss: 0.0008 / Epsilon: 0.3497\n",
      "2380 Episode / Step: 33932 / Score: 0.78 / Loss: 0.0008 / Epsilon: 0.3490\n",
      "2390 Episode / Step: 33976 / Score: 0.97 / Loss: 0.0007 / Epsilon: 0.3480\n",
      "2400 Episode / Step: 34033 / Score: 0.75 / Loss: 0.0008 / Epsilon: 0.3468\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "2410 Episode / Step: 34114 / Score: 0.53 / Loss: 0.0008 / Epsilon: 0.3449\n",
      "2420 Episode / Step: 34177 / Score: 0.95 / Loss: 0.0008 / Epsilon: 0.3435\n",
      "2430 Episode / Step: 34235 / Score: 0.75 / Loss: 0.0008 / Epsilon: 0.3422\n",
      "2440 Episode / Step: 34281 / Score: 0.56 / Loss: 0.0006 / Epsilon: 0.3412\n",
      "2450 Episode / Step: 34322 / Score: 0.97 / Loss: 0.0008 / Epsilon: 0.3403\n",
      "2460 Episode / Step: 34414 / Score: 0.72 / Loss: 0.0007 / Epsilon: 0.3382\n",
      "2470 Episode / Step: 34486 / Score: 0.94 / Loss: 0.0007 / Epsilon: 0.3366\n",
      "2480 Episode / Step: 34559 / Score: 0.94 / Loss: 0.0008 / Epsilon: 0.3349\n",
      "2490 Episode / Step: 34626 / Score: 0.54 / Loss: 0.0010 / Epsilon: 0.3334\n",
      "2500 Episode / Step: 34683 / Score: 0.95 / Loss: 0.0012 / Epsilon: 0.3321\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "2510 Episode / Step: 34730 / Score: 0.76 / Loss: 0.0008 / Epsilon: 0.3311\n",
      "2520 Episode / Step: 34790 / Score: 0.95 / Loss: 0.0012 / Epsilon: 0.3297\n",
      "2530 Episode / Step: 34854 / Score: 0.75 / Loss: 0.0007 / Epsilon: 0.3283\n",
      "2540 Episode / Step: 34892 / Score: 0.77 / Loss: 0.0008 / Epsilon: 0.3274\n",
      "2550 Episode / Step: 34940 / Score: 0.76 / Loss: 0.0008 / Epsilon: 0.3263\n",
      "2560 Episode / Step: 35006 / Score: 0.94 / Loss: 0.0011 / Epsilon: 0.3249\n",
      "2570 Episode / Step: 35082 / Score: 0.73 / Loss: 0.0009 / Epsilon: 0.3232\n",
      "2580 Episode / Step: 35135 / Score: 0.96 / Loss: 0.0007 / Epsilon: 0.3220\n",
      "2590 Episode / Step: 35210 / Score: 0.74 / Loss: 0.0007 / Epsilon: 0.3203\n",
      "2600 Episode / Step: 35254 / Score: 0.97 / Loss: 0.0008 / Epsilon: 0.3193\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "2610 Episode / Step: 35312 / Score: 0.75 / Loss: 0.0008 / Epsilon: 0.3180\n",
      "2620 Episode / Step: 35368 / Score: 0.95 / Loss: 0.0008 / Epsilon: 0.3167\n",
      "2630 Episode / Step: 35441 / Score: 0.94 / Loss: 0.0005 / Epsilon: 0.3151\n",
      "2640 Episode / Step: 35486 / Score: 0.77 / Loss: 0.0009 / Epsilon: 0.3141\n",
      "2650 Episode / Step: 35542 / Score: 0.95 / Loss: 0.0007 / Epsilon: 0.3128\n",
      "2660 Episode / Step: 35592 / Score: 0.76 / Loss: 0.0014 / Epsilon: 0.3117\n",
      "2670 Episode / Step: 35654 / Score: 0.55 / Loss: 0.0007 / Epsilon: 0.3103\n",
      "2680 Episode / Step: 35708 / Score: 0.56 / Loss: 0.0007 / Epsilon: 0.3091\n",
      "2690 Episode / Step: 35785 / Score: 0.53 / Loss: 0.0006 / Epsilon: 0.3073\n",
      "2700 Episode / Step: 35924 / Score: 0.47 / Loss: 0.0009 / Epsilon: 0.3042\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "2710 Episode / Step: 35988 / Score: 0.75 / Loss: 0.0006 / Epsilon: 0.3028\n",
      "2720 Episode / Step: 36035 / Score: 0.76 / Loss: 0.0006 / Epsilon: 0.3017\n",
      "2730 Episode / Step: 36158 / Score: 0.69 / Loss: 0.0009 / Epsilon: 0.2989\n",
      "2740 Episode / Step: 36226 / Score: 0.54 / Loss: 0.0007 / Epsilon: 0.2974\n",
      "2750 Episode / Step: 36285 / Score: 0.75 / Loss: 0.0012 / Epsilon: 0.2961\n",
      "2760 Episode / Step: 36353 / Score: 0.94 / Loss: 0.0007 / Epsilon: 0.2946\n",
      "2770 Episode / Step: 36388 / Score: 0.78 / Loss: 0.0007 / Epsilon: 0.2938\n",
      "2780 Episode / Step: 36441 / Score: 0.76 / Loss: 0.0006 / Epsilon: 0.2926\n",
      "2790 Episode / Step: 36509 / Score: 0.54 / Loss: 0.0010 / Epsilon: 0.2910\n",
      "2800 Episode / Step: 36556 / Score: 0.96 / Loss: 0.0007 / Epsilon: 0.2900\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "2810 Episode / Step: 36611 / Score: 0.96 / Loss: 0.0007 / Epsilon: 0.2888\n",
      "2820 Episode / Step: 36695 / Score: 0.73 / Loss: 0.0006 / Epsilon: 0.2869\n",
      "2830 Episode / Step: 36749 / Score: 0.96 / Loss: 0.0011 / Epsilon: 0.2856\n",
      "2840 Episode / Step: 36830 / Score: 0.53 / Loss: 0.0010 / Epsilon: 0.2838\n",
      "2850 Episode / Step: 36874 / Score: 0.97 / Loss: 0.0013 / Epsilon: 0.2828\n",
      "2860 Episode / Step: 36932 / Score: 0.95 / Loss: 0.0006 / Epsilon: 0.2815\n",
      "2870 Episode / Step: 36996 / Score: 0.75 / Loss: 0.0005 / Epsilon: 0.2801\n",
      "2880 Episode / Step: 37042 / Score: 0.96 / Loss: 0.0008 / Epsilon: 0.2791\n",
      "2890 Episode / Step: 37098 / Score: 0.55 / Loss: 0.0014 / Epsilon: 0.2778\n",
      "2900 Episode / Step: 37140 / Score: 0.97 / Loss: 0.0006 / Epsilon: 0.2768\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "2910 Episode / Step: 37183 / Score: 0.97 / Loss: 0.0009 / Epsilon: 0.2759\n",
      "2920 Episode / Step: 37241 / Score: 0.75 / Loss: 0.0010 / Epsilon: 0.2746\n",
      "2930 Episode / Step: 37289 / Score: 0.76 / Loss: 0.0012 / Epsilon: 0.2735\n",
      "2940 Episode / Step: 37326 / Score: 0.97 / Loss: 0.0008 / Epsilon: 0.2727\n",
      "2950 Episode / Step: 37405 / Score: 0.73 / Loss: 0.0009 / Epsilon: 0.2709\n",
      "2960 Episode / Step: 37446 / Score: 0.97 / Loss: 0.0008 / Epsilon: 0.2700\n",
      "2970 Episode / Step: 37509 / Score: 0.75 / Loss: 0.0011 / Epsilon: 0.2685\n",
      "2980 Episode / Step: 37565 / Score: 0.75 / Loss: 0.0007 / Epsilon: 0.2673\n",
      "2990 Episode / Step: 37610 / Score: 0.57 / Loss: 0.0006 / Epsilon: 0.2663\n",
      "3000 Episode / Step: 37664 / Score: 0.56 / Loss: 0.0005 / Epsilon: 0.2651\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "3010 Episode / Step: 37732 / Score: 0.94 / Loss: 0.0008 / Epsilon: 0.2635\n",
      "3020 Episode / Step: 37771 / Score: 0.77 / Loss: 0.0013 / Epsilon: 0.2627\n",
      "3030 Episode / Step: 37807 / Score: 0.97 / Loss: 0.0005 / Epsilon: 0.2618\n",
      "3040 Episode / Step: 37851 / Score: 0.97 / Loss: 0.0005 / Epsilon: 0.2609\n",
      "3050 Episode / Step: 37902 / Score: 0.96 / Loss: 0.0009 / Epsilon: 0.2597\n",
      "3060 Episode / Step: 37942 / Score: 0.97 / Loss: 0.0011 / Epsilon: 0.2588\n",
      "3070 Episode / Step: 37973 / Score: 0.98 / Loss: 0.0006 / Epsilon: 0.2581\n",
      "3080 Episode / Step: 38033 / Score: 0.75 / Loss: 0.0010 / Epsilon: 0.2568\n",
      "3090 Episode / Step: 38079 / Score: 0.96 / Loss: 0.0008 / Epsilon: 0.2557\n",
      "3100 Episode / Step: 38129 / Score: 0.96 / Loss: 0.0007 / Epsilon: 0.2546\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "3110 Episode / Step: 38164 / Score: 0.78 / Loss: 0.0007 / Epsilon: 0.2538\n",
      "3120 Episode / Step: 38211 / Score: 0.76 / Loss: 0.0007 / Epsilon: 0.2528\n",
      "3130 Episode / Step: 38248 / Score: 0.97 / Loss: 0.0006 / Epsilon: 0.2519\n",
      "3140 Episode / Step: 38291 / Score: 0.77 / Loss: 0.0008 / Epsilon: 0.2510\n",
      "3150 Episode / Step: 38345 / Score: 0.76 / Loss: 0.0006 / Epsilon: 0.2497\n",
      "3160 Episode / Step: 38393 / Score: 0.96 / Loss: 0.0006 / Epsilon: 0.2487\n",
      "3170 Episode / Step: 38451 / Score: 0.75 / Loss: 0.0005 / Epsilon: 0.2474\n",
      "3180 Episode / Step: 38524 / Score: 0.74 / Loss: 0.0005 / Epsilon: 0.2457\n",
      "3190 Episode / Step: 38569 / Score: 0.97 / Loss: 0.0006 / Epsilon: 0.2447\n",
      "3200 Episode / Step: 38615 / Score: 0.96 / Loss: 0.0006 / Epsilon: 0.2437\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "3210 Episode / Step: 38665 / Score: 0.76 / Loss: 0.0007 / Epsilon: 0.2425\n",
      "3220 Episode / Step: 38719 / Score: 0.96 / Loss: 0.0005 / Epsilon: 0.2413\n",
      "3230 Episode / Step: 38751 / Score: 0.98 / Loss: 0.0006 / Epsilon: 0.2406\n",
      "3240 Episode / Step: 38785 / Score: 0.98 / Loss: 0.0005 / Epsilon: 0.2398\n",
      "3250 Episode / Step: 38825 / Score: 0.77 / Loss: 0.0005 / Epsilon: 0.2389\n",
      "3260 Episode / Step: 38886 / Score: 0.95 / Loss: 0.0007 / Epsilon: 0.2376\n",
      "3270 Episode / Step: 38920 / Score: 0.98 / Loss: 0.0006 / Epsilon: 0.2368\n",
      "3280 Episode / Step: 38965 / Score: 0.97 / Loss: 0.0004 / Epsilon: 0.2358\n",
      "3290 Episode / Step: 39004 / Score: 0.97 / Loss: 0.0005 / Epsilon: 0.2349\n",
      "3300 Episode / Step: 39043 / Score: 0.97 / Loss: 0.0009 / Epsilon: 0.2340\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "3310 Episode / Step: 39096 / Score: 0.96 / Loss: 0.0009 / Epsilon: 0.2328\n",
      "3320 Episode / Step: 39141 / Score: 0.97 / Loss: 0.0008 / Epsilon: 0.2318\n",
      "3330 Episode / Step: 39177 / Score: 0.97 / Loss: 0.0006 / Epsilon: 0.2310\n",
      "3340 Episode / Step: 39220 / Score: 0.97 / Loss: 0.0005 / Epsilon: 0.2300\n",
      "3350 Episode / Step: 39255 / Score: 0.78 / Loss: 0.0004 / Epsilon: 0.2293\n",
      "3360 Episode / Step: 39307 / Score: 0.96 / Loss: 0.0004 / Epsilon: 0.2281\n",
      "3370 Episode / Step: 39344 / Score: 0.97 / Loss: 0.0005 / Epsilon: 0.2273\n",
      "3380 Episode / Step: 39391 / Score: 0.76 / Loss: 0.0004 / Epsilon: 0.2262\n",
      "3390 Episode / Step: 39438 / Score: 0.96 / Loss: 0.0004 / Epsilon: 0.2251\n",
      "3400 Episode / Step: 39484 / Score: 0.96 / Loss: 0.0004 / Epsilon: 0.2241\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "3410 Episode / Step: 39513 / Score: 0.98 / Loss: 0.0005 / Epsilon: 0.2235\n",
      "3420 Episode / Step: 39555 / Score: 0.97 / Loss: 0.0008 / Epsilon: 0.2225\n",
      "3430 Episode / Step: 39593 / Score: 0.97 / Loss: 0.0007 / Epsilon: 0.2217\n",
      "3440 Episode / Step: 39627 / Score: 0.58 / Loss: 0.0006 / Epsilon: 0.2209\n",
      "3450 Episode / Step: 39683 / Score: 0.95 / Loss: 0.0006 / Epsilon: 0.2196\n",
      "3460 Episode / Step: 39732 / Score: 0.96 / Loss: 0.0006 / Epsilon: 0.2185\n",
      "3470 Episode / Step: 39781 / Score: 0.96 / Loss: 0.0004 / Epsilon: 0.2174\n",
      "3480 Episode / Step: 39817 / Score: 0.97 / Loss: 0.0006 / Epsilon: 0.2166\n",
      "3490 Episode / Step: 39858 / Score: 0.97 / Loss: 0.0004 / Epsilon: 0.2157\n",
      "3500 Episode / Step: 39897 / Score: 0.97 / Loss: 0.0006 / Epsilon: 0.2148\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "3510 Episode / Step: 39974 / Score: 0.93 / Loss: 0.0005 / Epsilon: 0.2131\n",
      "3520 Episode / Step: 40034 / Score: 0.95 / Loss: 0.0007 / Epsilon: 0.2117\n",
      "3530 Episode / Step: 40106 / Score: 0.94 / Loss: 0.0005 / Epsilon: 0.2101\n",
      "3540 Episode / Step: 40140 / Score: 0.98 / Loss: 0.0005 / Epsilon: 0.2093\n",
      "3550 Episode / Step: 40200 / Score: 0.95 / Loss: 0.0005 / Epsilon: 0.2080\n",
      "3560 Episode / Step: 40246 / Score: 0.96 / Loss: 0.0004 / Epsilon: 0.2070\n",
      "3570 Episode / Step: 40290 / Score: 0.77 / Loss: 0.0004 / Epsilon: 0.2060\n",
      "3580 Episode / Step: 40337 / Score: 0.96 / Loss: 0.0004 / Epsilon: 0.2049\n",
      "3590 Episode / Step: 40389 / Score: 0.96 / Loss: 0.0004 / Epsilon: 0.2037\n",
      "3600 Episode / Step: 40446 / Score: 0.75 / Loss: 0.0004 / Epsilon: 0.2025\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "3610 Episode / Step: 40495 / Score: 0.96 / Loss: 0.0004 / Epsilon: 0.2014\n",
      "3620 Episode / Step: 40537 / Score: 0.97 / Loss: 0.0007 / Epsilon: 0.2004\n",
      "3630 Episode / Step: 40567 / Score: 0.98 / Loss: 0.0006 / Epsilon: 0.1997\n",
      "3640 Episode / Step: 40593 / Score: 0.78 / Loss: 0.0005 / Epsilon: 0.1992\n",
      "3650 Episode / Step: 40641 / Score: 0.96 / Loss: 0.0007 / Epsilon: 0.1981\n",
      "3660 Episode / Step: 40696 / Score: 0.76 / Loss: 0.0006 / Epsilon: 0.1968\n",
      "3670 Episode / Step: 40741 / Score: 0.97 / Loss: 0.0005 / Epsilon: 0.1958\n",
      "3680 Episode / Step: 40825 / Score: 0.93 / Loss: 0.0006 / Epsilon: 0.1939\n",
      "3690 Episode / Step: 40885 / Score: 0.95 / Loss: 0.0005 / Epsilon: 0.1926\n",
      "3700 Episode / Step: 40935 / Score: 0.96 / Loss: 0.0004 / Epsilon: 0.1915\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "3710 Episode / Step: 40966 / Score: 0.98 / Loss: 0.0005 / Epsilon: 0.1908\n",
      "3720 Episode / Step: 41060 / Score: 0.72 / Loss: 0.0007 / Epsilon: 0.1886\n",
      "3730 Episode / Step: 41093 / Score: 0.98 / Loss: 0.0008 / Epsilon: 0.1879\n",
      "3740 Episode / Step: 41147 / Score: 0.96 / Loss: 0.0006 / Epsilon: 0.1867\n",
      "3750 Episode / Step: 41218 / Score: 0.94 / Loss: 0.0006 / Epsilon: 0.1851\n",
      "3760 Episode / Step: 41281 / Score: 0.75 / Loss: 0.0007 / Epsilon: 0.1837\n",
      "3770 Episode / Step: 41313 / Score: 0.78 / Loss: 0.0004 / Epsilon: 0.1830\n",
      "3780 Episode / Step: 41357 / Score: 0.77 / Loss: 0.0006 / Epsilon: 0.1820\n",
      "3790 Episode / Step: 41406 / Score: 0.96 / Loss: 0.0006 / Epsilon: 0.1809\n",
      "3800 Episode / Step: 41436 / Score: 0.78 / Loss: 0.0005 / Epsilon: 0.1802\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "3810 Episode / Step: 41499 / Score: 0.95 / Loss: 0.0006 / Epsilon: 0.1788\n",
      "3820 Episode / Step: 41547 / Score: 0.96 / Loss: 0.0008 / Epsilon: 0.1777\n",
      "3830 Episode / Step: 41583 / Score: 0.97 / Loss: 0.0008 / Epsilon: 0.1769\n",
      "3840 Episode / Step: 41627 / Score: 0.97 / Loss: 0.0006 / Epsilon: 0.1759\n",
      "3850 Episode / Step: 41663 / Score: 0.97 / Loss: 0.0005 / Epsilon: 0.1751\n",
      "3860 Episode / Step: 41713 / Score: 0.96 / Loss: 0.0005 / Epsilon: 0.1740\n",
      "3870 Episode / Step: 41776 / Score: 0.95 / Loss: 0.0006 / Epsilon: 0.1725\n",
      "3880 Episode / Step: 41821 / Score: 0.97 / Loss: 0.0007 / Epsilon: 0.1715\n",
      "3890 Episode / Step: 41858 / Score: 0.97 / Loss: 0.0007 / Epsilon: 0.1707\n",
      "3900 Episode / Step: 41890 / Score: 0.98 / Loss: 0.0005 / Epsilon: 0.1700\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "3910 Episode / Step: 41965 / Score: 0.94 / Loss: 0.0006 / Epsilon: 0.1683\n",
      "3920 Episode / Step: 42080 / Score: 0.90 / Loss: 0.0006 / Epsilon: 0.1657\n",
      "3930 Episode / Step: 42114 / Score: 0.78 / Loss: 0.0006 / Epsilon: 0.1649\n",
      "3940 Episode / Step: 42166 / Score: 0.96 / Loss: 0.0007 / Epsilon: 0.1638\n",
      "3950 Episode / Step: 42221 / Score: 0.96 / Loss: 0.0007 / Epsilon: 0.1625\n",
      "3960 Episode / Step: 42258 / Score: 0.77 / Loss: 0.0006 / Epsilon: 0.1617\n",
      "3970 Episode / Step: 42367 / Score: 0.70 / Loss: 0.0004 / Epsilon: 0.1592\n",
      "3980 Episode / Step: 42398 / Score: 0.98 / Loss: 0.0004 / Epsilon: 0.1585\n",
      "3990 Episode / Step: 42434 / Score: 0.97 / Loss: 0.0005 / Epsilon: 0.1577\n",
      "4000 Episode / Step: 42477 / Score: 0.97 / Loss: 0.0004 / Epsilon: 0.1568\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "4010 Episode / Step: 42525 / Score: 0.96 / Loss: 0.0005 / Epsilon: 0.1557\n",
      "4020 Episode / Step: 42591 / Score: 0.94 / Loss: 0.0005 / Epsilon: 0.1542\n",
      "4030 Episode / Step: 42657 / Score: 0.94 / Loss: 0.0006 / Epsilon: 0.1527\n",
      "4040 Episode / Step: 42681 / Score: 0.99 / Loss: 0.0006 / Epsilon: 0.1522\n",
      "4050 Episode / Step: 42730 / Score: 0.96 / Loss: 0.0005 / Epsilon: 0.1511\n",
      "4060 Episode / Step: 42775 / Score: 0.57 / Loss: 0.0004 / Epsilon: 0.1501\n",
      "4070 Episode / Step: 42812 / Score: 0.97 / Loss: 0.0009 / Epsilon: 0.1492\n",
      "4080 Episode / Step: 42964 / Score: 0.76 / Loss: 0.0004 / Epsilon: 0.1458\n",
      "4090 Episode / Step: 43017 / Score: 0.76 / Loss: 0.0005 / Epsilon: 0.1446\n",
      "4100 Episode / Step: 43119 / Score: 0.91 / Loss: 0.0006 / Epsilon: 0.1423\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "4110 Episode / Step: 43158 / Score: 0.97 / Loss: 0.0007 / Epsilon: 0.1414\n",
      "4120 Episode / Step: 43201 / Score: 0.77 / Loss: 0.0004 / Epsilon: 0.1405\n",
      "4130 Episode / Step: 43234 / Score: 0.58 / Loss: 0.0005 / Epsilon: 0.1397\n",
      "4140 Episode / Step: 43278 / Score: 0.57 / Loss: 0.0009 / Epsilon: 0.1387\n",
      "4150 Episode / Step: 43312 / Score: 0.98 / Loss: 0.0007 / Epsilon: 0.1380\n",
      "4160 Episode / Step: 43350 / Score: 0.77 / Loss: 0.0007 / Epsilon: 0.1371\n",
      "4170 Episode / Step: 43404 / Score: 0.96 / Loss: 0.0005 / Epsilon: 0.1359\n",
      "4180 Episode / Step: 43462 / Score: 0.55 / Loss: 0.0004 / Epsilon: 0.1346\n",
      "4190 Episode / Step: 43504 / Score: 0.77 / Loss: 0.0004 / Epsilon: 0.1337\n",
      "4200 Episode / Step: 43536 / Score: 0.98 / Loss: 0.0007 / Epsilon: 0.1329\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "4210 Episode / Step: 43580 / Score: 0.97 / Loss: 0.0006 / Epsilon: 0.1319\n",
      "4220 Episode / Step: 43637 / Score: 0.95 / Loss: 0.0005 / Epsilon: 0.1307\n",
      "4230 Episode / Step: 43687 / Score: 0.76 / Loss: 0.0004 / Epsilon: 0.1295\n",
      "4240 Episode / Step: 43715 / Score: 0.98 / Loss: 0.0004 / Epsilon: 0.1289\n",
      "4250 Episode / Step: 43766 / Score: 0.96 / Loss: 0.0005 / Epsilon: 0.1278\n",
      "4260 Episode / Step: 43805 / Score: 0.97 / Loss: 0.0009 / Epsilon: 0.1269\n",
      "4270 Episode / Step: 43840 / Score: 0.98 / Loss: 0.0006 / Epsilon: 0.1261\n",
      "4280 Episode / Step: 43875 / Score: 0.98 / Loss: 0.0004 / Epsilon: 0.1253\n",
      "4290 Episode / Step: 43907 / Score: 0.78 / Loss: 0.0004 / Epsilon: 0.1246\n",
      "4300 Episode / Step: 43941 / Score: 0.78 / Loss: 0.0003 / Epsilon: 0.1238\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "4310 Episode / Step: 43979 / Score: 0.97 / Loss: 0.0007 / Epsilon: 0.1230\n",
      "4320 Episode / Step: 44028 / Score: 0.96 / Loss: 0.0005 / Epsilon: 0.1219\n",
      "4330 Episode / Step: 44144 / Score: 0.89 / Loss: 0.0005 / Epsilon: 0.1193\n",
      "4340 Episode / Step: 44195 / Score: 0.96 / Loss: 0.0004 / Epsilon: 0.1181\n",
      "4350 Episode / Step: 44248 / Score: 0.96 / Loss: 0.0004 / Epsilon: 0.1169\n",
      "4360 Episode / Step: 44281 / Score: 0.78 / Loss: 0.0004 / Epsilon: 0.1162\n",
      "4370 Episode / Step: 44316 / Score: 0.98 / Loss: 0.0003 / Epsilon: 0.1154\n",
      "4380 Episode / Step: 44431 / Score: 0.90 / Loss: 0.0006 / Epsilon: 0.1128\n",
      "4390 Episode / Step: 44470 / Score: 0.97 / Loss: 0.0003 / Epsilon: 0.1119\n",
      "4400 Episode / Step: 44502 / Score: 0.98 / Loss: 0.0004 / Epsilon: 0.1112\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "4410 Episode / Step: 44549 / Score: 0.96 / Loss: 0.0004 / Epsilon: 0.1101\n",
      "4420 Episode / Step: 44596 / Score: 0.96 / Loss: 0.0004 / Epsilon: 0.1091\n",
      "4430 Episode / Step: 44639 / Score: 0.77 / Loss: 0.0004 / Epsilon: 0.1081\n",
      "4440 Episode / Step: 44698 / Score: 0.95 / Loss: 0.0003 / Epsilon: 0.1068\n",
      "4450 Episode / Step: 44746 / Score: 0.96 / Loss: 0.0004 / Epsilon: 0.1057\n",
      "4460 Episode / Step: 44782 / Score: 0.97 / Loss: 0.0003 / Epsilon: 0.1049\n",
      "4470 Episode / Step: 44815 / Score: 0.98 / Loss: 0.0005 / Epsilon: 0.1042\n",
      "4480 Episode / Step: 44851 / Score: 0.77 / Loss: 0.0015 / Epsilon: 0.1034\n",
      "4490 Episode / Step: 44935 / Score: 0.93 / Loss: 0.0005 / Epsilon: 0.1015\n",
      "4500 Episode / Step: 44962 / Score: 0.98 / Loss: 0.0009 / Epsilon: 0.1009\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "4510 Episode / Step: 45006 / Score: 0.77 / Loss: 0.0007 / Epsilon: 0.1000\n",
      "4520 Episode / Step: 45070 / Score: 0.95 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "4530 Episode / Step: 45109 / Score: 0.97 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "4540 Episode / Step: 45137 / Score: 0.78 / Loss: 0.0006 / Epsilon: 0.1000\n",
      "4550 Episode / Step: 45168 / Score: 0.98 / Loss: 0.0005 / Epsilon: 0.1000\n",
      "4560 Episode / Step: 45201 / Score: 0.98 / Loss: 0.0005 / Epsilon: 0.1000\n",
      "4570 Episode / Step: 45241 / Score: 0.97 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "4580 Episode / Step: 45278 / Score: 0.97 / Loss: 0.0005 / Epsilon: 0.1000\n",
      "4590 Episode / Step: 45313 / Score: 0.98 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "4600 Episode / Step: 45393 / Score: 0.73 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "4610 Episode / Step: 45427 / Score: 0.98 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "4620 Episode / Step: 45467 / Score: 0.97 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "4630 Episode / Step: 45510 / Score: 0.77 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "4640 Episode / Step: 45541 / Score: 0.98 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "4650 Episode / Step: 45580 / Score: 0.97 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "4660 Episode / Step: 45621 / Score: 0.97 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "4670 Episode / Step: 45660 / Score: 0.97 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "4680 Episode / Step: 45721 / Score: 0.95 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "4690 Episode / Step: 45759 / Score: 0.97 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "4700 Episode / Step: 45793 / Score: 0.98 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "4710 Episode / Step: 45844 / Score: 0.96 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "4720 Episode / Step: 45891 / Score: 0.76 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "4730 Episode / Step: 45920 / Score: 0.98 / Loss: 0.0005 / Epsilon: 0.1000\n",
      "4740 Episode / Step: 45949 / Score: 0.98 / Loss: 0.0005 / Epsilon: 0.1000\n",
      "4750 Episode / Step: 45976 / Score: 0.98 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "4760 Episode / Step: 46014 / Score: 0.77 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "4770 Episode / Step: 46057 / Score: 0.97 / Loss: 0.0005 / Epsilon: 0.1000\n",
      "4780 Episode / Step: 46093 / Score: 0.97 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "4790 Episode / Step: 46124 / Score: 0.98 / Loss: 0.0005 / Epsilon: 0.1000\n",
      "4800 Episode / Step: 46166 / Score: 0.97 / Loss: 0.0005 / Epsilon: 0.1000\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "4810 Episode / Step: 46200 / Score: 0.98 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "4820 Episode / Step: 46231 / Score: 0.98 / Loss: 0.0005 / Epsilon: 0.1000\n",
      "4830 Episode / Step: 46306 / Score: 0.54 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "4840 Episode / Step: 46351 / Score: 0.97 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "4850 Episode / Step: 46410 / Score: 0.55 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "4860 Episode / Step: 46455 / Score: 0.97 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "4870 Episode / Step: 46488 / Score: 0.98 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "4880 Episode / Step: 46530 / Score: 0.97 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "4890 Episode / Step: 46558 / Score: 0.98 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "4900 Episode / Step: 46604 / Score: 0.96 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "4910 Episode / Step: 46654 / Score: 0.76 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "4920 Episode / Step: 46701 / Score: 0.96 / Loss: 0.0007 / Epsilon: 0.1000\n",
      "4930 Episode / Step: 46732 / Score: 0.98 / Loss: 0.0005 / Epsilon: 0.1000\n",
      "4940 Episode / Step: 46770 / Score: 0.97 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "4950 Episode / Step: 46839 / Score: 0.74 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "4960 Episode / Step: 46866 / Score: 0.98 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "4970 Episode / Step: 46907 / Score: 0.97 / Loss: 0.0005 / Epsilon: 0.1000\n",
      "4980 Episode / Step: 46943 / Score: 0.97 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "4990 Episode / Step: 46985 / Score: 0.97 / Loss: 0.0009 / Epsilon: 0.1000\n",
      "5000 Episode / Step: 47018 / Score: 0.98 / Loss: 0.0005 / Epsilon: 0.1000\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "5010 Episode / Step: 47050 / Score: 0.98 / Loss: 0.0006 / Epsilon: 0.1000\n",
      "5020 Episode / Step: 47084 / Score: 0.98 / Loss: 0.0005 / Epsilon: 0.1000\n",
      "5030 Episode / Step: 47117 / Score: 0.98 / Loss: 0.0005 / Epsilon: 0.1000\n",
      "5040 Episode / Step: 47159 / Score: 0.97 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "5050 Episode / Step: 47211 / Score: 0.96 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5060 Episode / Step: 47247 / Score: 0.97 / Loss: 0.0002 / Epsilon: 0.1000\n",
      "5070 Episode / Step: 47311 / Score: 0.95 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5080 Episode / Step: 47350 / Score: 0.97 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5090 Episode / Step: 47401 / Score: 0.96 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5100 Episode / Step: 47455 / Score: 0.76 / Loss: 0.0002 / Epsilon: 0.1000\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "5110 Episode / Step: 47598 / Score: 0.57 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "5120 Episode / Step: 47628 / Score: 0.78 / Loss: 0.0006 / Epsilon: 0.1000\n",
      "5130 Episode / Step: 47665 / Score: 0.97 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5140 Episode / Step: 47711 / Score: 0.76 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5150 Episode / Step: 47745 / Score: 0.98 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5160 Episode / Step: 47783 / Score: 0.97 / Loss: 0.0005 / Epsilon: 0.1000\n",
      "5170 Episode / Step: 47826 / Score: 0.77 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5180 Episode / Step: 47865 / Score: 0.97 / Loss: 0.0005 / Epsilon: 0.1000\n",
      "5190 Episode / Step: 47895 / Score: 0.98 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5200 Episode / Step: 47926 / Score: 0.98 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "5210 Episode / Step: 48019 / Score: 0.92 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5220 Episode / Step: 48061 / Score: 0.97 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "5230 Episode / Step: 48099 / Score: 0.97 / Loss: 0.0006 / Epsilon: 0.1000\n",
      "5240 Episode / Step: 48139 / Score: 0.97 / Loss: 0.0005 / Epsilon: 0.1000\n",
      "5250 Episode / Step: 48181 / Score: 0.97 / Loss: 0.0005 / Epsilon: 0.1000\n",
      "5260 Episode / Step: 48226 / Score: 0.97 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5270 Episode / Step: 48274 / Score: 0.96 / Loss: 0.0002 / Epsilon: 0.1000\n",
      "5280 Episode / Step: 48314 / Score: 0.97 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "5290 Episode / Step: 48346 / Score: 0.98 / Loss: 0.0005 / Epsilon: 0.1000\n",
      "5300 Episode / Step: 48380 / Score: 0.98 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "5310 Episode / Step: 48423 / Score: 0.97 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "5320 Episode / Step: 48462 / Score: 0.77 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5330 Episode / Step: 48507 / Score: 0.97 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5340 Episode / Step: 48537 / Score: 0.98 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "5350 Episode / Step: 48570 / Score: 0.98 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "5360 Episode / Step: 48598 / Score: 0.98 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5370 Episode / Step: 48636 / Score: 0.97 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5380 Episode / Step: 48671 / Score: 0.98 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5390 Episode / Step: 48709 / Score: 0.97 / Loss: 0.0002 / Epsilon: 0.1000\n",
      "5400 Episode / Step: 48750 / Score: 0.97 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "5410 Episode / Step: 48796 / Score: 0.96 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5420 Episode / Step: 48825 / Score: 0.98 / Loss: 0.0002 / Epsilon: 0.1000\n",
      "5430 Episode / Step: 48857 / Score: 0.98 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "5440 Episode / Step: 48903 / Score: 0.96 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5450 Episode / Step: 48945 / Score: 0.97 / Loss: 0.0002 / Epsilon: 0.1000\n",
      "5460 Episode / Step: 48979 / Score: 0.98 / Loss: 0.0002 / Epsilon: 0.1000\n",
      "5470 Episode / Step: 49027 / Score: 0.96 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5480 Episode / Step: 49065 / Score: 0.97 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5490 Episode / Step: 49120 / Score: 0.96 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5500 Episode / Step: 49162 / Score: 0.77 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "5510 Episode / Step: 49205 / Score: 0.97 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5520 Episode / Step: 49227 / Score: 0.99 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5530 Episode / Step: 49267 / Score: 0.97 / Loss: 0.0002 / Epsilon: 0.1000\n",
      "5540 Episode / Step: 49297 / Score: 0.98 / Loss: 0.0002 / Epsilon: 0.1000\n",
      "5550 Episode / Step: 49334 / Score: 0.97 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5560 Episode / Step: 49374 / Score: 0.97 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5570 Episode / Step: 49397 / Score: 0.79 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5580 Episode / Step: 49465 / Score: 0.94 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5590 Episode / Step: 49502 / Score: 0.97 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5600 Episode / Step: 49556 / Score: 0.76 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "5610 Episode / Step: 49585 / Score: 0.98 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5620 Episode / Step: 49621 / Score: 0.97 / Loss: 0.0005 / Epsilon: 0.1000\n",
      "5630 Episode / Step: 49654 / Score: 0.98 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5640 Episode / Step: 49684 / Score: 0.98 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5650 Episode / Step: 49719 / Score: 0.98 / Loss: 0.0002 / Epsilon: 0.1000\n",
      "5660 Episode / Step: 49751 / Score: 0.98 / Loss: 0.0005 / Epsilon: 0.1000\n",
      "5670 Episode / Step: 49792 / Score: 0.97 / Loss: 0.0004 / Epsilon: 0.1000\n",
      "5680 Episode / Step: 49830 / Score: 0.97 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5690 Episode / Step: 49864 / Score: 0.98 / Loss: 0.0002 / Epsilon: 0.1000\n",
      "5700 Episode / Step: 49900 / Score: 0.97 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "5710 Episode / Step: 49943 / Score: 0.77 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "5720 Episode / Step: 49987 / Score: 0.97 / Loss: 0.0003 / Epsilon: 0.1000\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "TEST START\n",
      "5730 Episode / Step: 50015 / Score: 0.98 / Loss: 0.0002 / Epsilon: 0.1000\n",
      "5740 Episode / Step: 50041 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "5750 Episode / Step: 50081 / Score: 0.97 / Loss: nan / Epsilon: 0.1000\n",
      "5760 Episode / Step: 50115 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "5770 Episode / Step: 50232 / Score: 0.79 / Loss: nan / Epsilon: 0.1000\n",
      "5780 Episode / Step: 50268 / Score: 0.77 / Loss: nan / Epsilon: 0.1000\n",
      "5790 Episode / Step: 50299 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "5800 Episode / Step: 50324 / Score: 0.99 / Loss: nan / Epsilon: 0.1000\n",
      "5810 Episode / Step: 50354 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "5820 Episode / Step: 50393 / Score: 0.77 / Loss: nan / Epsilon: 0.1000\n",
      "5830 Episode / Step: 50524 / Score: 0.78 / Loss: nan / Epsilon: 0.1000\n",
      "5840 Episode / Step: 50555 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "5850 Episode / Step: 50623 / Score: 0.94 / Loss: nan / Epsilon: 0.1000\n",
      "5860 Episode / Step: 50649 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "5870 Episode / Step: 50683 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "5880 Episode / Step: 50721 / Score: 0.97 / Loss: nan / Epsilon: 0.1000\n",
      "5890 Episode / Step: 50756 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "5900 Episode / Step: 50835 / Score: 0.93 / Loss: nan / Epsilon: 0.1000\n",
      "5910 Episode / Step: 50959 / Score: 0.89 / Loss: nan / Epsilon: 0.1000\n",
      "5920 Episode / Step: 51004 / Score: 0.97 / Loss: nan / Epsilon: 0.1000\n",
      "5930 Episode / Step: 51042 / Score: 0.97 / Loss: nan / Epsilon: 0.1000\n",
      "5940 Episode / Step: 51146 / Score: 0.91 / Loss: nan / Epsilon: 0.1000\n",
      "5950 Episode / Step: 51212 / Score: 0.94 / Loss: nan / Epsilon: 0.1000\n",
      "5960 Episode / Step: 51245 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "5970 Episode / Step: 51345 / Score: 0.71 / Loss: nan / Epsilon: 0.1000\n",
      "5980 Episode / Step: 51397 / Score: 0.96 / Loss: nan / Epsilon: 0.1000\n",
      "5990 Episode / Step: 51433 / Score: 0.97 / Loss: nan / Epsilon: 0.1000\n",
      "6000 Episode / Step: 51489 / Score: 0.95 / Loss: nan / Epsilon: 0.1000\n",
      "6010 Episode / Step: 51524 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6020 Episode / Step: 51560 / Score: 0.97 / Loss: nan / Epsilon: 0.1000\n",
      "6030 Episode / Step: 51590 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6040 Episode / Step: 51657 / Score: 0.94 / Loss: nan / Epsilon: 0.1000\n",
      "6050 Episode / Step: 51689 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6060 Episode / Step: 51720 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6070 Episode / Step: 51755 / Score: 0.78 / Loss: nan / Epsilon: 0.1000\n",
      "6080 Episode / Step: 51859 / Score: 0.91 / Loss: nan / Epsilon: 0.1000\n",
      "6090 Episode / Step: 51890 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6100 Episode / Step: 51932 / Score: 0.97 / Loss: nan / Epsilon: 0.1000\n",
      "6110 Episode / Step: 51985 / Score: 0.96 / Loss: nan / Epsilon: 0.1000\n",
      "6120 Episode / Step: 52031 / Score: 0.96 / Loss: nan / Epsilon: 0.1000\n",
      "6130 Episode / Step: 52063 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6140 Episode / Step: 52104 / Score: 0.97 / Loss: nan / Epsilon: 0.1000\n",
      "6150 Episode / Step: 52155 / Score: 0.96 / Loss: nan / Epsilon: 0.1000\n",
      "6160 Episode / Step: 52190 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6170 Episode / Step: 52220 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6180 Episode / Step: 52262 / Score: 0.97 / Loss: nan / Epsilon: 0.1000\n",
      "6190 Episode / Step: 52300 / Score: 0.97 / Loss: nan / Epsilon: 0.1000\n",
      "6200 Episode / Step: 52424 / Score: 0.69 / Loss: nan / Epsilon: 0.1000\n",
      "6210 Episode / Step: 52458 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6220 Episode / Step: 52501 / Score: 0.97 / Loss: nan / Epsilon: 0.1000\n",
      "6230 Episode / Step: 52533 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6240 Episode / Step: 52564 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6250 Episode / Step: 52595 / Score: 0.78 / Loss: nan / Epsilon: 0.1000\n",
      "6260 Episode / Step: 52717 / Score: 0.79 / Loss: nan / Epsilon: 0.1000\n",
      "6270 Episode / Step: 52745 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6280 Episode / Step: 52790 / Score: 0.97 / Loss: nan / Epsilon: 0.1000\n",
      "6290 Episode / Step: 52839 / Score: 0.96 / Loss: nan / Epsilon: 0.1000\n",
      "6300 Episode / Step: 52870 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6310 Episode / Step: 52917 / Score: 0.96 / Loss: nan / Epsilon: 0.1000\n",
      "6320 Episode / Step: 52953 / Score: 0.77 / Loss: nan / Epsilon: 0.1000\n",
      "6330 Episode / Step: 53011 / Score: 0.95 / Loss: nan / Epsilon: 0.1000\n",
      "6340 Episode / Step: 53048 / Score: 0.77 / Loss: nan / Epsilon: 0.1000\n",
      "6350 Episode / Step: 53083 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6360 Episode / Step: 53207 / Score: 0.69 / Loss: nan / Epsilon: 0.1000\n",
      "6370 Episode / Step: 53241 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6380 Episode / Step: 53274 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6390 Episode / Step: 53305 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6400 Episode / Step: 53356 / Score: 0.96 / Loss: nan / Epsilon: 0.1000\n",
      "6410 Episode / Step: 53397 / Score: 0.97 / Loss: nan / Epsilon: 0.1000\n",
      "6420 Episode / Step: 53530 / Score: 0.58 / Loss: nan / Epsilon: 0.1000\n",
      "6430 Episode / Step: 53658 / Score: 0.78 / Loss: nan / Epsilon: 0.1000\n",
      "6440 Episode / Step: 53732 / Score: 0.94 / Loss: nan / Epsilon: 0.1000\n",
      "6450 Episode / Step: 53781 / Score: 0.96 / Loss: nan / Epsilon: 0.1000\n",
      "6460 Episode / Step: 53817 / Score: 0.97 / Loss: nan / Epsilon: 0.1000\n",
      "6470 Episode / Step: 53850 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6480 Episode / Step: 53888 / Score: 0.97 / Loss: nan / Epsilon: 0.1000\n",
      "6490 Episode / Step: 53930 / Score: 0.97 / Loss: nan / Epsilon: 0.1000\n",
      "6500 Episode / Step: 53969 / Score: 0.97 / Loss: nan / Epsilon: 0.1000\n",
      "6510 Episode / Step: 54003 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6520 Episode / Step: 54035 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6530 Episode / Step: 54069 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6540 Episode / Step: 54112 / Score: 0.97 / Loss: nan / Epsilon: 0.1000\n",
      "6550 Episode / Step: 54155 / Score: 0.97 / Loss: nan / Epsilon: 0.1000\n",
      "6560 Episode / Step: 54187 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6570 Episode / Step: 54232 / Score: 0.97 / Loss: nan / Epsilon: 0.1000\n",
      "6580 Episode / Step: 54260 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6590 Episode / Step: 54297 / Score: 0.97 / Loss: nan / Epsilon: 0.1000\n",
      "6600 Episode / Step: 54336 / Score: 0.97 / Loss: nan / Epsilon: 0.1000\n",
      "6610 Episode / Step: 54375 / Score: 0.97 / Loss: nan / Epsilon: 0.1000\n",
      "6620 Episode / Step: 54406 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6630 Episode / Step: 54433 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6640 Episode / Step: 54463 / Score: 0.78 / Loss: nan / Epsilon: 0.1000\n",
      "6650 Episode / Step: 54496 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6660 Episode / Step: 54523 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6670 Episode / Step: 54562 / Score: 0.97 / Loss: nan / Epsilon: 0.1000\n",
      "6680 Episode / Step: 54597 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6690 Episode / Step: 54769 / Score: 0.54 / Loss: nan / Epsilon: 0.1000\n",
      "6700 Episode / Step: 54803 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6710 Episode / Step: 54840 / Score: 0.97 / Loss: nan / Epsilon: 0.1000\n",
      "6720 Episode / Step: 54931 / Score: 0.92 / Loss: nan / Epsilon: 0.1000\n",
      "6730 Episode / Step: 54958 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n",
      "6740 Episode / Step: 54990 / Score: 0.98 / Loss: nan / Epsilon: 0.1000\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 불러오기\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import datetime\n",
    "import platform\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque\n",
    "from mlagents_envs.environment import UnityEnvironment, ActionTuple\n",
    "from mlagents_envs.side_channel.engine_configuration_channel\\\n",
    "                             import EngineConfigurationChannel\n",
    "\n",
    "# DQN을 위한 파라미터 값 세팅 \n",
    "state_size = [3*2, 64, 84]\n",
    "action_size = 4 \n",
    "\n",
    "load_model = False\n",
    "train_mode = True\n",
    "\n",
    "batch_size = 32\n",
    "mem_maxlen = 10000\n",
    "discount_factor = 0.9\n",
    "learning_rate = 0.00025\n",
    "\n",
    "run_step = 50000 if train_mode else 0\n",
    "test_step = 5000\n",
    "train_start_step = 5000\n",
    "target_update_step = 500\n",
    "\n",
    "print_interval = 10\n",
    "save_interval = 100\n",
    "\n",
    "epsilon_eval = 0.05\n",
    "epsilon_init = 1.0 if train_mode else epsilon_eval\n",
    "epsilon_min = 0.1\n",
    "explore_step = run_step * 0.8\n",
    "eplsilon_delta = (epsilon_init - epsilon_min)/explore_step if train_mode else 0.\n",
    "\n",
    "VISUAL_OBS = 0\n",
    "GOAL_OBS = 1\n",
    "VECTOR_OBS = 2\n",
    "OBS = VISUAL_OBS\n",
    "\n",
    "# 유니티 환경 경로 \n",
    "game = \"GridWorld\"\n",
    "env_name = \"./Unity_practice/GridWorld/GridWorld\"\n",
    "\n",
    "# 모델 저장 및 불러오기 경로\n",
    "date_time = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "save_path = f\"./Unity_practice/saved_models/{game}/DQN/{date_time}\"\n",
    "load_path = f\"./Unity_practice/saved_models/{game}/DQN/20210514201212\"\n",
    "\n",
    "# 연산 장치\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# DQN 클래스 -> Deep Q Network 정의 \n",
    "class DQN(torch.nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DQN, self).__init__(**kwargs)\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=state_size[0], out_channels=32,\n",
    "                                     kernel_size=8, stride=4)\n",
    "        dim1 = ((state_size[1] - 8)//4 + 1, (state_size[2] - 8)//4 + 1)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=32, out_channels=64,\n",
    "                                     kernel_size=4, stride=2)\n",
    "        dim2 = ((dim1[0] - 4)//2 + 1, (dim1[1] - 4)//2 + 1)\n",
    "        self.conv3 = torch.nn.Conv2d(in_channels=64, out_channels=64,\n",
    "                                     kernel_size=3, stride=1)\n",
    "        dim3 = ((dim2[0] - 3)//1 + 1, (dim2[1] - 3)//1 + 1)\n",
    "\n",
    "        self.flat = torch.nn.Flatten()\n",
    "        self.fc1 = torch.nn.Linear(64*dim3[0]*dim3[1], 512)\n",
    "        self.q = torch.nn.Linear(512, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.flat(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.q(x)\n",
    "\n",
    "# DQNAgent 클래스 -> DQN 알고리즘을 위한 다양한 함수 정의 \n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.network = DQN().to(device)\n",
    "        self.target_network = copy.deepcopy(self.network)\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=learning_rate)\n",
    "        self.memory = deque(maxlen=mem_maxlen)\n",
    "        self.epsilon = epsilon_init\n",
    "        self.writer = SummaryWriter(save_path)\n",
    "\n",
    "        if load_model == True:\n",
    "            print(f\"... Load Model from {load_path}/ckpt ...\")\n",
    "            checkpoint = torch.load(load_path+'/ckpt', map_location=device)\n",
    "            self.network.load_state_dict(checkpoint[\"network\"])\n",
    "            self.target_network.load_state_dict(checkpoint[\"network\"])\n",
    "            self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        \n",
    "    # Epsilon greedy 기법에 따라 행동 결정 \n",
    "    def get_action(self, state, training=True):\n",
    "        #  네트워크 모드 설정\n",
    "        self.network.train(training)\n",
    "        epsilon = self.epsilon if training else epsilon_eval\n",
    "\n",
    "        # 랜덤하게 행동 결정\n",
    "        if epsilon > random.random():  \n",
    "            action = np.random.randint(0, action_size, size=(state.shape[0],1))\n",
    "        # 네트워크 연산에 따라 행동 결정\n",
    "        else:\n",
    "            q = self.network(torch.FloatTensor(state).to(device))\n",
    "            action = torch.argmax(q, axis=-1, keepdim=True).data.cpu().numpy()\n",
    "        return action\n",
    "\n",
    "    # 리플레이 메모리에 데이터 추가 (상태, 행동, 보상, 다음 상태, 게임 종료 여부)\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # 학습 수행\n",
    "    def train_model(self):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        state      = np.stack([b[0] for b in batch], axis=0)\n",
    "        action     = np.stack([b[1] for b in batch], axis=0)\n",
    "        reward     = np.stack([b[2] for b in batch], axis=0)\n",
    "        next_state = np.stack([b[3] for b in batch], axis=0)\n",
    "        done       = np.stack([b[4] for b in batch], axis=0)\n",
    "\n",
    "        state, action, reward, next_state, done = map(lambda x: torch.FloatTensor(x).to(device),\n",
    "                                                        [state, action, reward, next_state, done])\n",
    "\n",
    "        eye = torch.eye(action_size).to(device)\n",
    "        one_hot_action = eye[action.view(-1).long()]\n",
    "        q = (self.network(state) * one_hot_action).sum(1, keepdims=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_network(next_state)\n",
    "            target_q = reward + next_q.max(1, keepdims=True).values * ((1 - done) * discount_factor)\n",
    "\n",
    "        loss = F.smooth_l1_loss(q, target_q)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # 엡실론 감소\n",
    "        self.epsilon = max(epsilon_min, self.epsilon - eplsilon_delta)\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    # 타겟 네트워크 업데이트\n",
    "    def update_target(self):\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "\n",
    "    # 네트워크 모델 저장 \n",
    "    def save_model(self):\n",
    "        print(f\"... Save Model to {save_path}/ckpt ...\")\n",
    "        torch.save({\n",
    "            \"network\" : self.network.state_dict(),\n",
    "            \"optimizer\" : self.optimizer.state_dict(),\n",
    "        }, save_path+'/ckpt')\n",
    "\n",
    "    # 학습 기록 \n",
    "    def write_summray(self, score, loss, epsilon, step):\n",
    "        self.writer.add_scalar(\"run/score\", score, step)\n",
    "        self.writer.add_scalar(\"model/loss\", loss, step)\n",
    "        self.writer.add_scalar(\"model/epsilon\", epsilon, step)\n",
    "\n",
    "# Main 함수 -> 전체적으로 DQN 알고리즘을 진행 \n",
    "if __name__ == '__main__':\n",
    "    # 유니티 환경 경로 설정 (file_name)\n",
    "    engine_configuration_channel = EngineConfigurationChannel()\n",
    "    env = UnityEnvironment(file_name=env_name,\n",
    "                           side_channels=[engine_configuration_channel],\n",
    "                           worker_id=101\n",
    "                           )\n",
    "    env.reset()\n",
    "\n",
    "    # 유니티 브레인 설정 \n",
    "    behavior_name = list(env.behavior_specs.keys())[0]\n",
    "    spec = env.behavior_specs[behavior_name]\n",
    "    engine_configuration_channel.set_configuration_parameters(time_scale=12.0)\n",
    "    dec, term = env.get_steps(behavior_name)\n",
    "\n",
    "    # DQNAgent 클래스를 agent로 정의 \n",
    "    agent = DQNAgent()\n",
    "    \n",
    "    losses, scores, episode, score = [], [], 0, 0\n",
    "    for step in range(run_step + test_step):\n",
    "        if step == run_step:\n",
    "            if train_mode:\n",
    "                agent.save_model()\n",
    "            print(\"TEST START\")\n",
    "            train_mode = False\n",
    "            engine_configuration_channel.set_configuration_parameters(time_scale=1.0)\n",
    "\n",
    "        preprocess = lambda obs, goal: np.concatenate((obs*goal[0][0], obs*goal[0][1]), axis=-1) \n",
    "        state = preprocess(dec.obs[OBS],dec.obs[GOAL_OBS])\n",
    "        action = agent.get_action(state, train_mode)\n",
    "        real_action = action + 1\n",
    "        action_tuple = ActionTuple()\n",
    "        action_tuple.add_discrete(real_action)\n",
    "        env.set_actions(behavior_name, action_tuple)\n",
    "        env.step()\n",
    "\n",
    "        dec, term = env.get_steps(behavior_name)\n",
    "        done = len(term.agent_id) > 0\n",
    "        reward = term.reward if done else dec.reward\n",
    "        next_state = preprocess(term.obs[OBS], term.obs[GOAL_OBS]) if done\\\n",
    "                     else preprocess(dec.obs[OBS], dec.obs[GOAL_OBS])\n",
    "        score += reward[0]\n",
    "\n",
    "        if train_mode:\n",
    "            agent.append_sample(state[0], action[0], reward, next_state[0], [done])\n",
    "\n",
    "        if train_mode and step > max(batch_size, train_start_step):\n",
    "            # 학습 수행\n",
    "            loss = agent.train_model()\n",
    "            losses.append(loss)\n",
    "\n",
    "            # 타겟 네트워크 업데이트 \n",
    "            if step % target_update_step == 0:\n",
    "                agent.update_target()\n",
    "\n",
    "        if done:\n",
    "            episode +=1\n",
    "            scores.append(score)\n",
    "            score = 0\n",
    "\n",
    "            # 게임 진행 상황 출력 및 텐서 보드에 보상과 손실함수 값 기록 \n",
    "            if episode % print_interval == 0:\n",
    "                mean_score = np.mean(scores)\n",
    "                mean_loss = np.mean(losses)\n",
    "                agent.write_summray(mean_score, mean_loss, agent.epsilon, step)\n",
    "                losses, scores = [], []\n",
    "\n",
    "                print(f\"{episode} Episode / Step: {step} / Score: {mean_score:.2f} / \" +\\\n",
    "                      f\"Loss: {mean_loss:.4f} / Epsilon: {agent.epsilon:.4f}\")\n",
    "\n",
    "            # 네트워크 모델 저장 \n",
    "            if train_mode and episode % save_interval == 0:\n",
    "                agent.save_model()\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN Testing\n",
    "========"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[UnityMemory] Configuration Parameters - Can be set up in boot.config\n",
      "    \"memorysetup-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-main-allocator-block-size=16777216\"\n",
      "    \"memorysetup-gfx-thread-allocator-block-size=16777216\"\n",
      "    \"memorysetup-cache-allocator-block-size=4194304\"\n",
      "    \"memorysetup-typetree-allocator-block-size=2097152\"\n",
      "    \"memorysetup-profiler-bucket-allocator-granularity=16\"\n",
      "    \"memorysetup-profiler-bucket-allocator-bucket-count=8\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-size=4194304\"\n",
      "    \"memorysetup-profiler-bucket-allocator-block-count=1\"\n",
      "    \"memorysetup-profiler-allocator-block-size=16777216\"\n",
      "    \"memorysetup-profiler-editor-allocator-block-size=1048576\"\n",
      "    \"memorysetup-temp-allocator-size-main=4194304\"\n",
      "    \"memorysetup-job-temp-allocator-block-size=2097152\"\n",
      "    \"memorysetup-job-temp-allocator-block-size-background=1048576\"\n",
      "    \"memorysetup-job-temp-allocator-reduction-small-platforms=262144\"\n",
      "    \"memorysetup-temp-allocator-size-background-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-job-worker=262144\"\n",
      "    \"memorysetup-temp-allocator-size-preload-manager=262144\"\n",
      "    \"memorysetup-temp-allocator-size-nav-mesh-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-audio-worker=65536\"\n",
      "    \"memorysetup-temp-allocator-size-cloud-worker=32768\"\n",
      "    \"memorysetup-temp-allocator-size-gfx=262144\"\n",
      "... Load Model from ./Unity_practice/saved_models/GridWorld/DQN/20240220164701/ckpt ...\n",
      "TEST START\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeongminlee/miniforge3/envs/tf29_py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/jeongminlee/miniforge3/envs/tf29_py37/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Episode / Step: 29 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "20 Episode / Step: 64 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "30 Episode / Step: 109 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "40 Episode / Step: 148 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "50 Episode / Step: 183 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "60 Episode / Step: 213 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "70 Episode / Step: 238 / Score: 0.79 / Loss: nan / Epsilon: 0.0500\n",
      "80 Episode / Step: 280 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "90 Episode / Step: 313 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "100 Episode / Step: 349 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "110 Episode / Step: 385 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "120 Episode / Step: 420 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "130 Episode / Step: 459 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "140 Episode / Step: 584 / Score: 0.78 / Loss: nan / Epsilon: 0.0500\n",
      "150 Episode / Step: 616 / Score: 0.78 / Loss: nan / Epsilon: 0.0500\n",
      "160 Episode / Step: 761 / Score: 0.76 / Loss: nan / Epsilon: 0.0500\n",
      "170 Episode / Step: 801 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "180 Episode / Step: 827 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "190 Episode / Step: 856 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "200 Episode / Step: 898 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "210 Episode / Step: 933 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "220 Episode / Step: 970 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "230 Episode / Step: 1001 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "240 Episode / Step: 1045 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "250 Episode / Step: 1085 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "260 Episode / Step: 1211 / Score: 0.78 / Loss: nan / Epsilon: 0.0500\n",
      "270 Episode / Step: 1259 / Score: 0.76 / Loss: nan / Epsilon: 0.0500\n",
      "280 Episode / Step: 1300 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "290 Episode / Step: 1321 / Score: 0.99 / Loss: nan / Epsilon: 0.0500\n",
      "300 Episode / Step: 1469 / Score: 0.76 / Loss: nan / Epsilon: 0.0500\n",
      "310 Episode / Step: 1510 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "320 Episode / Step: 1541 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "330 Episode / Step: 1647 / Score: 0.50 / Loss: nan / Epsilon: 0.0500\n",
      "340 Episode / Step: 1733 / Score: 0.92 / Loss: nan / Epsilon: 0.0500\n",
      "350 Episode / Step: 1806 / Score: 0.94 / Loss: nan / Epsilon: 0.0500\n",
      "360 Episode / Step: 1856 / Score: 0.96 / Loss: nan / Epsilon: 0.0500\n",
      "370 Episode / Step: 1901 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "380 Episode / Step: 1935 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "390 Episode / Step: 1969 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "400 Episode / Step: 2100 / Score: 0.78 / Loss: nan / Epsilon: 0.0500\n",
      "410 Episode / Step: 2137 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "420 Episode / Step: 2176 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "430 Episode / Step: 2212 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "440 Episode / Step: 2244 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "450 Episode / Step: 2273 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "460 Episode / Step: 2313 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "470 Episode / Step: 2354 / Score: 0.77 / Loss: nan / Epsilon: 0.0500\n",
      "480 Episode / Step: 2385 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "490 Episode / Step: 2426 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "500 Episode / Step: 2452 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "510 Episode / Step: 2490 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "520 Episode / Step: 2519 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "530 Episode / Step: 2552 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "540 Episode / Step: 2587 / Score: 0.78 / Loss: nan / Epsilon: 0.0500\n",
      "550 Episode / Step: 2633 / Score: 0.96 / Loss: nan / Epsilon: 0.0500\n",
      "560 Episode / Step: 2663 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "570 Episode / Step: 2696 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "580 Episode / Step: 2736 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "590 Episode / Step: 2775 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "600 Episode / Step: 2887 / Score: 0.70 / Loss: nan / Epsilon: 0.0500\n",
      "610 Episode / Step: 2920 / Score: 0.78 / Loss: nan / Epsilon: 0.0500\n",
      "620 Episode / Step: 3056 / Score: 0.77 / Loss: nan / Epsilon: 0.0500\n",
      "630 Episode / Step: 3085 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "640 Episode / Step: 3220 / Score: 0.68 / Loss: nan / Epsilon: 0.0500\n",
      "650 Episode / Step: 3242 / Score: 0.99 / Loss: nan / Epsilon: 0.0500\n",
      "660 Episode / Step: 3272 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "670 Episode / Step: 3309 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "680 Episode / Step: 3350 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "690 Episode / Step: 3439 / Score: 0.92 / Loss: nan / Epsilon: 0.0500\n",
      "700 Episode / Step: 3484 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "710 Episode / Step: 3521 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "720 Episode / Step: 3552 / Score: 0.78 / Loss: nan / Epsilon: 0.0500\n",
      "730 Episode / Step: 3581 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "740 Episode / Step: 3620 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "750 Episode / Step: 3652 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "760 Episode / Step: 3683 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "770 Episode / Step: 3709 / Score: 0.78 / Loss: nan / Epsilon: 0.0500\n",
      "780 Episode / Step: 3736 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "790 Episode / Step: 3764 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "800 Episode / Step: 3823 / Score: 0.75 / Loss: nan / Epsilon: 0.0500\n",
      "810 Episode / Step: 3852 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "820 Episode / Step: 3887 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "830 Episode / Step: 3922 / Score: 0.78 / Loss: nan / Epsilon: 0.0500\n",
      "840 Episode / Step: 3958 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "850 Episode / Step: 3985 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "860 Episode / Step: 4023 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "870 Episode / Step: 4051 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "880 Episode / Step: 4093 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "890 Episode / Step: 4164 / Score: 0.74 / Loss: nan / Epsilon: 0.0500\n",
      "900 Episode / Step: 4198 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "910 Episode / Step: 4239 / Score: 0.77 / Loss: nan / Epsilon: 0.0500\n",
      "920 Episode / Step: 4280 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "930 Episode / Step: 4308 / Score: 0.78 / Loss: nan / Epsilon: 0.0500\n",
      "940 Episode / Step: 4344 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "950 Episode / Step: 4374 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "960 Episode / Step: 4405 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "970 Episode / Step: 4437 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "980 Episode / Step: 4484 / Score: 0.96 / Loss: nan / Epsilon: 0.0500\n",
      "990 Episode / Step: 4519 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "1000 Episode / Step: 4546 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "1010 Episode / Step: 4574 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "1020 Episode / Step: 4607 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "1030 Episode / Step: 4711 / Score: 0.91 / Loss: nan / Epsilon: 0.0500\n",
      "1040 Episode / Step: 4751 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "1050 Episode / Step: 4786 / Score: 0.78 / Loss: nan / Epsilon: 0.0500\n",
      "1060 Episode / Step: 4819 / Score: 0.98 / Loss: nan / Epsilon: 0.0500\n",
      "1070 Episode / Step: 4857 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "1080 Episode / Step: 4900 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "1090 Episode / Step: 4943 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n",
      "1100 Episode / Step: 4980 / Score: 0.97 / Loss: nan / Epsilon: 0.0500\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 불러오기\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import datetime\n",
    "import platform\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque\n",
    "from mlagents_envs.environment import UnityEnvironment, ActionTuple\n",
    "from mlagents_envs.side_channel.engine_configuration_channel\\\n",
    "                             import EngineConfigurationChannel\n",
    "\n",
    "# DQN을 위한 파라미터 값 세팅 \n",
    "state_size = [3*2, 64, 84]\n",
    "action_size = 4 \n",
    "\n",
    "load_model = True\n",
    "train_mode = False\n",
    "\n",
    "batch_size = 32\n",
    "mem_maxlen = 10000\n",
    "discount_factor = 0.9\n",
    "learning_rate = 0.00025\n",
    "\n",
    "run_step = 50000 if train_mode else 0\n",
    "test_step = 5000\n",
    "train_start_step = 5000\n",
    "target_update_step = 500\n",
    "\n",
    "print_interval = 10\n",
    "save_interval = 100\n",
    "\n",
    "epsilon_eval = 0.05\n",
    "epsilon_init = 1.0 if train_mode else epsilon_eval\n",
    "epsilon_min = 0.1\n",
    "explore_step = run_step * 0.8\n",
    "eplsilon_delta = (epsilon_init - epsilon_min)/explore_step if train_mode else 0.\n",
    "\n",
    "VISUAL_OBS = 0\n",
    "GOAL_OBS = 1\n",
    "VECTOR_OBS = 2\n",
    "OBS = VISUAL_OBS\n",
    "\n",
    "# 유니티 환경 경로 \n",
    "game = \"GridWorld\"\n",
    "env_name = \"./Unity_practice/GridWorld/GridWorld\"\n",
    "\n",
    "# 모델 저장 및 불러오기 경로\n",
    "date_time = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "save_path = f\"./Unity_practice/saved_models/{game}/DQN/{date_time}\"\n",
    "load_path = f\"./Unity_practice/saved_models/{game}/DQN/20240220164701\"\n",
    "\n",
    "# 연산 장치\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# DQN 클래스 -> Deep Q Network 정의 \n",
    "class DQN(torch.nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(DQN, self).__init__(**kwargs)\n",
    "        self.conv1 = torch.nn.Conv2d(in_channels=state_size[0], out_channels=32,\n",
    "                                     kernel_size=8, stride=4)\n",
    "        dim1 = ((state_size[1] - 8)//4 + 1, (state_size[2] - 8)//4 + 1)\n",
    "        self.conv2 = torch.nn.Conv2d(in_channels=32, out_channels=64,\n",
    "                                     kernel_size=4, stride=2)\n",
    "        dim2 = ((dim1[0] - 4)//2 + 1, (dim1[1] - 4)//2 + 1)\n",
    "        self.conv3 = torch.nn.Conv2d(in_channels=64, out_channels=64,\n",
    "                                     kernel_size=3, stride=1)\n",
    "        dim3 = ((dim2[0] - 3)//1 + 1, (dim2[1] - 3)//1 + 1)\n",
    "\n",
    "        self.flat = torch.nn.Flatten()\n",
    "        self.fc1 = torch.nn.Linear(64*dim3[0]*dim3[1], 512)\n",
    "        self.q = torch.nn.Linear(512, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.flat(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.q(x)\n",
    "\n",
    "# DQNAgent 클래스 -> DQN 알고리즘을 위한 다양한 함수 정의 \n",
    "class DQNAgent:\n",
    "    def __init__(self):\n",
    "        self.network = DQN().to(device)\n",
    "        self.target_network = copy.deepcopy(self.network)\n",
    "        self.optimizer = torch.optim.Adam(self.network.parameters(), lr=learning_rate)\n",
    "        self.memory = deque(maxlen=mem_maxlen)\n",
    "        self.epsilon = epsilon_init\n",
    "        self.writer = SummaryWriter(save_path)\n",
    "\n",
    "        if load_model == True:\n",
    "            print(f\"... Load Model from {load_path}/ckpt ...\")\n",
    "            checkpoint = torch.load(load_path+'/ckpt', map_location=device)\n",
    "            self.network.load_state_dict(checkpoint[\"network\"])\n",
    "            self.target_network.load_state_dict(checkpoint[\"network\"])\n",
    "            self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "        \n",
    "    # Epsilon greedy 기법에 따라 행동 결정 \n",
    "    def get_action(self, state, training=True):\n",
    "        #  네트워크 모드 설정\n",
    "        self.network.train(training)\n",
    "        epsilon = self.epsilon if training else epsilon_eval\n",
    "\n",
    "        # 랜덤하게 행동 결정\n",
    "        if epsilon > random.random():  \n",
    "            action = np.random.randint(0, action_size, size=(state.shape[0],1))\n",
    "        # 네트워크 연산에 따라 행동 결정\n",
    "        else:\n",
    "            q = self.network(torch.FloatTensor(state).to(device))\n",
    "            action = torch.argmax(q, axis=-1, keepdim=True).data.cpu().numpy()\n",
    "        return action\n",
    "\n",
    "    # 리플레이 메모리에 데이터 추가 (상태, 행동, 보상, 다음 상태, 게임 종료 여부)\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    # 학습 수행\n",
    "    def train_model(self):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        state      = np.stack([b[0] for b in batch], axis=0)\n",
    "        action     = np.stack([b[1] for b in batch], axis=0)\n",
    "        reward     = np.stack([b[2] for b in batch], axis=0)\n",
    "        next_state = np.stack([b[3] for b in batch], axis=0)\n",
    "        done       = np.stack([b[4] for b in batch], axis=0)\n",
    "\n",
    "        state, action, reward, next_state, done = map(lambda x: torch.FloatTensor(x).to(device),\n",
    "                                                        [state, action, reward, next_state, done])\n",
    "\n",
    "        eye = torch.eye(action_size).to(device)\n",
    "        one_hot_action = eye[action.view(-1).long()]\n",
    "        q = (self.network(state) * one_hot_action).sum(1, keepdims=True)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_q = self.target_network(next_state)\n",
    "            target_q = reward + next_q.max(1, keepdims=True).values * ((1 - done) * discount_factor)\n",
    "\n",
    "        loss = F.smooth_l1_loss(q, target_q)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # 엡실론 감소\n",
    "        self.epsilon = max(epsilon_min, self.epsilon - eplsilon_delta)\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    # 타겟 네트워크 업데이트\n",
    "    def update_target(self):\n",
    "        self.target_network.load_state_dict(self.network.state_dict())\n",
    "\n",
    "    # 네트워크 모델 저장 \n",
    "    def save_model(self):\n",
    "        print(f\"... Save Model to {save_path}/ckpt ...\")\n",
    "        torch.save({\n",
    "            \"network\" : self.network.state_dict(),\n",
    "            \"optimizer\" : self.optimizer.state_dict(),\n",
    "        }, save_path+'/ckpt')\n",
    "\n",
    "    # 학습 기록 \n",
    "    def write_summray(self, score, loss, epsilon, step):\n",
    "        self.writer.add_scalar(\"run/score\", score, step)\n",
    "        self.writer.add_scalar(\"model/loss\", loss, step)\n",
    "        self.writer.add_scalar(\"model/epsilon\", epsilon, step)\n",
    "\n",
    "# Main 함수 -> 전체적으로 DQN 알고리즘을 진행 \n",
    "if __name__ == '__main__':\n",
    "    # 유니티 환경 경로 설정 (file_name)\n",
    "    engine_configuration_channel = EngineConfigurationChannel()\n",
    "    env = UnityEnvironment(file_name=env_name,\n",
    "                           side_channels=[engine_configuration_channel],\n",
    "                           worker_id=101\n",
    "                           )\n",
    "    env.reset()\n",
    "\n",
    "    # 유니티 브레인 설정 \n",
    "    behavior_name = list(env.behavior_specs.keys())[0]\n",
    "    spec = env.behavior_specs[behavior_name]\n",
    "    engine_configuration_channel.set_configuration_parameters(time_scale=12.0)\n",
    "    dec, term = env.get_steps(behavior_name)\n",
    "\n",
    "    # DQNAgent 클래스를 agent로 정의 \n",
    "    agent = DQNAgent()\n",
    "    \n",
    "    losses, scores, episode, score = [], [], 0, 0\n",
    "    for step in range(run_step + test_step):\n",
    "        if step == run_step:\n",
    "            if train_mode:\n",
    "                agent.save_model()\n",
    "            print(\"TEST START\")\n",
    "            train_mode = False\n",
    "            engine_configuration_channel.set_configuration_parameters(time_scale=1.0)\n",
    "\n",
    "        preprocess = lambda obs, goal: np.concatenate((obs*goal[0][0], obs*goal[0][1]), axis=-1) \n",
    "        state = preprocess(dec.obs[OBS],dec.obs[GOAL_OBS])\n",
    "        action = agent.get_action(state, train_mode)\n",
    "        real_action = action + 1\n",
    "        action_tuple = ActionTuple()\n",
    "        action_tuple.add_discrete(real_action)\n",
    "        env.set_actions(behavior_name, action_tuple)\n",
    "        env.step()\n",
    "\n",
    "        dec, term = env.get_steps(behavior_name)\n",
    "        done = len(term.agent_id) > 0\n",
    "        reward = term.reward if done else dec.reward\n",
    "        next_state = preprocess(term.obs[OBS], term.obs[GOAL_OBS]) if done\\\n",
    "                     else preprocess(dec.obs[OBS], dec.obs[GOAL_OBS])\n",
    "        score += reward[0]\n",
    "\n",
    "        if train_mode:\n",
    "            agent.append_sample(state[0], action[0], reward, next_state[0], [done])\n",
    "\n",
    "        if train_mode and step > max(batch_size, train_start_step):\n",
    "            # 학습 수행\n",
    "            loss = agent.train_model()\n",
    "            losses.append(loss)\n",
    "\n",
    "            # 타겟 네트워크 업데이트 \n",
    "            if step % target_update_step == 0:\n",
    "                agent.update_target()\n",
    "\n",
    "        if done:\n",
    "            episode +=1\n",
    "            scores.append(score)\n",
    "            score = 0\n",
    "\n",
    "            # 게임 진행 상황 출력 및 텐서 보드에 보상과 손실함수 값 기록 \n",
    "            if episode % print_interval == 0:\n",
    "                mean_score = np.mean(scores)\n",
    "                mean_loss = np.mean(losses)\n",
    "                agent.write_summray(mean_score, mean_loss, agent.epsilon, step)\n",
    "                losses, scores = [], []\n",
    "\n",
    "                print(f\"{episode} Episode / Step: {step} / Score: {mean_score:.2f} / \" +\\\n",
    "                      f\"Loss: {mean_loss:.4f} / Epsilon: {agent.epsilon:.4f}\")\n",
    "\n",
    "            # 네트워크 모델 저장 \n",
    "            if train_mode and episode % save_interval == 0:\n",
    "                agent.save_model()\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A2C Training\n",
    "======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Episode / Step: 170 / Score: 0.24 / Actor loss: 0.03 / Critic loss: 0.0582\n",
      "20 Episode / Step: 418 / Score: -0.24 / Actor loss: -0.01 / Critic loss: 0.0408\n",
      "30 Episode / Step: 865 / Score: -0.74 / Actor loss: -0.01 / Critic loss: 0.0178\n",
      "40 Episode / Step: 1140 / Score: 0.43 / Actor loss: 0.00 / Critic loss: 0.0327\n",
      "50 Episode / Step: 1421 / Score: -0.47 / Actor loss: -0.00 / Critic loss: 0.0381\n",
      "60 Episode / Step: 1795 / Score: -0.46 / Actor loss: -0.00 / Critic loss: 0.0238\n",
      "70 Episode / Step: 2391 / Score: -0.69 / Actor loss: -0.00 / Critic loss: 0.0080\n",
      "80 Episode / Step: 2597 / Score: -0.10 / Actor loss: 0.02 / Critic loss: 0.0362\n",
      "90 Episode / Step: 2935 / Score: -0.73 / Actor loss: -0.01 / Critic loss: 0.0341\n",
      "100 Episode / Step: 3405 / Score: -0.56 / Actor loss: 0.01 / Critic loss: 0.0131\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "110 Episode / Step: 4118 / Score: -0.51 / Actor loss: 0.00 / Critic loss: 0.0055\n",
      "120 Episode / Step: 4701 / Score: -0.58 / Actor loss: -0.00 / Critic loss: 0.0107\n",
      "130 Episode / Step: 4981 / Score: -0.27 / Actor loss: 0.01 / Critic loss: 0.0280\n",
      "140 Episode / Step: 5415 / Score: -0.33 / Actor loss: 0.01 / Critic loss: 0.0150\n",
      "150 Episode / Step: 5766 / Score: -0.64 / Actor loss: -0.01 / Critic loss: 0.0236\n",
      "160 Episode / Step: 6348 / Score: 0.02 / Actor loss: 0.01 / Critic loss: 0.0105\n",
      "170 Episode / Step: 6659 / Score: -0.30 / Actor loss: -0.01 / Critic loss: 0.0217\n",
      "180 Episode / Step: 6926 / Score: 0.34 / Actor loss: 0.02 / Critic loss: 0.0300\n",
      "190 Episode / Step: 7372 / Score: -0.24 / Actor loss: 0.00 / Critic loss: 0.0176\n",
      "200 Episode / Step: 7692 / Score: 0.19 / Actor loss: 0.00 / Critic loss: 0.0238\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "210 Episode / Step: 8216 / Score: -0.62 / Actor loss: -0.01 / Critic loss: 0.0116\n",
      "220 Episode / Step: 8865 / Score: -0.54 / Actor loss: -0.00 / Critic loss: 0.0071\n",
      "230 Episode / Step: 9241 / Score: -0.47 / Actor loss: -0.01 / Critic loss: 0.0185\n",
      "240 Episode / Step: 9543 / Score: 0.21 / Actor loss: 0.00 / Critic loss: 0.0286\n",
      "250 Episode / Step: 9927 / Score: 0.12 / Actor loss: -0.01 / Critic loss: 0.0133\n",
      "260 Episode / Step: 10077 / Score: 0.06 / Actor loss: -0.01 / Critic loss: 0.0595\n",
      "270 Episode / Step: 10254 / Score: 0.13 / Actor loss: 0.02 / Critic loss: 0.0379\n",
      "280 Episode / Step: 10487 / Score: 0.48 / Actor loss: 0.01 / Critic loss: 0.0261\n",
      "290 Episode / Step: 10805 / Score: 0.19 / Actor loss: 0.00 / Critic loss: 0.0174\n",
      "300 Episode / Step: 11026 / Score: 0.29 / Actor loss: 0.01 / Critic loss: 0.0356\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "310 Episode / Step: 11374 / Score: -0.04 / Actor loss: -0.01 / Critic loss: 0.0239\n",
      "320 Episode / Step: 11465 / Score: 0.32 / Actor loss: 0.03 / Critic loss: 0.1149\n",
      "330 Episode / Step: 11835 / Score: 0.04 / Actor loss: 0.00 / Critic loss: 0.0169\n",
      "340 Episode / Step: 12044 / Score: 0.20 / Actor loss: -0.01 / Critic loss: 0.0467\n",
      "350 Episode / Step: 12148 / Score: 0.31 / Actor loss: -0.02 / Critic loss: 0.0903\n",
      "360 Episode / Step: 12310 / Score: 0.05 / Actor loss: -0.00 / Critic loss: 0.0570\n",
      "370 Episode / Step: 12421 / Score: 0.50 / Actor loss: 0.04 / Critic loss: 0.0843\n",
      "380 Episode / Step: 12571 / Score: 0.46 / Actor loss: -0.01 / Critic loss: 0.0550\n",
      "390 Episode / Step: 12685 / Score: 0.70 / Actor loss: 0.01 / Critic loss: 0.0594\n",
      "400 Episode / Step: 12765 / Score: 0.33 / Actor loss: -0.04 / Critic loss: 0.0854\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "410 Episode / Step: 12852 / Score: 0.72 / Actor loss: 0.01 / Critic loss: 0.0721\n",
      "420 Episode / Step: 12910 / Score: -0.05 / Actor loss: -0.06 / Critic loss: 0.1531\n",
      "430 Episode / Step: 13120 / Score: 0.70 / Actor loss: 0.02 / Critic loss: 0.0299\n",
      "440 Episode / Step: 13202 / Score: 0.73 / Actor loss: 0.01 / Critic loss: 0.0661\n",
      "450 Episode / Step: 13283 / Score: 0.73 / Actor loss: -0.02 / Critic loss: 0.0750\n",
      "460 Episode / Step: 13409 / Score: 0.48 / Actor loss: -0.02 / Critic loss: 0.0480\n",
      "470 Episode / Step: 13489 / Score: 0.33 / Actor loss: -0.04 / Critic loss: 0.1004\n",
      "480 Episode / Step: 13586 / Score: 0.71 / Actor loss: 0.02 / Critic loss: 0.0676\n",
      "490 Episode / Step: 13656 / Score: 0.34 / Actor loss: -0.03 / Critic loss: 0.1071\n",
      "500 Episode / Step: 13728 / Score: 0.74 / Actor loss: -0.02 / Critic loss: 0.0799\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "510 Episode / Step: 13824 / Score: 0.71 / Actor loss: -0.03 / Critic loss: 0.0484\n",
      "520 Episode / Step: 13887 / Score: 0.95 / Actor loss: 0.03 / Critic loss: 0.0581\n",
      "530 Episode / Step: 13954 / Score: 0.74 / Actor loss: -0.02 / Critic loss: 0.0606\n",
      "540 Episode / Step: 14034 / Score: 0.73 / Actor loss: -0.06 / Critic loss: 0.0574\n",
      "550 Episode / Step: 14163 / Score: 0.48 / Actor loss: -0.03 / Critic loss: 0.0576\n",
      "560 Episode / Step: 14224 / Score: 0.75 / Actor loss: 0.00 / Critic loss: 0.0735\n",
      "570 Episode / Step: 14286 / Score: 0.55 / Actor loss: -0.03 / Critic loss: 0.0964\n",
      "580 Episode / Step: 14350 / Score: 0.55 / Actor loss: -0.03 / Critic loss: 0.0984\n",
      "590 Episode / Step: 14399 / Score: 0.16 / Actor loss: -0.13 / Critic loss: 0.1569\n",
      "600 Episode / Step: 14540 / Score: 0.07 / Actor loss: -0.01 / Critic loss: 0.0571\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "610 Episode / Step: 14612 / Score: 0.74 / Actor loss: 0.03 / Critic loss: 0.0882\n",
      "620 Episode / Step: 14648 / Score: 0.57 / Actor loss: 0.03 / Critic loss: 0.1417\n",
      "630 Episode / Step: 14723 / Score: 0.74 / Actor loss: -0.01 / Critic loss: 0.0541\n",
      "640 Episode / Step: 14787 / Score: 0.55 / Actor loss: -0.01 / Critic loss: 0.1087\n",
      "650 Episode / Step: 14820 / Score: 0.78 / Actor loss: 0.03 / Critic loss: 0.1146\n",
      "660 Episode / Step: 14853 / Score: 0.38 / Actor loss: -0.09 / Critic loss: 0.2180\n",
      "670 Episode / Step: 14906 / Score: 0.76 / Actor loss: -0.02 / Critic loss: 0.0792\n",
      "680 Episode / Step: 14963 / Score: 0.75 / Actor loss: -0.04 / Critic loss: 0.0629\n",
      "690 Episode / Step: 15024 / Score: 0.95 / Actor loss: -0.01 / Critic loss: 0.0388\n",
      "700 Episode / Step: 15086 / Score: 0.55 / Actor loss: -0.04 / Critic loss: 0.1078\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "710 Episode / Step: 15124 / Score: 0.97 / Actor loss: 0.03 / Critic loss: 0.0310\n",
      "720 Episode / Step: 15216 / Score: 0.72 / Actor loss: -0.05 / Critic loss: 0.0443\n",
      "730 Episode / Step: 15258 / Score: 0.77 / Actor loss: 0.01 / Critic loss: 0.1006\n",
      "740 Episode / Step: 15317 / Score: 0.35 / Actor loss: -0.09 / Critic loss: 0.1328\n",
      "750 Episode / Step: 15371 / Score: 0.76 / Actor loss: 0.01 / Critic loss: 0.0952\n",
      "760 Episode / Step: 15433 / Score: 0.75 / Actor loss: -0.02 / Critic loss: 0.0680\n",
      "770 Episode / Step: 15462 / Score: 0.38 / Actor loss: -0.06 / Critic loss: 0.2462\n",
      "780 Episode / Step: 15514 / Score: 0.76 / Actor loss: -0.04 / Critic loss: 0.0877\n",
      "790 Episode / Step: 15569 / Score: 0.76 / Actor loss: -0.04 / Critic loss: 0.0750\n",
      "800 Episode / Step: 15610 / Score: 0.17 / Actor loss: -0.08 / Critic loss: 0.2081\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "810 Episode / Step: 15661 / Score: 0.76 / Actor loss: -0.02 / Critic loss: 0.0867\n",
      "820 Episode / Step: 15704 / Score: 0.57 / Actor loss: -0.00 / Critic loss: 0.1396\n",
      "830 Episode / Step: 15742 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0850\n",
      "840 Episode / Step: 15789 / Score: 0.76 / Actor loss: -0.02 / Critic loss: 0.0818\n",
      "850 Episode / Step: 15826 / Score: 0.97 / Actor loss: 0.01 / Critic loss: 0.0462\n",
      "860 Episode / Step: 15877 / Score: 0.96 / Actor loss: -0.02 / Critic loss: 0.0263\n",
      "870 Episode / Step: 15915 / Score: 0.77 / Actor loss: -0.07 / Critic loss: 0.0904\n",
      "880 Episode / Step: 15963 / Score: 0.96 / Actor loss: 0.00 / Critic loss: 0.0309\n",
      "890 Episode / Step: 16014 / Score: 0.76 / Actor loss: -0.06 / Critic loss: 0.0663\n",
      "900 Episode / Step: 16061 / Score: 0.96 / Actor loss: -0.03 / Critic loss: 0.0313\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "910 Episode / Step: 16113 / Score: 0.76 / Actor loss: -0.06 / Critic loss: 0.0919\n",
      "920 Episode / Step: 16146 / Score: 0.58 / Actor loss: -0.10 / Critic loss: 0.1707\n",
      "930 Episode / Step: 16208 / Score: 0.75 / Actor loss: 0.00 / Critic loss: 0.0677\n",
      "940 Episode / Step: 16270 / Score: 0.75 / Actor loss: -0.03 / Critic loss: 0.0688\n",
      "950 Episode / Step: 16323 / Score: 0.76 / Actor loss: -0.05 / Critic loss: 0.0894\n",
      "960 Episode / Step: 16391 / Score: 0.94 / Actor loss: -0.03 / Critic loss: 0.0215\n",
      "970 Episode / Step: 16437 / Score: 0.96 / Actor loss: -0.02 / Critic loss: 0.0273\n",
      "980 Episode / Step: 16486 / Score: 0.76 / Actor loss: -0.01 / Critic loss: 0.0551\n",
      "990 Episode / Step: 16526 / Score: 0.77 / Actor loss: -0.00 / Critic loss: 0.1003\n",
      "1000 Episode / Step: 16566 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0460\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "1010 Episode / Step: 16601 / Score: 0.78 / Actor loss: -0.06 / Critic loss: 0.0959\n",
      "1020 Episode / Step: 16640 / Score: 0.97 / Actor loss: -0.03 / Critic loss: 0.0183\n",
      "1030 Episode / Step: 16687 / Score: 0.56 / Actor loss: -0.05 / Critic loss: 0.1164\n",
      "1040 Episode / Step: 16746 / Score: 0.75 / Actor loss: -0.01 / Critic loss: 0.0672\n",
      "1050 Episode / Step: 16782 / Score: 0.77 / Actor loss: -0.02 / Critic loss: 0.0931\n",
      "1060 Episode / Step: 16825 / Score: 0.97 / Actor loss: -0.02 / Critic loss: 0.0260\n",
      "1070 Episode / Step: 16865 / Score: 0.77 / Actor loss: -0.02 / Critic loss: 0.0648\n",
      "1080 Episode / Step: 16919 / Score: 0.76 / Actor loss: -0.02 / Critic loss: 0.0508\n",
      "1090 Episode / Step: 16968 / Score: 0.76 / Actor loss: -0.02 / Critic loss: 0.0885\n",
      "1100 Episode / Step: 17026 / Score: 0.95 / Actor loss: -0.01 / Critic loss: 0.0336\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "1110 Episode / Step: 17059 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0195\n",
      "1120 Episode / Step: 17103 / Score: 0.77 / Actor loss: -0.02 / Critic loss: 0.0791\n",
      "1130 Episode / Step: 17150 / Score: 0.76 / Actor loss: -0.12 / Critic loss: 0.0584\n",
      "1140 Episode / Step: 17194 / Score: 0.77 / Actor loss: -0.04 / Critic loss: 0.0711\n",
      "1150 Episode / Step: 17235 / Score: 0.97 / Actor loss: -0.00 / Critic loss: 0.0247\n",
      "1160 Episode / Step: 17274 / Score: 0.37 / Actor loss: -0.04 / Critic loss: 0.2116\n",
      "1170 Episode / Step: 17313 / Score: 0.97 / Actor loss: 0.02 / Critic loss: 0.0228\n",
      "1180 Episode / Step: 17348 / Score: 0.98 / Actor loss: -0.05 / Critic loss: 0.0330\n",
      "1190 Episode / Step: 17395 / Score: 0.56 / Actor loss: -0.03 / Critic loss: 0.1073\n",
      "1200 Episode / Step: 17441 / Score: 0.96 / Actor loss: -0.02 / Critic loss: 0.0388\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "1210 Episode / Step: 17470 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0971\n",
      "1220 Episode / Step: 17514 / Score: 0.97 / Actor loss: -0.03 / Critic loss: 0.0204\n",
      "1230 Episode / Step: 17544 / Score: 0.78 / Actor loss: -0.06 / Critic loss: 0.1155\n",
      "1240 Episode / Step: 17579 / Score: 0.58 / Actor loss: 0.03 / Critic loss: 0.1978\n",
      "1250 Episode / Step: 17624 / Score: 0.97 / Actor loss: -0.02 / Critic loss: 0.0350\n",
      "1260 Episode / Step: 17673 / Score: 0.96 / Actor loss: -0.03 / Critic loss: 0.0238\n",
      "1270 Episode / Step: 17723 / Score: 0.76 / Actor loss: -0.02 / Critic loss: 0.0538\n",
      "1280 Episode / Step: 17765 / Score: 0.97 / Actor loss: -0.04 / Critic loss: 0.0280\n",
      "1290 Episode / Step: 17813 / Score: 0.76 / Actor loss: -0.03 / Critic loss: 0.0606\n",
      "1300 Episode / Step: 17852 / Score: 0.77 / Actor loss: -0.02 / Critic loss: 0.0739\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "1310 Episode / Step: 17892 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0195\n",
      "1320 Episode / Step: 17938 / Score: 0.76 / Actor loss: -0.08 / Critic loss: 0.0670\n",
      "1330 Episode / Step: 17972 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0700\n",
      "1340 Episode / Step: 18016 / Score: 0.97 / Actor loss: -0.03 / Critic loss: 0.0184\n",
      "1350 Episode / Step: 18059 / Score: 0.77 / Actor loss: -0.04 / Critic loss: 0.0807\n",
      "1360 Episode / Step: 18103 / Score: 0.77 / Actor loss: -0.03 / Critic loss: 0.0654\n",
      "1370 Episode / Step: 18140 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0931\n",
      "1380 Episode / Step: 18174 / Score: 0.78 / Actor loss: -0.04 / Critic loss: 0.0993\n",
      "1390 Episode / Step: 18209 / Score: 0.98 / Actor loss: -0.05 / Critic loss: 0.0196\n",
      "1400 Episode / Step: 18255 / Score: 0.76 / Actor loss: -0.04 / Critic loss: 0.0472\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "1410 Episode / Step: 18302 / Score: 0.36 / Actor loss: -0.08 / Critic loss: 0.1304\n",
      "1420 Episode / Step: 18330 / Score: 0.58 / Actor loss: -0.00 / Critic loss: 0.1845\n",
      "1430 Episode / Step: 18360 / Score: 0.78 / Actor loss: 0.03 / Critic loss: 0.1272\n",
      "1440 Episode / Step: 18398 / Score: 0.37 / Actor loss: -0.30 / Critic loss: 0.1739\n",
      "1450 Episode / Step: 18437 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0417\n",
      "1460 Episode / Step: 18486 / Score: 0.96 / Actor loss: -0.02 / Critic loss: 0.0180\n",
      "1470 Episode / Step: 18525 / Score: 0.57 / Actor loss: -0.04 / Critic loss: 0.1020\n",
      "1480 Episode / Step: 18566 / Score: 0.97 / Actor loss: -0.04 / Critic loss: 0.0169\n",
      "1490 Episode / Step: 18599 / Score: 0.38 / Actor loss: -0.02 / Critic loss: 0.2044\n",
      "1500 Episode / Step: 18633 / Score: 0.98 / Actor loss: -0.00 / Critic loss: 0.0301\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "1510 Episode / Step: 18670 / Score: 0.97 / Actor loss: -0.04 / Critic loss: 0.0154\n",
      "1520 Episode / Step: 18709 / Score: 0.97 / Actor loss: -0.03 / Critic loss: 0.0260\n",
      "1530 Episode / Step: 18751 / Score: 0.57 / Actor loss: -0.04 / Critic loss: 0.1094\n",
      "1540 Episode / Step: 18787 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0421\n",
      "1550 Episode / Step: 18820 / Score: 0.98 / Actor loss: -0.04 / Critic loss: 0.0246\n",
      "1560 Episode / Step: 18854 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0107\n",
      "1570 Episode / Step: 18896 / Score: 0.77 / Actor loss: -0.06 / Critic loss: 0.0828\n",
      "1580 Episode / Step: 18927 / Score: 0.98 / Actor loss: -0.03 / Critic loss: 0.0109\n",
      "1590 Episode / Step: 18975 / Score: 0.96 / Actor loss: -0.02 / Critic loss: 0.0099\n",
      "1600 Episode / Step: 19015 / Score: 0.97 / Actor loss: -0.06 / Critic loss: 0.0255\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "1610 Episode / Step: 19045 / Score: 0.38 / Actor loss: -0.13 / Critic loss: 0.2192\n",
      "1620 Episode / Step: 19079 / Score: 0.98 / Actor loss: 0.02 / Critic loss: 0.0368\n",
      "1630 Episode / Step: 19132 / Score: 0.76 / Actor loss: -0.05 / Critic loss: 0.0763\n",
      "1640 Episode / Step: 19182 / Score: 0.76 / Actor loss: -0.03 / Critic loss: 0.0669\n",
      "1650 Episode / Step: 19215 / Score: 0.78 / Actor loss: -0.03 / Critic loss: 0.0962\n",
      "1660 Episode / Step: 19260 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0304\n",
      "1670 Episode / Step: 19299 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.1037\n",
      "1680 Episode / Step: 19333 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.1160\n",
      "1690 Episode / Step: 19362 / Score: 0.78 / Actor loss: -0.02 / Critic loss: 0.1034\n",
      "1700 Episode / Step: 19396 / Score: 0.78 / Actor loss: -0.02 / Critic loss: 0.0689\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "1710 Episode / Step: 19426 / Score: 0.78 / Actor loss: 0.01 / Critic loss: 0.0978\n",
      "1720 Episode / Step: 19461 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0215\n",
      "1730 Episode / Step: 19495 / Score: 0.38 / Actor loss: -0.04 / Critic loss: 0.2119\n",
      "1740 Episode / Step: 19519 / Score: 0.79 / Actor loss: 0.03 / Critic loss: 0.1411\n",
      "1750 Episode / Step: 19549 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0343\n",
      "1760 Episode / Step: 19585 / Score: 0.97 / Actor loss: -0.03 / Critic loss: 0.0256\n",
      "1770 Episode / Step: 19617 / Score: 0.58 / Actor loss: -0.04 / Critic loss: 0.1714\n",
      "1780 Episode / Step: 19658 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0676\n",
      "1790 Episode / Step: 19694 / Score: 0.57 / Actor loss: -0.02 / Critic loss: 0.1147\n",
      "1800 Episode / Step: 19738 / Score: 0.77 / Actor loss: -0.06 / Critic loss: 0.0925\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "1810 Episode / Step: 19793 / Score: 0.76 / Actor loss: -0.02 / Critic loss: 0.0470\n",
      "1820 Episode / Step: 19833 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0157\n",
      "1830 Episode / Step: 19870 / Score: 0.97 / Actor loss: -0.03 / Critic loss: 0.0271\n",
      "1840 Episode / Step: 19896 / Score: 0.78 / Actor loss: 0.01 / Critic loss: 0.1012\n",
      "1850 Episode / Step: 19928 / Score: 0.58 / Actor loss: -0.09 / Critic loss: 0.1647\n",
      "1860 Episode / Step: 19964 / Score: 0.77 / Actor loss: -0.05 / Critic loss: 0.0737\n",
      "1870 Episode / Step: 19997 / Score: 0.58 / Actor loss: 0.01 / Critic loss: 0.1176\n",
      "1880 Episode / Step: 20048 / Score: 0.96 / Actor loss: -0.04 / Critic loss: 0.0132\n",
      "1890 Episode / Step: 20104 / Score: 0.55 / Actor loss: -0.03 / Critic loss: 0.0882\n",
      "1900 Episode / Step: 20142 / Score: 0.57 / Actor loss: -0.09 / Critic loss: 0.1302\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "1910 Episode / Step: 20171 / Score: 0.38 / Actor loss: -0.07 / Critic loss: 0.2396\n",
      "1920 Episode / Step: 20202 / Score: 0.38 / Actor loss: 0.00 / Critic loss: 0.2033\n",
      "1930 Episode / Step: 20258 / Score: 0.95 / Actor loss: -0.03 / Critic loss: 0.0292\n",
      "1940 Episode / Step: 20295 / Score: 0.77 / Actor loss: -0.04 / Critic loss: 0.0891\n",
      "1950 Episode / Step: 20330 / Score: 0.98 / Actor loss: 0.01 / Critic loss: 0.0168\n",
      "1960 Episode / Step: 20373 / Score: 0.97 / Actor loss: -0.05 / Critic loss: 0.0182\n",
      "1970 Episode / Step: 20399 / Score: 0.58 / Actor loss: -0.09 / Critic loss: 0.2023\n",
      "1980 Episode / Step: 20437 / Score: 0.57 / Actor loss: -0.03 / Critic loss: 0.1317\n",
      "1990 Episode / Step: 20466 / Score: 0.98 / Actor loss: 0.01 / Critic loss: 0.0406\n",
      "2000 Episode / Step: 20522 / Score: 0.75 / Actor loss: -0.08 / Critic loss: 0.0446\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "2010 Episode / Step: 20559 / Score: 0.97 / Actor loss: -0.03 / Critic loss: 0.0157\n",
      "2020 Episode / Step: 20601 / Score: 0.97 / Actor loss: -0.08 / Critic loss: 0.0128\n",
      "2030 Episode / Step: 20644 / Score: 0.37 / Actor loss: -0.07 / Critic loss: 0.2714\n",
      "2040 Episode / Step: 20693 / Score: 0.56 / Actor loss: 0.03 / Critic loss: 0.1149\n",
      "2050 Episode / Step: 20735 / Score: 0.57 / Actor loss: -0.06 / Critic loss: 0.1430\n",
      "2060 Episode / Step: 20766 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0285\n",
      "2070 Episode / Step: 20797 / Score: 0.38 / Actor loss: -0.04 / Critic loss: 0.2483\n",
      "2080 Episode / Step: 20844 / Score: 0.96 / Actor loss: -0.04 / Critic loss: 0.0147\n",
      "2090 Episode / Step: 20882 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0537\n",
      "2100 Episode / Step: 20917 / Score: 0.98 / Actor loss: -0.00 / Critic loss: 0.0184\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "2110 Episode / Step: 20952 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0120\n",
      "2120 Episode / Step: 20988 / Score: 0.37 / Actor loss: -0.01 / Critic loss: 0.1999\n",
      "2130 Episode / Step: 21021 / Score: 0.38 / Actor loss: -0.06 / Critic loss: 0.1948\n",
      "2140 Episode / Step: 21052 / Score: 0.58 / Actor loss: 0.01 / Critic loss: 0.0980\n",
      "2150 Episode / Step: 21092 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.1039\n",
      "2160 Episode / Step: 21129 / Score: 0.77 / Actor loss: -0.06 / Critic loss: 0.0901\n",
      "2170 Episode / Step: 21175 / Score: 0.96 / Actor loss: -0.03 / Critic loss: 0.0307\n",
      "2180 Episode / Step: 21203 / Score: 0.58 / Actor loss: -0.02 / Critic loss: 0.1221\n",
      "2190 Episode / Step: 21237 / Score: 0.98 / Actor loss: 0.01 / Critic loss: 0.0241\n",
      "2200 Episode / Step: 21284 / Score: 0.36 / Actor loss: -0.09 / Critic loss: 0.1616\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "2210 Episode / Step: 21314 / Score: 0.58 / Actor loss: -0.09 / Critic loss: 0.1887\n",
      "2220 Episode / Step: 21355 / Score: 0.97 / Actor loss: 0.01 / Critic loss: 0.0321\n",
      "2230 Episode / Step: 21390 / Score: 0.78 / Actor loss: -0.00 / Critic loss: 0.0744\n",
      "2240 Episode / Step: 21419 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.1189\n",
      "2250 Episode / Step: 21454 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0918\n",
      "2260 Episode / Step: 21486 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0867\n",
      "2270 Episode / Step: 21533 / Score: 0.96 / Actor loss: -0.02 / Critic loss: 0.0163\n",
      "2280 Episode / Step: 21562 / Score: 0.58 / Actor loss: -0.20 / Critic loss: 0.1882\n",
      "2290 Episode / Step: 21602 / Score: 0.97 / Actor loss: 0.01 / Critic loss: 0.0081\n",
      "2300 Episode / Step: 21647 / Score: 0.77 / Actor loss: -0.03 / Critic loss: 0.0521\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "2310 Episode / Step: 21680 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0316\n",
      "2320 Episode / Step: 21714 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0880\n",
      "2330 Episode / Step: 21752 / Score: 0.97 / Actor loss: -0.02 / Critic loss: 0.0181\n",
      "2340 Episode / Step: 21784 / Score: 0.78 / Actor loss: -0.07 / Critic loss: 0.1040\n",
      "2350 Episode / Step: 21828 / Score: 0.77 / Actor loss: -0.03 / Critic loss: 0.0536\n",
      "2360 Episode / Step: 21856 / Score: 0.78 / Actor loss: -0.02 / Critic loss: 0.0818\n",
      "2370 Episode / Step: 21892 / Score: 0.57 / Actor loss: -0.02 / Critic loss: 0.1697\n",
      "2380 Episode / Step: 21926 / Score: 0.98 / Actor loss: 0.02 / Critic loss: 0.0236\n",
      "2390 Episode / Step: 21961 / Score: 0.78 / Actor loss: -0.08 / Critic loss: 0.0800\n",
      "2400 Episode / Step: 22003 / Score: 0.97 / Actor loss: -0.03 / Critic loss: 0.0139\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "2410 Episode / Step: 22034 / Score: 0.18 / Actor loss: -0.06 / Critic loss: 0.2370\n",
      "2420 Episode / Step: 22068 / Score: 0.98 / Actor loss: 0.02 / Critic loss: 0.0406\n",
      "2430 Episode / Step: 22107 / Score: 0.77 / Actor loss: -0.02 / Critic loss: 0.0592\n",
      "2440 Episode / Step: 22136 / Score: 0.98 / Actor loss: -0.00 / Critic loss: 0.0194\n",
      "2450 Episode / Step: 22183 / Score: 0.76 / Actor loss: -0.08 / Critic loss: 0.0805\n",
      "2460 Episode / Step: 22225 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0639\n",
      "2470 Episode / Step: 22267 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0209\n",
      "2480 Episode / Step: 22296 / Score: 0.78 / Actor loss: -0.04 / Critic loss: 0.0872\n",
      "2490 Episode / Step: 22333 / Score: 0.97 / Actor loss: -0.04 / Critic loss: 0.0203\n",
      "2500 Episode / Step: 22384 / Score: 0.96 / Actor loss: -0.03 / Critic loss: 0.0109\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "2510 Episode / Step: 22426 / Score: 0.97 / Actor loss: -0.03 / Critic loss: 0.0109\n",
      "2520 Episode / Step: 22460 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0667\n",
      "2530 Episode / Step: 22495 / Score: 0.98 / Actor loss: -0.04 / Critic loss: 0.0164\n",
      "2540 Episode / Step: 22538 / Score: 0.97 / Actor loss: -0.02 / Critic loss: 0.0167\n",
      "2550 Episode / Step: 22582 / Score: 0.77 / Actor loss: -0.41 / Critic loss: 0.0734\n",
      "2560 Episode / Step: 22620 / Score: 0.77 / Actor loss: -0.00 / Critic loss: 0.0566\n",
      "2570 Episode / Step: 22664 / Score: 0.57 / Actor loss: -0.06 / Critic loss: 0.1471\n",
      "2580 Episode / Step: 22786 / Score: 0.39 / Actor loss: 0.00 / Critic loss: 0.0538\n",
      "2590 Episode / Step: 22823 / Score: 0.77 / Actor loss: -0.03 / Critic loss: 0.0717\n",
      "2600 Episode / Step: 22845 / Score: 0.59 / Actor loss: -0.05 / Critic loss: 0.2234\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "2610 Episode / Step: 22887 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0187\n",
      "2620 Episode / Step: 22919 / Score: 0.58 / Actor loss: -0.04 / Critic loss: 0.1413\n",
      "2630 Episode / Step: 22966 / Score: 0.96 / Actor loss: -0.00 / Critic loss: 0.0118\n",
      "2640 Episode / Step: 22995 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0860\n",
      "2650 Episode / Step: 23038 / Score: 0.77 / Actor loss: -0.05 / Critic loss: 0.0738\n",
      "2660 Episode / Step: 23113 / Score: 0.74 / Actor loss: 0.01 / Critic loss: 0.0510\n",
      "2670 Episode / Step: 23151 / Score: 0.97 / Actor loss: -0.00 / Critic loss: 0.0263\n",
      "2680 Episode / Step: 23179 / Score: 0.58 / Actor loss: -0.03 / Critic loss: 0.1308\n",
      "2690 Episode / Step: 23220 / Score: 0.77 / Actor loss: 0.02 / Critic loss: 0.0627\n",
      "2700 Episode / Step: 23260 / Score: 0.77 / Actor loss: -0.05 / Critic loss: 0.0620\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "2710 Episode / Step: 23294 / Score: 0.58 / Actor loss: -0.04 / Critic loss: 0.1226\n",
      "2720 Episode / Step: 23329 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0198\n",
      "2730 Episode / Step: 23362 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0148\n",
      "2740 Episode / Step: 23384 / Score: 0.59 / Actor loss: -0.11 / Critic loss: 0.2117\n",
      "2750 Episode / Step: 23435 / Score: 0.56 / Actor loss: -0.18 / Critic loss: 0.1001\n",
      "2760 Episode / Step: 23469 / Score: 0.98 / Actor loss: 0.02 / Critic loss: 0.0295\n",
      "2770 Episode / Step: 23507 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.1110\n",
      "2780 Episode / Step: 23542 / Score: 0.98 / Actor loss: -0.03 / Critic loss: 0.0299\n",
      "2790 Episode / Step: 23593 / Score: 0.56 / Actor loss: -0.07 / Critic loss: 0.1000\n",
      "2800 Episode / Step: 23627 / Score: 0.98 / Actor loss: 0.01 / Critic loss: 0.0150\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "2810 Episode / Step: 23663 / Score: 0.77 / Actor loss: -0.00 / Critic loss: 0.0918\n",
      "2820 Episode / Step: 23693 / Score: 0.58 / Actor loss: -0.03 / Critic loss: 0.1256\n",
      "2830 Episode / Step: 23721 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0320\n",
      "2840 Episode / Step: 23755 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.1267\n",
      "2850 Episode / Step: 23786 / Score: 0.98 / Actor loss: -0.00 / Critic loss: 0.0117\n",
      "2860 Episode / Step: 23823 / Score: 0.57 / Actor loss: -0.14 / Critic loss: 0.1414\n",
      "2870 Episode / Step: 23872 / Score: 0.76 / Actor loss: -0.03 / Critic loss: 0.0483\n",
      "2880 Episode / Step: 23900 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0788\n",
      "2890 Episode / Step: 23935 / Score: 0.78 / Actor loss: -0.05 / Critic loss: 0.0753\n",
      "2900 Episode / Step: 23972 / Score: 0.97 / Actor loss: -0.00 / Critic loss: 0.0203\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "2910 Episode / Step: 24001 / Score: 0.98 / Actor loss: -0.05 / Critic loss: 0.0225\n",
      "2920 Episode / Step: 24034 / Score: 0.58 / Actor loss: -0.11 / Critic loss: 0.1358\n",
      "2930 Episode / Step: 24077 / Score: 0.77 / Actor loss: -0.00 / Critic loss: 0.0739\n",
      "2940 Episode / Step: 24102 / Score: 0.79 / Actor loss: 0.01 / Critic loss: 0.0974\n",
      "2950 Episode / Step: 24131 / Score: 0.18 / Actor loss: -0.10 / Critic loss: 0.3089\n",
      "2960 Episode / Step: 24170 / Score: 0.97 / Actor loss: -0.00 / Critic loss: 0.0550\n",
      "2970 Episode / Step: 24208 / Score: 0.77 / Actor loss: -0.02 / Critic loss: 0.0870\n",
      "2980 Episode / Step: 24234 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0298\n",
      "2990 Episode / Step: 24281 / Score: 0.76 / Actor loss: -0.03 / Critic loss: 0.0623\n",
      "3000 Episode / Step: 24320 / Score: 0.77 / Actor loss: -0.03 / Critic loss: 0.0679\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "3010 Episode / Step: 24351 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0125\n",
      "3020 Episode / Step: 24387 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0230\n",
      "3030 Episode / Step: 24418 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0059\n",
      "3040 Episode / Step: 24451 / Score: 0.78 / Actor loss: -0.00 / Critic loss: 0.1019\n",
      "3050 Episode / Step: 24481 / Score: 0.98 / Actor loss: -0.00 / Critic loss: 0.0151\n",
      "3060 Episode / Step: 24511 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0282\n",
      "3070 Episode / Step: 24541 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0190\n",
      "3080 Episode / Step: 24573 / Score: 0.38 / Actor loss: -0.07 / Critic loss: 0.2478\n",
      "3090 Episode / Step: 24604 / Score: 0.98 / Actor loss: -0.00 / Critic loss: 0.0291\n",
      "3100 Episode / Step: 24633 / Score: 0.78 / Actor loss: -0.02 / Critic loss: 0.0850\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "3110 Episode / Step: 24663 / Score: 0.58 / Actor loss: -0.02 / Critic loss: 0.1458\n",
      "3120 Episode / Step: 24702 / Score: 0.97 / Actor loss: 0.01 / Critic loss: 0.0224\n",
      "3130 Episode / Step: 24729 / Score: 0.78 / Actor loss: -0.09 / Critic loss: 0.0852\n",
      "3140 Episode / Step: 24772 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0687\n",
      "3150 Episode / Step: 24809 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0168\n",
      "3160 Episode / Step: 24862 / Score: 0.76 / Actor loss: -0.03 / Critic loss: 0.0656\n",
      "3170 Episode / Step: 24899 / Score: 0.77 / Actor loss: 0.01 / Critic loss: 0.0746\n",
      "3180 Episode / Step: 24939 / Score: 0.77 / Actor loss: -0.04 / Critic loss: 0.0772\n",
      "3190 Episode / Step: 24993 / Score: 0.76 / Actor loss: -0.03 / Critic loss: 0.0433\n",
      "3200 Episode / Step: 25026 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0283\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "3210 Episode / Step: 25056 / Score: 0.78 / Actor loss: -0.02 / Critic loss: 0.0806\n",
      "3220 Episode / Step: 25095 / Score: 0.77 / Actor loss: -0.04 / Critic loss: 0.0880\n",
      "3230 Episode / Step: 25121 / Score: 0.78 / Actor loss: -0.08 / Critic loss: 0.1139\n",
      "3240 Episode / Step: 25154 / Score: 0.98 / Actor loss: -0.03 / Critic loss: 0.0237\n",
      "3250 Episode / Step: 25180 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0146\n",
      "3260 Episode / Step: 25223 / Score: 0.57 / Actor loss: -0.12 / Critic loss: 0.1504\n",
      "3270 Episode / Step: 25253 / Score: 0.98 / Actor loss: 0.03 / Critic loss: 0.0351\n",
      "3280 Episode / Step: 25291 / Score: 0.77 / Actor loss: -0.04 / Critic loss: 0.0762\n",
      "3290 Episode / Step: 25320 / Score: 0.78 / Actor loss: 0.01 / Critic loss: 0.0617\n",
      "3300 Episode / Step: 25351 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0150\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "3310 Episode / Step: 25386 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0265\n",
      "3320 Episode / Step: 25417 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0226\n",
      "3330 Episode / Step: 25449 / Score: 0.78 / Actor loss: -0.05 / Critic loss: 0.1122\n",
      "3340 Episode / Step: 25478 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.1133\n",
      "3350 Episode / Step: 25509 / Score: 0.78 / Actor loss: -0.03 / Critic loss: 0.0826\n",
      "3360 Episode / Step: 25539 / Score: 0.78 / Actor loss: -0.00 / Critic loss: 0.0468\n",
      "3370 Episode / Step: 25571 / Score: 0.98 / Actor loss: -0.04 / Critic loss: 0.0309\n",
      "3380 Episode / Step: 25600 / Score: 0.58 / Actor loss: -0.06 / Critic loss: 0.1504\n",
      "3390 Episode / Step: 25637 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0892\n",
      "3400 Episode / Step: 25670 / Score: 0.78 / Actor loss: -0.07 / Critic loss: 0.0889\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "3410 Episode / Step: 25692 / Score: 0.79 / Actor loss: -0.01 / Critic loss: 0.1249\n",
      "3420 Episode / Step: 25719 / Score: 0.78 / Actor loss: -0.02 / Critic loss: 0.1240\n",
      "3430 Episode / Step: 25752 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0759\n",
      "3440 Episode / Step: 25784 / Score: 0.98 / Actor loss: -0.00 / Critic loss: 0.0121\n",
      "3450 Episode / Step: 25825 / Score: 0.77 / Actor loss: -0.02 / Critic loss: 0.0583\n",
      "3460 Episode / Step: 25863 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0692\n",
      "3470 Episode / Step: 25894 / Score: 0.78 / Actor loss: -0.04 / Critic loss: 0.0999\n",
      "3480 Episode / Step: 25924 / Score: 0.78 / Actor loss: -0.00 / Critic loss: 0.0871\n",
      "3490 Episode / Step: 25956 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.1002\n",
      "3500 Episode / Step: 25983 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0331\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "3510 Episode / Step: 26021 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0115\n",
      "3520 Episode / Step: 26059 / Score: 0.57 / Actor loss: -0.04 / Critic loss: 0.1132\n",
      "3530 Episode / Step: 26093 / Score: 0.78 / Actor loss: -0.00 / Critic loss: 0.0773\n",
      "3540 Episode / Step: 26124 / Score: 0.78 / Actor loss: -0.06 / Critic loss: 0.1117\n",
      "3550 Episode / Step: 26169 / Score: 0.97 / Actor loss: -0.02 / Critic loss: 0.0147\n",
      "3560 Episode / Step: 26196 / Score: 0.78 / Actor loss: -0.04 / Critic loss: 0.1096\n",
      "3570 Episode / Step: 26235 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0136\n",
      "3580 Episode / Step: 26278 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0554\n",
      "3590 Episode / Step: 26315 / Score: 0.77 / Actor loss: -0.02 / Critic loss: 0.0619\n",
      "3600 Episode / Step: 26351 / Score: 0.57 / Actor loss: -0.02 / Critic loss: 0.1110\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "3610 Episode / Step: 26384 / Score: 0.78 / Actor loss: -0.05 / Critic loss: 0.0705\n",
      "3620 Episode / Step: 26411 / Score: 0.58 / Actor loss: 0.02 / Critic loss: 0.1520\n",
      "3630 Episode / Step: 26432 / Score: 0.79 / Actor loss: 0.00 / Critic loss: 0.1190\n",
      "3640 Episode / Step: 26456 / Score: 0.79 / Actor loss: -0.04 / Critic loss: 0.0631\n",
      "3650 Episode / Step: 26490 / Score: 0.58 / Actor loss: -0.02 / Critic loss: 0.1242\n",
      "3660 Episode / Step: 26516 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0723\n",
      "3670 Episode / Step: 26549 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.1189\n",
      "3680 Episode / Step: 26580 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0111\n",
      "3690 Episode / Step: 26619 / Score: 0.97 / Actor loss: -0.03 / Critic loss: 0.0133\n",
      "3700 Episode / Step: 26656 / Score: 0.97 / Actor loss: -0.03 / Critic loss: 0.0092\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "3710 Episode / Step: 26680 / Score: 0.59 / Actor loss: -0.02 / Critic loss: 0.1857\n",
      "3720 Episode / Step: 26712 / Score: 0.18 / Actor loss: -0.03 / Critic loss: 0.2301\n",
      "3730 Episode / Step: 26747 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0399\n",
      "3740 Episode / Step: 26779 / Score: 0.58 / Actor loss: -0.01 / Critic loss: 0.1083\n",
      "3750 Episode / Step: 26806 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.1250\n",
      "3760 Episode / Step: 26843 / Score: 0.97 / Actor loss: -0.00 / Critic loss: 0.0133\n",
      "3770 Episode / Step: 26869 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0160\n",
      "3780 Episode / Step: 26900 / Score: 0.78 / Actor loss: -0.00 / Critic loss: 0.0844\n",
      "3790 Episode / Step: 26942 / Score: 0.97 / Actor loss: -0.03 / Critic loss: 0.0270\n",
      "3800 Episode / Step: 26965 / Score: 0.39 / Actor loss: -0.01 / Critic loss: 0.2710\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "3810 Episode / Step: 27002 / Score: 0.77 / Actor loss: 0.01 / Critic loss: 0.0895\n",
      "3820 Episode / Step: 27039 / Score: 0.77 / Actor loss: -0.05 / Critic loss: 0.0854\n",
      "3830 Episode / Step: 27073 / Score: 0.98 / Actor loss: 0.01 / Critic loss: 0.0144\n",
      "3840 Episode / Step: 27110 / Score: 0.97 / Actor loss: -0.02 / Critic loss: 0.0170\n",
      "3850 Episode / Step: 27131 / Score: 0.59 / Actor loss: -0.01 / Critic loss: 0.1927\n",
      "3860 Episode / Step: 27156 / Score: 0.59 / Actor loss: -0.01 / Critic loss: 0.1511\n",
      "3870 Episode / Step: 27194 / Score: 0.97 / Actor loss: -0.02 / Critic loss: 0.0285\n",
      "3880 Episode / Step: 27224 / Score: 0.58 / Actor loss: 0.01 / Critic loss: 0.0890\n",
      "3890 Episode / Step: 27257 / Score: 0.78 / Actor loss: -0.04 / Critic loss: 0.0874\n",
      "3900 Episode / Step: 27291 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0234\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "3910 Episode / Step: 27328 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0649\n",
      "3920 Episode / Step: 27363 / Score: 0.78 / Actor loss: -0.03 / Critic loss: 0.0778\n",
      "3930 Episode / Step: 27399 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0131\n",
      "3940 Episode / Step: 27434 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0792\n",
      "3950 Episode / Step: 27470 / Score: 0.77 / Actor loss: -0.03 / Critic loss: 0.0531\n",
      "3960 Episode / Step: 27502 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0684\n",
      "3970 Episode / Step: 27534 / Score: 0.78 / Actor loss: -0.05 / Critic loss: 0.0797\n",
      "3980 Episode / Step: 27564 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0099\n",
      "3990 Episode / Step: 27601 / Score: 0.97 / Actor loss: -0.03 / Critic loss: 0.0131\n",
      "4000 Episode / Step: 27639 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0122\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "4010 Episode / Step: 27672 / Score: 0.78 / Actor loss: -0.04 / Critic loss: 0.0600\n",
      "4020 Episode / Step: 27711 / Score: 0.97 / Actor loss: -0.00 / Critic loss: 0.0230\n",
      "4030 Episode / Step: 27741 / Score: 0.58 / Actor loss: -0.10 / Critic loss: 0.1327\n",
      "4040 Episode / Step: 27770 / Score: 0.98 / Actor loss: -0.03 / Critic loss: 0.0211\n",
      "4050 Episode / Step: 27806 / Score: 0.97 / Actor loss: 0.03 / Critic loss: 0.0142\n",
      "4060 Episode / Step: 27856 / Score: 0.56 / Actor loss: -0.02 / Critic loss: 0.0893\n",
      "4070 Episode / Step: 27897 / Score: 0.77 / Actor loss: -0.03 / Critic loss: 0.0862\n",
      "4080 Episode / Step: 27920 / Score: 0.59 / Actor loss: -0.04 / Critic loss: 0.1476\n",
      "4090 Episode / Step: 27957 / Score: 0.57 / Actor loss: -0.07 / Critic loss: 0.1470\n",
      "4100 Episode / Step: 28011 / Score: 0.96 / Actor loss: 0.02 / Critic loss: 0.0304\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "4110 Episode / Step: 28045 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0211\n",
      "4120 Episode / Step: 28079 / Score: 0.78 / Actor loss: -0.11 / Critic loss: 0.0982\n",
      "4130 Episode / Step: 28124 / Score: 0.97 / Actor loss: -0.00 / Critic loss: 0.0194\n",
      "4140 Episode / Step: 28166 / Score: 0.97 / Actor loss: -0.00 / Critic loss: 0.0103\n",
      "4150 Episode / Step: 28198 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0199\n",
      "4160 Episode / Step: 28224 / Score: 0.18 / Actor loss: -0.24 / Critic loss: 0.3776\n",
      "4170 Episode / Step: 28257 / Score: 0.78 / Actor loss: 0.02 / Critic loss: 0.1066\n",
      "4180 Episode / Step: 28300 / Score: 0.77 / Actor loss: -0.02 / Critic loss: 0.0752\n",
      "4190 Episode / Step: 28335 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0261\n",
      "4200 Episode / Step: 28363 / Score: 0.78 / Actor loss: -0.07 / Critic loss: 0.1271\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "4210 Episode / Step: 28401 / Score: 0.97 / Actor loss: -0.05 / Critic loss: 0.0113\n",
      "4220 Episode / Step: 28431 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0860\n",
      "4230 Episode / Step: 28474 / Score: 0.97 / Actor loss: -0.00 / Critic loss: 0.0134\n",
      "4240 Episode / Step: 28502 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0202\n",
      "4250 Episode / Step: 28543 / Score: 0.97 / Actor loss: -0.00 / Critic loss: 0.0070\n",
      "4260 Episode / Step: 28575 / Score: 0.78 / Actor loss: -0.04 / Critic loss: 0.0853\n",
      "4270 Episode / Step: 28609 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0075\n",
      "4280 Episode / Step: 28642 / Score: 0.58 / Actor loss: -0.03 / Critic loss: 0.1653\n",
      "4290 Episode / Step: 28666 / Score: 0.79 / Actor loss: 0.01 / Critic loss: 0.1093\n",
      "4300 Episode / Step: 28700 / Score: 0.58 / Actor loss: -0.04 / Critic loss: 0.1197\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "4310 Episode / Step: 28733 / Score: 0.78 / Actor loss: 0.01 / Critic loss: 0.0693\n",
      "4320 Episode / Step: 28773 / Score: 0.37 / Actor loss: -0.04 / Critic loss: 0.1568\n",
      "4330 Episode / Step: 28812 / Score: 0.97 / Actor loss: -0.00 / Critic loss: 0.0305\n",
      "4340 Episode / Step: 28848 / Score: 0.97 / Actor loss: -0.00 / Critic loss: 0.0099\n",
      "4350 Episode / Step: 28876 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0187\n",
      "4360 Episode / Step: 28913 / Score: 0.97 / Actor loss: -0.03 / Critic loss: 0.0098\n",
      "4370 Episode / Step: 28945 / Score: 0.78 / Actor loss: 0.01 / Critic loss: 0.0547\n",
      "4380 Episode / Step: 28984 / Score: 0.77 / Actor loss: -0.02 / Critic loss: 0.0459\n",
      "4390 Episode / Step: 29018 / Score: 0.98 / Actor loss: -0.00 / Critic loss: 0.0078\n",
      "4400 Episode / Step: 29048 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0082\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "4410 Episode / Step: 29086 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0701\n",
      "4420 Episode / Step: 29155 / Score: 0.74 / Actor loss: -0.04 / Critic loss: 0.0495\n",
      "4430 Episode / Step: 29185 / Score: 0.58 / Actor loss: -0.07 / Critic loss: 0.1690\n",
      "4440 Episode / Step: 29224 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0681\n",
      "4450 Episode / Step: 29270 / Score: 0.96 / Actor loss: -0.03 / Critic loss: 0.0128\n",
      "4460 Episode / Step: 29300 / Score: 0.58 / Actor loss: -0.07 / Critic loss: 0.1371\n",
      "4470 Episode / Step: 29335 / Score: 0.98 / Actor loss: -0.04 / Critic loss: 0.0118\n",
      "4480 Episode / Step: 29362 / Score: 0.98 / Actor loss: -0.00 / Critic loss: 0.0164\n",
      "4490 Episode / Step: 29397 / Score: 0.38 / Actor loss: -0.02 / Critic loss: 0.1361\n",
      "4500 Episode / Step: 29423 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0712\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "4510 Episode / Step: 29464 / Score: 0.77 / Actor loss: -0.00 / Critic loss: 0.0709\n",
      "4520 Episode / Step: 29502 / Score: 0.77 / Actor loss: -0.02 / Critic loss: 0.0823\n",
      "4530 Episode / Step: 29535 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0095\n",
      "4540 Episode / Step: 29571 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0656\n",
      "4550 Episode / Step: 29603 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0128\n",
      "4560 Episode / Step: 29630 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0218\n",
      "4570 Episode / Step: 29659 / Score: 0.58 / Actor loss: -0.02 / Critic loss: 0.1161\n",
      "4580 Episode / Step: 29691 / Score: 0.58 / Actor loss: -0.04 / Critic loss: 0.1726\n",
      "4590 Episode / Step: 29726 / Score: 0.78 / Actor loss: -0.06 / Critic loss: 0.0798\n",
      "4600 Episode / Step: 29767 / Score: 0.97 / Actor loss: -0.03 / Critic loss: 0.0130\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "4610 Episode / Step: 29808 / Score: 0.97 / Actor loss: -0.04 / Critic loss: 0.0126\n",
      "4620 Episode / Step: 29846 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0408\n",
      "4630 Episode / Step: 29884 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0697\n",
      "4640 Episode / Step: 29911 / Score: 0.78 / Actor loss: -0.09 / Critic loss: 0.0920\n",
      "4650 Episode / Step: 29940 / Score: 0.78 / Actor loss: 0.01 / Critic loss: 0.1017\n",
      "4660 Episode / Step: 29969 / Score: 0.78 / Actor loss: -0.09 / Critic loss: 0.0847\n",
      "4670 Episode / Step: 30021 / Score: 0.96 / Actor loss: 0.00 / Critic loss: 0.0192\n",
      "4680 Episode / Step: 30057 / Score: 0.37 / Actor loss: -0.05 / Critic loss: 0.2057\n",
      "4690 Episode / Step: 30091 / Score: 0.58 / Actor loss: 0.02 / Critic loss: 0.0984\n",
      "4700 Episode / Step: 30118 / Score: 0.78 / Actor loss: -0.00 / Critic loss: 0.0874\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "4710 Episode / Step: 30152 / Score: 0.98 / Actor loss: 0.01 / Critic loss: 0.0201\n",
      "4720 Episode / Step: 30184 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0153\n",
      "4730 Episode / Step: 30208 / Score: 0.79 / Actor loss: -0.06 / Critic loss: 0.0994\n",
      "4740 Episode / Step: 30236 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0168\n",
      "4750 Episode / Step: 30269 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0118\n",
      "4760 Episode / Step: 30308 / Score: 0.77 / Actor loss: -0.00 / Critic loss: 0.0591\n",
      "4770 Episode / Step: 30341 / Score: 0.58 / Actor loss: -0.20 / Critic loss: 0.1441\n",
      "4780 Episode / Step: 30370 / Score: 0.58 / Actor loss: -0.02 / Critic loss: 0.1623\n",
      "4790 Episode / Step: 30407 / Score: 0.77 / Actor loss: -0.05 / Critic loss: 0.0684\n",
      "4800 Episode / Step: 30447 / Score: 0.77 / Actor loss: -0.02 / Critic loss: 0.0960\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "4810 Episode / Step: 30476 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0785\n",
      "4820 Episode / Step: 30500 / Score: 0.79 / Actor loss: -0.02 / Critic loss: 0.1223\n",
      "4830 Episode / Step: 30528 / Score: 0.58 / Actor loss: -0.01 / Critic loss: 0.1720\n",
      "4840 Episode / Step: 30561 / Score: 0.58 / Actor loss: -0.02 / Critic loss: 0.1033\n",
      "4850 Episode / Step: 30603 / Score: 0.37 / Actor loss: -0.05 / Critic loss: 0.1162\n",
      "4860 Episode / Step: 30633 / Score: 0.78 / Actor loss: -0.03 / Critic loss: 0.0747\n",
      "4870 Episode / Step: 30669 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0145\n",
      "4880 Episode / Step: 30707 / Score: 0.97 / Actor loss: 0.01 / Critic loss: 0.0194\n",
      "4890 Episode / Step: 30741 / Score: 0.98 / Actor loss: -0.04 / Critic loss: 0.0301\n",
      "4900 Episode / Step: 30784 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0179\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "4910 Episode / Step: 30812 / Score: 0.58 / Actor loss: -0.01 / Critic loss: 0.1799\n",
      "4920 Episode / Step: 30848 / Score: 0.77 / Actor loss: -0.00 / Critic loss: 0.0688\n",
      "4930 Episode / Step: 30872 / Score: 0.79 / Actor loss: -0.00 / Critic loss: 0.1207\n",
      "4940 Episode / Step: 30905 / Score: 0.58 / Actor loss: -0.19 / Critic loss: 0.1247\n",
      "4950 Episode / Step: 30940 / Score: 0.38 / Actor loss: -0.02 / Critic loss: 0.1940\n",
      "4960 Episode / Step: 30961 / Score: 0.99 / Actor loss: 0.01 / Critic loss: 0.0215\n",
      "4970 Episode / Step: 30993 / Score: 0.78 / Actor loss: -0.00 / Critic loss: 0.0707\n",
      "4980 Episode / Step: 31029 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0136\n",
      "4990 Episode / Step: 31049 / Score: 0.79 / Actor loss: -0.02 / Critic loss: 0.1205\n",
      "5000 Episode / Step: 31073 / Score: 0.79 / Actor loss: 0.01 / Critic loss: 0.1117\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "5010 Episode / Step: 31112 / Score: 0.97 / Actor loss: -0.04 / Critic loss: 0.0194\n",
      "5020 Episode / Step: 31142 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.1039\n",
      "5030 Episode / Step: 31169 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0182\n",
      "5040 Episode / Step: 31213 / Score: 0.97 / Actor loss: -0.00 / Critic loss: 0.0095\n",
      "5050 Episode / Step: 31256 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.1025\n",
      "5060 Episode / Step: 31285 / Score: 0.58 / Actor loss: -0.09 / Critic loss: 0.1354\n",
      "5070 Episode / Step: 31320 / Score: 0.98 / Actor loss: 0.01 / Critic loss: 0.0095\n",
      "5080 Episode / Step: 31347 / Score: 0.58 / Actor loss: -0.07 / Critic loss: 0.1456\n",
      "5090 Episode / Step: 31383 / Score: 0.97 / Actor loss: -0.03 / Critic loss: 0.0086\n",
      "5100 Episode / Step: 31414 / Score: 0.58 / Actor loss: -0.00 / Critic loss: 0.1319\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "5110 Episode / Step: 31460 / Score: 0.96 / Actor loss: -0.01 / Critic loss: 0.0199\n",
      "5120 Episode / Step: 31486 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0206\n",
      "5130 Episode / Step: 31518 / Score: 0.38 / Actor loss: -0.00 / Critic loss: 0.2386\n",
      "5140 Episode / Step: 31554 / Score: 0.77 / Actor loss: 0.01 / Critic loss: 0.0642\n",
      "5150 Episode / Step: 31586 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0233\n",
      "5160 Episode / Step: 31636 / Score: 0.96 / Actor loss: 0.00 / Critic loss: 0.0195\n",
      "5170 Episode / Step: 31663 / Score: 0.98 / Actor loss: -0.04 / Critic loss: 0.0186\n",
      "5180 Episode / Step: 31699 / Score: 0.57 / Actor loss: -0.10 / Critic loss: 0.1298\n",
      "5190 Episode / Step: 31733 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0089\n",
      "5200 Episode / Step: 31762 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0085\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "5210 Episode / Step: 31787 / Score: 0.59 / Actor loss: -0.04 / Critic loss: 0.1851\n",
      "5220 Episode / Step: 31825 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0706\n",
      "5230 Episode / Step: 31879 / Score: 0.96 / Actor loss: -0.02 / Critic loss: 0.0137\n",
      "5240 Episode / Step: 31912 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0181\n",
      "5250 Episode / Step: 31959 / Score: 0.96 / Actor loss: -0.01 / Critic loss: 0.0150\n",
      "5260 Episode / Step: 31993 / Score: 0.58 / Actor loss: -0.03 / Critic loss: 0.1545\n",
      "5270 Episode / Step: 32028 / Score: 0.98 / Actor loss: -0.00 / Critic loss: 0.0195\n",
      "5280 Episode / Step: 32063 / Score: 0.78 / Actor loss: -0.02 / Critic loss: 0.0947\n",
      "5290 Episode / Step: 32087 / Score: 0.79 / Actor loss: -0.00 / Critic loss: 0.1039\n",
      "5300 Episode / Step: 32118 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0212\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "5310 Episode / Step: 32159 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0061\n",
      "5320 Episode / Step: 32190 / Score: 0.38 / Actor loss: -0.03 / Critic loss: 0.2524\n",
      "5330 Episode / Step: 32227 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0662\n",
      "5340 Episode / Step: 32253 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0127\n",
      "5350 Episode / Step: 32280 / Score: 0.58 / Actor loss: -0.16 / Critic loss: 0.1737\n",
      "5360 Episode / Step: 32308 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0232\n",
      "5370 Episode / Step: 32339 / Score: 0.78 / Actor loss: -0.00 / Critic loss: 0.0724\n",
      "5380 Episode / Step: 32369 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0098\n",
      "5390 Episode / Step: 32405 / Score: 0.77 / Actor loss: -0.04 / Critic loss: 0.0675\n",
      "5400 Episode / Step: 32448 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0078\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "5410 Episode / Step: 32480 / Score: 0.78 / Actor loss: -0.02 / Critic loss: 0.0487\n",
      "5420 Episode / Step: 32512 / Score: 0.98 / Actor loss: -0.03 / Critic loss: 0.0199\n",
      "5430 Episode / Step: 32543 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0750\n",
      "5440 Episode / Step: 32578 / Score: 0.78 / Actor loss: 0.01 / Critic loss: 0.0476\n",
      "5450 Episode / Step: 32618 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0114\n",
      "5460 Episode / Step: 32650 / Score: 0.98 / Actor loss: -0.03 / Critic loss: 0.0071\n",
      "5470 Episode / Step: 32687 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0619\n",
      "5480 Episode / Step: 32717 / Score: 0.78 / Actor loss: 0.01 / Critic loss: 0.1144\n",
      "5490 Episode / Step: 32750 / Score: 0.58 / Actor loss: -0.00 / Critic loss: 0.1180\n",
      "5500 Episode / Step: 32782 / Score: 0.78 / Actor loss: -0.02 / Critic loss: 0.0623\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "5510 Episode / Step: 32814 / Score: 0.38 / Actor loss: -0.05 / Critic loss: 0.1731\n",
      "5520 Episode / Step: 32836 / Score: 0.99 / Actor loss: 0.02 / Critic loss: 0.0536\n",
      "5530 Episode / Step: 32868 / Score: 0.78 / Actor loss: -0.10 / Critic loss: 0.1121\n",
      "5540 Episode / Step: 32896 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0184\n",
      "5550 Episode / Step: 32931 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.1325\n",
      "5560 Episode / Step: 32966 / Score: 0.78 / Actor loss: -0.11 / Critic loss: 0.0807\n",
      "5570 Episode / Step: 32988 / Score: 0.79 / Actor loss: 0.00 / Critic loss: 0.0935\n",
      "5580 Episode / Step: 33040 / Score: 0.16 / Actor loss: -0.04 / Critic loss: 0.0903\n",
      "5590 Episode / Step: 33073 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0924\n",
      "5600 Episode / Step: 33104 / Score: 0.78 / Actor loss: 0.01 / Critic loss: 0.1010\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "5610 Episode / Step: 33138 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0217\n",
      "5620 Episode / Step: 33174 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0388\n",
      "5630 Episode / Step: 33206 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0277\n",
      "5640 Episode / Step: 33234 / Score: 0.98 / Actor loss: -0.00 / Critic loss: 0.0157\n",
      "5650 Episode / Step: 33262 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0607\n",
      "5660 Episode / Step: 33293 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.1057\n",
      "5670 Episode / Step: 33329 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0185\n",
      "5680 Episode / Step: 33362 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0121\n",
      "5690 Episode / Step: 33389 / Score: 0.78 / Actor loss: -0.02 / Critic loss: 0.0893\n",
      "5700 Episode / Step: 33421 / Score: 0.78 / Actor loss: -0.08 / Critic loss: 0.1219\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "5710 Episode / Step: 33454 / Score: 0.58 / Actor loss: -0.06 / Critic loss: 0.1421\n",
      "5720 Episode / Step: 33491 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0658\n",
      "5730 Episode / Step: 33521 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0239\n",
      "5740 Episode / Step: 33553 / Score: 0.98 / Actor loss: -0.04 / Critic loss: 0.0108\n",
      "5750 Episode / Step: 33582 / Score: 0.38 / Actor loss: -0.00 / Critic loss: 0.2180\n",
      "5760 Episode / Step: 33614 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0241\n",
      "5770 Episode / Step: 33637 / Score: 0.99 / Actor loss: 0.01 / Critic loss: 0.0303\n",
      "5780 Episode / Step: 33668 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0931\n",
      "5790 Episode / Step: 33698 / Score: 0.78 / Actor loss: -0.03 / Critic loss: 0.0824\n",
      "5800 Episode / Step: 33721 / Score: 0.79 / Actor loss: -0.08 / Critic loss: 0.1184\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "5810 Episode / Step: 33748 / Score: 0.78 / Actor loss: -0.02 / Critic loss: 0.0896\n",
      "5820 Episode / Step: 33789 / Score: 0.57 / Actor loss: -0.03 / Critic loss: 0.1148\n",
      "5830 Episode / Step: 33839 / Score: 0.56 / Actor loss: -0.01 / Critic loss: 0.0889\n",
      "5840 Episode / Step: 33865 / Score: 0.78 / Actor loss: 0.01 / Critic loss: 0.0734\n",
      "5850 Episode / Step: 33893 / Score: 0.58 / Actor loss: -0.02 / Critic loss: 0.1034\n",
      "5860 Episode / Step: 33918 / Score: 0.59 / Actor loss: -0.03 / Critic loss: 0.0940\n",
      "5870 Episode / Step: 33957 / Score: 0.97 / Actor loss: -0.04 / Critic loss: 0.0286\n",
      "5880 Episode / Step: 33995 / Score: 0.97 / Actor loss: -0.02 / Critic loss: 0.0130\n",
      "5890 Episode / Step: 34020 / Score: 0.39 / Actor loss: -0.00 / Critic loss: 0.1943\n",
      "5900 Episode / Step: 34053 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0174\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "5910 Episode / Step: 34084 / Score: 0.58 / Actor loss: -0.00 / Critic loss: 0.1356\n",
      "5920 Episode / Step: 34123 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0479\n",
      "5930 Episode / Step: 34160 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0900\n",
      "5940 Episode / Step: 34195 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0064\n",
      "5950 Episode / Step: 34247 / Score: 0.76 / Actor loss: 0.01 / Critic loss: 0.0766\n",
      "5960 Episode / Step: 34296 / Score: 0.96 / Actor loss: -0.02 / Critic loss: 0.0202\n",
      "5970 Episode / Step: 34325 / Score: 0.78 / Actor loss: -0.06 / Critic loss: 0.0893\n",
      "5980 Episode / Step: 34360 / Score: 0.38 / Actor loss: -0.15 / Critic loss: 0.1766\n",
      "5990 Episode / Step: 34394 / Score: 0.58 / Actor loss: -0.03 / Critic loss: 0.1250\n",
      "6000 Episode / Step: 34429 / Score: 0.78 / Actor loss: -0.10 / Critic loss: 0.0730\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "6010 Episode / Step: 34464 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0341\n",
      "6020 Episode / Step: 34496 / Score: 0.78 / Actor loss: -0.00 / Critic loss: 0.0596\n",
      "6030 Episode / Step: 34524 / Score: 0.78 / Actor loss: -0.03 / Critic loss: 0.0876\n",
      "6040 Episode / Step: 34555 / Score: 0.78 / Actor loss: -0.02 / Critic loss: 0.0916\n",
      "6050 Episode / Step: 34585 / Score: 0.78 / Actor loss: 0.01 / Critic loss: 0.1188\n",
      "6060 Episode / Step: 34624 / Score: 0.97 / Actor loss: -0.02 / Critic loss: 0.0188\n",
      "6070 Episode / Step: 34654 / Score: 0.58 / Actor loss: -0.17 / Critic loss: 0.1441\n",
      "6080 Episode / Step: 34748 / Score: 0.92 / Actor loss: 0.02 / Critic loss: 0.0173\n",
      "6090 Episode / Step: 34783 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0183\n",
      "6100 Episode / Step: 34821 / Score: 0.97 / Actor loss: -0.00 / Critic loss: 0.0177\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "6110 Episode / Step: 34860 / Score: 0.77 / Actor loss: -0.00 / Critic loss: 0.0654\n",
      "6120 Episode / Step: 34894 / Score: 0.78 / Actor loss: -0.02 / Critic loss: 0.0800\n",
      "6130 Episode / Step: 34931 / Score: 0.97 / Actor loss: -0.02 / Critic loss: 0.0079\n",
      "6140 Episode / Step: 34972 / Score: 0.97 / Actor loss: 0.01 / Critic loss: 0.0074\n",
      "6150 Episode / Step: 35008 / Score: 0.57 / Actor loss: 0.00 / Critic loss: 0.0931\n",
      "6160 Episode / Step: 35048 / Score: 0.97 / Actor loss: -0.02 / Critic loss: 0.0136\n",
      "6170 Episode / Step: 35076 / Score: 0.98 / Actor loss: -0.00 / Critic loss: 0.0113\n",
      "6180 Episode / Step: 35110 / Score: 0.58 / Actor loss: -0.01 / Critic loss: 0.1229\n",
      "6190 Episode / Step: 35144 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0249\n",
      "6200 Episode / Step: 35173 / Score: 0.58 / Actor loss: 0.01 / Critic loss: 0.1313\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "6210 Episode / Step: 35203 / Score: 0.58 / Actor loss: 0.01 / Critic loss: 0.1239\n",
      "6220 Episode / Step: 35239 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0218\n",
      "6230 Episode / Step: 35268 / Score: 0.78 / Actor loss: -0.03 / Critic loss: 0.0520\n",
      "6240 Episode / Step: 35296 / Score: 0.78 / Actor loss: -0.02 / Critic loss: 0.0774\n",
      "6250 Episode / Step: 35324 / Score: 0.58 / Actor loss: -0.02 / Critic loss: 0.1658\n",
      "6260 Episode / Step: 35349 / Score: 0.99 / Actor loss: 0.00 / Critic loss: 0.0212\n",
      "6270 Episode / Step: 35379 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0215\n",
      "6280 Episode / Step: 35418 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0724\n",
      "6290 Episode / Step: 35456 / Score: 0.57 / Actor loss: 0.00 / Critic loss: 0.1007\n",
      "6300 Episode / Step: 35488 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0325\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "6310 Episode / Step: 35524 / Score: 0.57 / Actor loss: -0.02 / Critic loss: 0.1111\n",
      "6320 Episode / Step: 35556 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.1074\n",
      "6330 Episode / Step: 35582 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0558\n",
      "6340 Episode / Step: 35618 / Score: 0.97 / Actor loss: -0.00 / Critic loss: 0.0247\n",
      "6350 Episode / Step: 35656 / Score: 0.97 / Actor loss: -0.05 / Critic loss: 0.0100\n",
      "6360 Episode / Step: 35692 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0621\n",
      "6370 Episode / Step: 35726 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0148\n",
      "6380 Episode / Step: 35756 / Score: 0.58 / Actor loss: -0.01 / Critic loss: 0.1635\n",
      "6390 Episode / Step: 35787 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0300\n",
      "6400 Episode / Step: 35840 / Score: 0.96 / Actor loss: -0.02 / Critic loss: 0.0159\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "6410 Episode / Step: 35877 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0738\n",
      "6420 Episode / Step: 35905 / Score: 0.38 / Actor loss: 0.01 / Critic loss: 0.2117\n",
      "6430 Episode / Step: 35933 / Score: 0.78 / Actor loss: -0.04 / Critic loss: 0.0926\n",
      "6440 Episode / Step: 35969 / Score: 0.57 / Actor loss: -0.00 / Critic loss: 0.1208\n",
      "6450 Episode / Step: 36009 / Score: 0.77 / Actor loss: -0.07 / Critic loss: 0.0666\n",
      "6460 Episode / Step: 36045 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0893\n",
      "6470 Episode / Step: 36077 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0119\n",
      "6480 Episode / Step: 36107 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0533\n",
      "6490 Episode / Step: 36156 / Score: 0.76 / Actor loss: -0.04 / Critic loss: 0.0605\n",
      "6500 Episode / Step: 36182 / Score: 0.38 / Actor loss: -0.04 / Critic loss: 0.2308\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "6510 Episode / Step: 36212 / Score: 0.38 / Actor loss: -0.03 / Critic loss: 0.1978\n",
      "6520 Episode / Step: 36248 / Score: 0.97 / Actor loss: -0.00 / Critic loss: 0.0167\n",
      "6530 Episode / Step: 36278 / Score: 0.78 / Actor loss: 0.01 / Critic loss: 0.0329\n",
      "6540 Episode / Step: 36319 / Score: 0.97 / Actor loss: 0.01 / Critic loss: 0.0173\n",
      "6550 Episode / Step: 36355 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0194\n",
      "6560 Episode / Step: 36385 / Score: 0.58 / Actor loss: -0.03 / Critic loss: 0.1852\n",
      "6570 Episode / Step: 36421 / Score: 0.77 / Actor loss: 0.01 / Critic loss: 0.0688\n",
      "6580 Episode / Step: 36453 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0111\n",
      "6590 Episode / Step: 36487 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0785\n",
      "6600 Episode / Step: 36527 / Score: 0.97 / Actor loss: -0.00 / Critic loss: 0.0140\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "6610 Episode / Step: 36564 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0117\n",
      "6620 Episode / Step: 36593 / Score: 0.58 / Actor loss: -0.10 / Critic loss: 0.1468\n",
      "6630 Episode / Step: 36624 / Score: 0.98 / Actor loss: 0.01 / Critic loss: 0.0265\n",
      "6640 Episode / Step: 36651 / Score: 0.58 / Actor loss: -0.09 / Critic loss: 0.1548\n",
      "6650 Episode / Step: 36712 / Score: 0.95 / Actor loss: -0.04 / Critic loss: 0.0262\n",
      "6660 Episode / Step: 36741 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0146\n",
      "6670 Episode / Step: 36782 / Score: 0.97 / Actor loss: -0.00 / Critic loss: 0.0082\n",
      "6680 Episode / Step: 36819 / Score: 0.97 / Actor loss: -0.02 / Critic loss: 0.0124\n",
      "6690 Episode / Step: 36854 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0698\n",
      "6700 Episode / Step: 36886 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0045\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "6710 Episode / Step: 36921 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0085\n",
      "6720 Episode / Step: 36952 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0102\n",
      "6730 Episode / Step: 36975 / Score: 0.79 / Actor loss: -0.00 / Critic loss: 0.0872\n",
      "6740 Episode / Step: 37006 / Score: 0.78 / Actor loss: -0.03 / Critic loss: 0.0499\n",
      "6750 Episode / Step: 37041 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0916\n",
      "6760 Episode / Step: 37069 / Score: 0.78 / Actor loss: -0.03 / Critic loss: 0.1011\n",
      "6770 Episode / Step: 37103 / Score: 0.98 / Actor loss: -0.00 / Critic loss: 0.0175\n",
      "6780 Episode / Step: 37132 / Score: 0.98 / Actor loss: -0.00 / Critic loss: 0.0117\n",
      "6790 Episode / Step: 37160 / Score: 0.98 / Actor loss: -0.00 / Critic loss: 0.0197\n",
      "6800 Episode / Step: 37201 / Score: 0.57 / Actor loss: 0.00 / Critic loss: 0.1136\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "6810 Episode / Step: 37235 / Score: 0.78 / Actor loss: -0.00 / Critic loss: 0.0557\n",
      "6820 Episode / Step: 37278 / Score: 0.97 / Actor loss: -0.00 / Critic loss: 0.0166\n",
      "6830 Episode / Step: 37308 / Score: 0.78 / Actor loss: -0.00 / Critic loss: 0.0912\n",
      "6840 Episode / Step: 37347 / Score: 0.97 / Actor loss: -0.03 / Critic loss: 0.0126\n",
      "6850 Episode / Step: 37374 / Score: 0.38 / Actor loss: -0.00 / Critic loss: 0.1983\n",
      "6860 Episode / Step: 37410 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0408\n",
      "6870 Episode / Step: 37445 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0839\n",
      "6880 Episode / Step: 37473 / Score: 0.78 / Actor loss: -0.00 / Critic loss: 0.0822\n",
      "6890 Episode / Step: 37505 / Score: 0.38 / Actor loss: 0.00 / Critic loss: 0.1808\n",
      "6900 Episode / Step: 37525 / Score: 0.59 / Actor loss: -0.03 / Critic loss: 0.2001\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "6910 Episode / Step: 37556 / Score: 0.58 / Actor loss: -0.02 / Critic loss: 0.1117\n",
      "6920 Episode / Step: 37589 / Score: 0.58 / Actor loss: -0.04 / Critic loss: 0.0790\n",
      "6930 Episode / Step: 37623 / Score: 0.98 / Actor loss: 0.01 / Critic loss: 0.0183\n",
      "6940 Episode / Step: 37646 / Score: 0.79 / Actor loss: -0.03 / Critic loss: 0.1756\n",
      "6950 Episode / Step: 37678 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0182\n",
      "6960 Episode / Step: 37717 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0072\n",
      "6970 Episode / Step: 37753 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0756\n",
      "6980 Episode / Step: 37785 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0092\n",
      "6990 Episode / Step: 37814 / Score: 0.58 / Actor loss: -0.01 / Critic loss: 0.1512\n",
      "7000 Episode / Step: 37843 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0263\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "7010 Episode / Step: 37888 / Score: 0.77 / Actor loss: -0.02 / Critic loss: 0.0436\n",
      "7020 Episode / Step: 37923 / Score: 0.38 / Actor loss: -0.01 / Critic loss: 0.1516\n",
      "7030 Episode / Step: 37976 / Score: 0.96 / Actor loss: 0.00 / Critic loss: 0.0250\n",
      "7040 Episode / Step: 38007 / Score: 0.58 / Actor loss: -0.00 / Critic loss: 0.1381\n",
      "7050 Episode / Step: 38042 / Score: 0.78 / Actor loss: 0.01 / Critic loss: 0.0534\n",
      "7060 Episode / Step: 38074 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0195\n",
      "7070 Episode / Step: 38096 / Score: 0.99 / Actor loss: 0.01 / Critic loss: 0.0266\n",
      "7080 Episode / Step: 38125 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0894\n",
      "7090 Episode / Step: 38162 / Score: 0.37 / Actor loss: -0.09 / Critic loss: 0.1749\n",
      "7100 Episode / Step: 38190 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0976\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "7110 Episode / Step: 38220 / Score: 0.78 / Actor loss: -0.09 / Critic loss: 0.0980\n",
      "7120 Episode / Step: 38250 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0219\n",
      "7130 Episode / Step: 38290 / Score: 0.97 / Actor loss: -0.02 / Critic loss: 0.0125\n",
      "7140 Episode / Step: 38324 / Score: 0.38 / Actor loss: -0.05 / Critic loss: 0.1584\n",
      "7150 Episode / Step: 38358 / Score: 0.98 / Actor loss: -0.05 / Critic loss: 0.0389\n",
      "7160 Episode / Step: 38401 / Score: 0.97 / Actor loss: -0.00 / Critic loss: 0.0147\n",
      "7170 Episode / Step: 38427 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0186\n",
      "7180 Episode / Step: 38460 / Score: 0.78 / Actor loss: -0.02 / Critic loss: 0.0560\n",
      "7190 Episode / Step: 38484 / Score: 0.79 / Actor loss: -0.03 / Critic loss: 0.1083\n",
      "7200 Episode / Step: 38528 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0063\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "7210 Episode / Step: 38553 / Score: 0.79 / Actor loss: -0.02 / Critic loss: 0.0846\n",
      "7220 Episode / Step: 38586 / Score: 0.58 / Actor loss: -0.01 / Critic loss: 0.0938\n",
      "7230 Episode / Step: 38623 / Score: 0.97 / Actor loss: -0.04 / Critic loss: 0.0149\n",
      "7240 Episode / Step: 38655 / Score: 0.98 / Actor loss: -0.03 / Critic loss: 0.0100\n",
      "7250 Episode / Step: 38678 / Score: 0.99 / Actor loss: -0.02 / Critic loss: 0.0264\n",
      "7260 Episode / Step: 38717 / Score: 0.97 / Actor loss: -0.02 / Critic loss: 0.0109\n",
      "7270 Episode / Step: 38753 / Score: 0.57 / Actor loss: -0.04 / Critic loss: 0.1271\n",
      "7280 Episode / Step: 38776 / Score: 0.79 / Actor loss: 0.00 / Critic loss: 0.1317\n",
      "7290 Episode / Step: 38802 / Score: 0.98 / Actor loss: 0.01 / Critic loss: 0.0354\n",
      "7300 Episode / Step: 38832 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.1380\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "7310 Episode / Step: 38872 / Score: 0.77 / Actor loss: -0.00 / Critic loss: 0.0746\n",
      "7320 Episode / Step: 38895 / Score: 0.79 / Actor loss: 0.00 / Critic loss: 0.0683\n",
      "7330 Episode / Step: 38927 / Score: 0.78 / Actor loss: -0.02 / Critic loss: 0.0640\n",
      "7340 Episode / Step: 38953 / Score: 0.78 / Actor loss: -0.03 / Critic loss: 0.1333\n",
      "7350 Episode / Step: 38986 / Score: 0.38 / Actor loss: -0.01 / Critic loss: 0.1299\n",
      "7360 Episode / Step: 39027 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0261\n",
      "7370 Episode / Step: 39064 / Score: 0.77 / Actor loss: 0.01 / Critic loss: 0.0690\n",
      "7380 Episode / Step: 39093 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0898\n",
      "7390 Episode / Step: 39122 / Score: 0.78 / Actor loss: -0.03 / Critic loss: 0.0714\n",
      "7400 Episode / Step: 39161 / Score: 0.77 / Actor loss: -0.06 / Critic loss: 0.0460\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "7410 Episode / Step: 39186 / Score: 0.99 / Actor loss: 0.00 / Critic loss: 0.0214\n",
      "7420 Episode / Step: 39217 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0106\n",
      "7430 Episode / Step: 39249 / Score: 0.78 / Actor loss: 0.02 / Critic loss: 0.0787\n",
      "7440 Episode / Step: 39280 / Score: 0.58 / Actor loss: -0.11 / Critic loss: 0.1285\n",
      "7450 Episode / Step: 39315 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0144\n",
      "7460 Episode / Step: 39347 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0643\n",
      "7470 Episode / Step: 39375 / Score: 0.78 / Actor loss: -0.03 / Critic loss: 0.0835\n",
      "7480 Episode / Step: 39402 / Score: 0.58 / Actor loss: -0.08 / Critic loss: 0.1687\n",
      "7490 Episode / Step: 39428 / Score: 0.78 / Actor loss: 0.02 / Critic loss: 0.0770\n",
      "7500 Episode / Step: 39465 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0171\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "7510 Episode / Step: 39504 / Score: 0.97 / Actor loss: -0.02 / Critic loss: 0.0170\n",
      "7520 Episode / Step: 39529 / Score: 0.79 / Actor loss: -0.01 / Critic loss: 0.0973\n",
      "7530 Episode / Step: 39556 / Score: 0.78 / Actor loss: -0.00 / Critic loss: 0.0827\n",
      "7540 Episode / Step: 39579 / Score: 0.59 / Actor loss: -0.07 / Critic loss: 0.1579\n",
      "7550 Episode / Step: 39609 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0214\n",
      "7560 Episode / Step: 39657 / Score: 0.56 / Actor loss: -0.03 / Critic loss: 0.0721\n",
      "7570 Episode / Step: 39695 / Score: 0.97 / Actor loss: 0.01 / Critic loss: 0.0219\n",
      "7580 Episode / Step: 39719 / Score: 0.99 / Actor loss: -0.00 / Critic loss: 0.0197\n",
      "7590 Episode / Step: 39745 / Score: 0.58 / Actor loss: -0.00 / Critic loss: 0.1637\n",
      "7600 Episode / Step: 39781 / Score: 0.57 / Actor loss: -0.01 / Critic loss: 0.1462\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "7610 Episode / Step: 39815 / Score: 0.58 / Actor loss: -0.04 / Critic loss: 0.1361\n",
      "7620 Episode / Step: 39847 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0545\n",
      "7630 Episode / Step: 39882 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0225\n",
      "7640 Episode / Step: 39906 / Score: 0.59 / Actor loss: -0.01 / Critic loss: 0.1243\n",
      "7650 Episode / Step: 39937 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0998\n",
      "7660 Episode / Step: 39965 / Score: 0.38 / Actor loss: -0.02 / Critic loss: 0.1660\n",
      "7670 Episode / Step: 39991 / Score: 0.78 / Actor loss: -0.03 / Critic loss: 0.1125\n",
      "7680 Episode / Step: 40027 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0147\n",
      "7690 Episode / Step: 40061 / Score: 0.98 / Actor loss: -0.03 / Critic loss: 0.0218\n",
      "7700 Episode / Step: 40091 / Score: 0.38 / Actor loss: -0.02 / Critic loss: 0.1836\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "7710 Episode / Step: 40124 / Score: 0.78 / Actor loss: -0.03 / Critic loss: 0.0871\n",
      "7720 Episode / Step: 40159 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0546\n",
      "7730 Episode / Step: 40189 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0306\n",
      "7740 Episode / Step: 40220 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0119\n",
      "7750 Episode / Step: 40261 / Score: 0.57 / Actor loss: 0.00 / Critic loss: 0.1117\n",
      "7760 Episode / Step: 40300 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0145\n",
      "7770 Episode / Step: 40336 / Score: 0.77 / Actor loss: -0.02 / Critic loss: 0.0911\n",
      "7780 Episode / Step: 40371 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0718\n",
      "7790 Episode / Step: 40399 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0174\n",
      "7800 Episode / Step: 40428 / Score: 0.58 / Actor loss: -0.04 / Critic loss: 0.1428\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "7810 Episode / Step: 40462 / Score: 0.78 / Actor loss: -0.00 / Critic loss: 0.0586\n",
      "7820 Episode / Step: 40493 / Score: 0.58 / Actor loss: -0.00 / Critic loss: 0.1305\n",
      "7830 Episode / Step: 40521 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0830\n",
      "7840 Episode / Step: 40555 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.1384\n",
      "7850 Episode / Step: 40585 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0617\n",
      "7860 Episode / Step: 40625 / Score: 0.97 / Actor loss: -0.00 / Critic loss: 0.0206\n",
      "7870 Episode / Step: 40658 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0179\n",
      "7880 Episode / Step: 40689 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0747\n",
      "7890 Episode / Step: 40771 / Score: 0.73 / Actor loss: 0.04 / Critic loss: 0.0522\n",
      "7900 Episode / Step: 40805 / Score: 0.58 / Actor loss: -0.01 / Critic loss: 0.1379\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "7910 Episode / Step: 40838 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.1150\n",
      "7920 Episode / Step: 40866 / Score: 0.38 / Actor loss: -0.05 / Critic loss: 0.2048\n",
      "7930 Episode / Step: 40893 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0378\n",
      "7940 Episode / Step: 40924 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0634\n",
      "7950 Episode / Step: 40959 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0231\n",
      "7960 Episode / Step: 40989 / Score: 0.78 / Actor loss: -0.02 / Critic loss: 0.0896\n",
      "7970 Episode / Step: 41015 / Score: 0.58 / Actor loss: -0.02 / Critic loss: 0.1667\n",
      "7980 Episode / Step: 41044 / Score: 0.78 / Actor loss: -0.02 / Critic loss: 0.0844\n",
      "7990 Episode / Step: 41075 / Score: 0.78 / Actor loss: 0.01 / Critic loss: 0.0595\n",
      "8000 Episode / Step: 41108 / Score: 0.58 / Actor loss: -0.04 / Critic loss: 0.1391\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "8010 Episode / Step: 41139 / Score: 0.58 / Actor loss: -0.02 / Critic loss: 0.1076\n",
      "8020 Episode / Step: 41168 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0483\n",
      "8030 Episode / Step: 41203 / Score: 0.78 / Actor loss: -0.00 / Critic loss: 0.0658\n",
      "8040 Episode / Step: 41231 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0471\n",
      "8050 Episode / Step: 41267 / Score: 0.77 / Actor loss: -0.08 / Critic loss: 0.0805\n",
      "8060 Episode / Step: 41304 / Score: 0.57 / Actor loss: -0.00 / Critic loss: 0.1155\n",
      "8070 Episode / Step: 41345 / Score: 0.97 / Actor loss: -0.03 / Critic loss: 0.0173\n",
      "8080 Episode / Step: 41370 / Score: 0.79 / Actor loss: -0.01 / Critic loss: 0.1169\n",
      "8090 Episode / Step: 41412 / Score: 0.97 / Actor loss: -0.03 / Critic loss: 0.0153\n",
      "8100 Episode / Step: 41440 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0750\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "8110 Episode / Step: 41469 / Score: 0.78 / Actor loss: -0.03 / Critic loss: 0.0740\n",
      "8120 Episode / Step: 41501 / Score: 0.58 / Actor loss: -0.02 / Critic loss: 0.1461\n",
      "8130 Episode / Step: 41529 / Score: 0.58 / Actor loss: -0.02 / Critic loss: 0.1441\n",
      "8140 Episode / Step: 41565 / Score: 0.77 / Actor loss: 0.01 / Critic loss: 0.0559\n",
      "8150 Episode / Step: 41596 / Score: 0.78 / Actor loss: -0.02 / Critic loss: 0.1182\n",
      "8160 Episode / Step: 41623 / Score: 0.98 / Actor loss: -0.03 / Critic loss: 0.0149\n",
      "8170 Episode / Step: 41651 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0224\n",
      "8180 Episode / Step: 41687 / Score: 0.57 / Actor loss: 0.02 / Critic loss: 0.0698\n",
      "8190 Episode / Step: 41715 / Score: 0.38 / Actor loss: -0.08 / Critic loss: 0.2072\n",
      "8200 Episode / Step: 41754 / Score: 0.77 / Actor loss: 0.01 / Critic loss: 0.0796\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "8210 Episode / Step: 41790 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0127\n",
      "8220 Episode / Step: 41826 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0103\n",
      "8230 Episode / Step: 41852 / Score: 0.78 / Actor loss: -0.00 / Critic loss: 0.0792\n",
      "8240 Episode / Step: 41888 / Score: 0.77 / Actor loss: -0.00 / Critic loss: 0.0600\n",
      "8250 Episode / Step: 41925 / Score: 0.97 / Actor loss: -0.04 / Critic loss: 0.0214\n",
      "8260 Episode / Step: 41948 / Score: 0.99 / Actor loss: -0.07 / Critic loss: 0.0126\n",
      "8270 Episode / Step: 41984 / Score: 0.77 / Actor loss: -0.00 / Critic loss: 0.0610\n",
      "8280 Episode / Step: 42009 / Score: 0.99 / Actor loss: -0.01 / Critic loss: 0.0084\n",
      "8290 Episode / Step: 42043 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0158\n",
      "8300 Episode / Step: 42080 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0144\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "8310 Episode / Step: 42127 / Score: 0.76 / Actor loss: -0.02 / Critic loss: 0.0692\n",
      "8320 Episode / Step: 42162 / Score: 0.78 / Actor loss: 0.01 / Critic loss: 0.0604\n",
      "8330 Episode / Step: 42197 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0158\n",
      "8340 Episode / Step: 42241 / Score: 0.97 / Actor loss: -0.02 / Critic loss: 0.0115\n",
      "8350 Episode / Step: 42272 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0645\n",
      "8360 Episode / Step: 42311 / Score: 0.97 / Actor loss: -0.00 / Critic loss: 0.0117\n",
      "8370 Episode / Step: 42342 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0868\n",
      "8380 Episode / Step: 42370 / Score: 0.98 / Actor loss: -0.03 / Critic loss: 0.0152\n",
      "8390 Episode / Step: 42400 / Score: 0.78 / Actor loss: -0.05 / Critic loss: 0.0674\n",
      "8400 Episode / Step: 42438 / Score: 0.57 / Actor loss: -0.02 / Critic loss: 0.0663\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "8410 Episode / Step: 42468 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0127\n",
      "8420 Episode / Step: 42488 / Score: 0.79 / Actor loss: -0.02 / Critic loss: 0.1252\n",
      "8430 Episode / Step: 42522 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0110\n",
      "8440 Episode / Step: 42544 / Score: 0.79 / Actor loss: -0.01 / Critic loss: 0.0996\n",
      "8450 Episode / Step: 42584 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0161\n",
      "8460 Episode / Step: 42609 / Score: 0.79 / Actor loss: -0.06 / Critic loss: 0.0877\n",
      "8470 Episode / Step: 42645 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0111\n",
      "8480 Episode / Step: 42686 / Score: 0.97 / Actor loss: -0.00 / Critic loss: 0.0121\n",
      "8490 Episode / Step: 42728 / Score: 0.37 / Actor loss: -0.07 / Critic loss: 0.1373\n",
      "8500 Episode / Step: 42763 / Score: 0.78 / Actor loss: -0.06 / Critic loss: 0.0804\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "8510 Episode / Step: 42807 / Score: 0.77 / Actor loss: -0.02 / Critic loss: 0.0603\n",
      "8520 Episode / Step: 42840 / Score: 0.58 / Actor loss: -0.03 / Critic loss: 0.0807\n",
      "8530 Episode / Step: 42879 / Score: 0.97 / Actor loss: -0.00 / Critic loss: 0.0223\n",
      "8540 Episode / Step: 42909 / Score: 0.58 / Actor loss: -0.01 / Critic loss: 0.1539\n",
      "8550 Episode / Step: 42951 / Score: 0.97 / Actor loss: -0.00 / Critic loss: 0.0150\n",
      "8560 Episode / Step: 42981 / Score: 0.58 / Actor loss: -0.03 / Critic loss: 0.1521\n",
      "8570 Episode / Step: 43009 / Score: 0.58 / Actor loss: -0.01 / Critic loss: 0.1228\n",
      "8580 Episode / Step: 43049 / Score: 0.57 / Actor loss: -0.02 / Critic loss: 0.0974\n",
      "8590 Episode / Step: 43080 / Score: 0.78 / Actor loss: -0.02 / Critic loss: 0.0747\n",
      "8600 Episode / Step: 43125 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0388\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "8610 Episode / Step: 43157 / Score: 0.58 / Actor loss: -0.02 / Critic loss: 0.1283\n",
      "8620 Episode / Step: 43192 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0252\n",
      "8630 Episode / Step: 43232 / Score: 0.77 / Actor loss: -0.09 / Critic loss: 0.0880\n",
      "8640 Episode / Step: 43260 / Score: 0.78 / Actor loss: -0.03 / Critic loss: 0.0768\n",
      "8650 Episode / Step: 43292 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0142\n",
      "8660 Episode / Step: 43325 / Score: 0.58 / Actor loss: 0.01 / Critic loss: 0.0889\n",
      "8670 Episode / Step: 43354 / Score: 0.98 / Actor loss: -0.00 / Critic loss: 0.0275\n",
      "8680 Episode / Step: 43383 / Score: 0.78 / Actor loss: -0.00 / Critic loss: 0.1107\n",
      "8690 Episode / Step: 43413 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0114\n",
      "8700 Episode / Step: 43444 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0699\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "8710 Episode / Step: 43481 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0123\n",
      "8720 Episode / Step: 43508 / Score: 0.58 / Actor loss: -0.03 / Critic loss: 0.1424\n",
      "8730 Episode / Step: 43536 / Score: 0.98 / Actor loss: 0.01 / Critic loss: 0.0177\n",
      "8740 Episode / Step: 43570 / Score: 0.58 / Actor loss: -0.05 / Critic loss: 0.1183\n",
      "8750 Episode / Step: 43608 / Score: 0.77 / Actor loss: 0.01 / Critic loss: 0.0828\n",
      "8760 Episode / Step: 43639 / Score: 0.38 / Actor loss: -0.05 / Critic loss: 0.2104\n",
      "8770 Episode / Step: 43674 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0301\n",
      "8780 Episode / Step: 43718 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0701\n",
      "8790 Episode / Step: 43758 / Score: 0.77 / Actor loss: -0.02 / Critic loss: 0.0541\n",
      "8800 Episode / Step: 43796 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0104\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "8810 Episode / Step: 43831 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0086\n",
      "8820 Episode / Step: 43861 / Score: 0.58 / Actor loss: -0.01 / Critic loss: 0.0692\n",
      "8830 Episode / Step: 43897 / Score: 0.97 / Actor loss: 0.01 / Critic loss: 0.0198\n",
      "8840 Episode / Step: 43935 / Score: 0.77 / Actor loss: -0.15 / Critic loss: 0.0608\n",
      "8850 Episode / Step: 43975 / Score: 0.77 / Actor loss: -0.04 / Critic loss: 0.0828\n",
      "8860 Episode / Step: 44003 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0168\n",
      "8870 Episode / Step: 44036 / Score: 0.78 / Actor loss: -0.07 / Critic loss: 0.0549\n",
      "8880 Episode / Step: 44063 / Score: 0.78 / Actor loss: -0.02 / Critic loss: 0.0939\n",
      "8890 Episode / Step: 44092 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0187\n",
      "8900 Episode / Step: 44124 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0629\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "8910 Episode / Step: 44153 / Score: 0.98 / Actor loss: -0.03 / Critic loss: 0.0179\n",
      "8920 Episode / Step: 44174 / Score: 0.79 / Actor loss: -0.01 / Critic loss: 0.0642\n",
      "8930 Episode / Step: 44209 / Score: 0.78 / Actor loss: -0.00 / Critic loss: 0.0539\n",
      "8940 Episode / Step: 44241 / Score: 0.38 / Actor loss: -0.01 / Critic loss: 0.1421\n",
      "8950 Episode / Step: 44279 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0427\n",
      "8960 Episode / Step: 44316 / Score: 0.77 / Actor loss: -0.00 / Critic loss: 0.0495\n",
      "8970 Episode / Step: 44355 / Score: 0.57 / Actor loss: -0.06 / Critic loss: 0.1031\n",
      "8980 Episode / Step: 44390 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0404\n",
      "8990 Episode / Step: 44425 / Score: 0.58 / Actor loss: -0.01 / Critic loss: 0.1140\n",
      "9000 Episode / Step: 44471 / Score: 0.96 / Actor loss: -0.00 / Critic loss: 0.0087\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "9010 Episode / Step: 44502 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0154\n",
      "9020 Episode / Step: 44520 / Score: 0.79 / Actor loss: 0.00 / Critic loss: 0.1220\n",
      "9030 Episode / Step: 44542 / Score: 0.79 / Actor loss: -0.00 / Critic loss: 0.1018\n",
      "9040 Episode / Step: 44574 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0138\n",
      "9050 Episode / Step: 44612 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0599\n",
      "9060 Episode / Step: 44648 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0106\n",
      "9070 Episode / Step: 44677 / Score: 0.98 / Actor loss: -0.00 / Critic loss: 0.0118\n",
      "9080 Episode / Step: 44704 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0056\n",
      "9090 Episode / Step: 44727 / Score: 0.59 / Actor loss: -0.03 / Critic loss: 0.1477\n",
      "9100 Episode / Step: 44762 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0670\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "9110 Episode / Step: 44793 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0674\n",
      "9120 Episode / Step: 44827 / Score: 0.78 / Actor loss: -0.05 / Critic loss: 0.0421\n",
      "9130 Episode / Step: 44867 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0685\n",
      "9140 Episode / Step: 44896 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0153\n",
      "9150 Episode / Step: 44933 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0091\n",
      "9160 Episode / Step: 44969 / Score: 0.57 / Actor loss: -0.02 / Critic loss: 0.1183\n",
      "9170 Episode / Step: 45002 / Score: 0.58 / Actor loss: -0.02 / Critic loss: 0.1221\n",
      "9180 Episode / Step: 45034 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0754\n",
      "9190 Episode / Step: 45066 / Score: 0.78 / Actor loss: -0.03 / Critic loss: 0.0681\n",
      "9200 Episode / Step: 45092 / Score: 0.78 / Actor loss: 0.02 / Critic loss: 0.0608\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "9210 Episode / Step: 45122 / Score: 0.58 / Actor loss: -0.03 / Critic loss: 0.1173\n",
      "9220 Episode / Step: 45159 / Score: 0.97 / Actor loss: -0.04 / Critic loss: 0.0130\n",
      "9230 Episode / Step: 45195 / Score: 0.57 / Actor loss: -0.02 / Critic loss: 0.0676\n",
      "9240 Episode / Step: 45223 / Score: 0.78 / Actor loss: -0.00 / Critic loss: 0.0468\n",
      "9250 Episode / Step: 45255 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0911\n",
      "9260 Episode / Step: 45297 / Score: 0.97 / Actor loss: 0.01 / Critic loss: 0.0190\n",
      "9270 Episode / Step: 45331 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0181\n",
      "9280 Episode / Step: 45352 / Score: 0.99 / Actor loss: -0.00 / Critic loss: 0.0242\n",
      "9290 Episode / Step: 45387 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0673\n",
      "9300 Episode / Step: 45420 / Score: 0.98 / Actor loss: -0.00 / Critic loss: 0.0147\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "9310 Episode / Step: 45454 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0588\n",
      "9320 Episode / Step: 45496 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0205\n",
      "9330 Episode / Step: 45529 / Score: 0.98 / Actor loss: -0.05 / Critic loss: 0.0096\n",
      "9340 Episode / Step: 45559 / Score: 0.78 / Actor loss: -0.06 / Critic loss: 0.1085\n",
      "9350 Episode / Step: 45599 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0448\n",
      "9360 Episode / Step: 45631 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0762\n",
      "9370 Episode / Step: 45658 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0096\n",
      "9380 Episode / Step: 45692 / Score: 0.58 / Actor loss: -0.03 / Critic loss: 0.1090\n",
      "9390 Episode / Step: 45729 / Score: 0.57 / Actor loss: -0.05 / Critic loss: 0.1251\n",
      "9400 Episode / Step: 45772 / Score: 0.77 / Actor loss: -0.02 / Critic loss: 0.0673\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "9410 Episode / Step: 45799 / Score: 0.98 / Actor loss: -0.00 / Critic loss: 0.0098\n",
      "9420 Episode / Step: 45830 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0133\n",
      "9430 Episode / Step: 45859 / Score: 0.98 / Actor loss: 0.01 / Critic loss: 0.0138\n",
      "9440 Episode / Step: 45885 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.1070\n",
      "9450 Episode / Step: 45905 / Score: 0.39 / Actor loss: 0.00 / Critic loss: 0.2425\n",
      "9460 Episode / Step: 45933 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0789\n",
      "9470 Episode / Step: 45962 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0299\n",
      "9480 Episode / Step: 45992 / Score: 0.78 / Actor loss: -0.05 / Critic loss: 0.0619\n",
      "9490 Episode / Step: 46022 / Score: 0.78 / Actor loss: 0.01 / Critic loss: 0.0906\n",
      "9500 Episode / Step: 46055 / Score: 0.98 / Actor loss: -0.00 / Critic loss: 0.0132\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "9510 Episode / Step: 46093 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0132\n",
      "9520 Episode / Step: 46123 / Score: 0.58 / Actor loss: -0.07 / Critic loss: 0.1251\n",
      "9530 Episode / Step: 46157 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0179\n",
      "9540 Episode / Step: 46186 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0148\n",
      "9550 Episode / Step: 46218 / Score: 0.78 / Actor loss: 0.01 / Critic loss: 0.0781\n",
      "9560 Episode / Step: 46249 / Score: 0.78 / Actor loss: -0.00 / Critic loss: 0.0600\n",
      "9570 Episode / Step: 46281 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0130\n",
      "9580 Episode / Step: 46311 / Score: 0.98 / Actor loss: -0.07 / Critic loss: 0.0147\n",
      "9590 Episode / Step: 46341 / Score: 0.78 / Actor loss: -0.06 / Critic loss: 0.0992\n",
      "9600 Episode / Step: 46380 / Score: 0.97 / Actor loss: -0.03 / Critic loss: 0.0106\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "9610 Episode / Step: 46407 / Score: 0.78 / Actor loss: -0.02 / Critic loss: 0.1072\n",
      "9620 Episode / Step: 46444 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0086\n",
      "9630 Episode / Step: 46483 / Score: 0.97 / Actor loss: -0.04 / Critic loss: 0.0109\n",
      "9640 Episode / Step: 46522 / Score: 0.97 / Actor loss: 0.01 / Critic loss: 0.0130\n",
      "9650 Episode / Step: 46545 / Score: 0.99 / Actor loss: -0.01 / Critic loss: 0.0133\n",
      "9660 Episode / Step: 46578 / Score: 0.98 / Actor loss: -0.00 / Critic loss: 0.0111\n",
      "9670 Episode / Step: 46615 / Score: 0.77 / Actor loss: 0.03 / Critic loss: 0.0699\n",
      "9680 Episode / Step: 46641 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0583\n",
      "9690 Episode / Step: 46665 / Score: 0.79 / Actor loss: -0.00 / Critic loss: 0.0694\n",
      "9700 Episode / Step: 46694 / Score: 0.58 / Actor loss: -0.01 / Critic loss: 0.1146\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "9710 Episode / Step: 46723 / Score: 0.58 / Actor loss: -0.00 / Critic loss: 0.1326\n",
      "9720 Episode / Step: 46759 / Score: 0.97 / Actor loss: -0.00 / Critic loss: 0.0136\n",
      "9730 Episode / Step: 46794 / Score: 0.78 / Actor loss: -0.00 / Critic loss: 0.0749\n",
      "9740 Episode / Step: 46828 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0474\n",
      "9750 Episode / Step: 46863 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0222\n",
      "9760 Episode / Step: 46895 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0147\n",
      "9770 Episode / Step: 46935 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0135\n",
      "9780 Episode / Step: 46976 / Score: 0.77 / Actor loss: -0.03 / Critic loss: 0.0439\n",
      "9790 Episode / Step: 47005 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0970\n",
      "9800 Episode / Step: 47038 / Score: 0.78 / Actor loss: -0.02 / Critic loss: 0.0979\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "9810 Episode / Step: 47080 / Score: 0.97 / Actor loss: -0.02 / Critic loss: 0.0121\n",
      "9820 Episode / Step: 47106 / Score: 0.58 / Actor loss: -0.04 / Critic loss: 0.1211\n",
      "9830 Episode / Step: 47137 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0135\n",
      "9840 Episode / Step: 47162 / Score: 0.59 / Actor loss: -0.06 / Critic loss: 0.1783\n",
      "9850 Episode / Step: 47199 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0661\n",
      "9860 Episode / Step: 47234 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0111\n",
      "9870 Episode / Step: 47262 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0724\n",
      "9880 Episode / Step: 47283 / Score: 0.79 / Actor loss: 0.01 / Critic loss: 0.1045\n",
      "9890 Episode / Step: 47314 / Score: 0.98 / Actor loss: -0.00 / Critic loss: 0.0117\n",
      "9900 Episode / Step: 47340 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0156\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "9910 Episode / Step: 47368 / Score: 0.98 / Actor loss: 0.01 / Critic loss: 0.0150\n",
      "9920 Episode / Step: 47405 / Score: 0.97 / Actor loss: -0.06 / Critic loss: 0.0103\n",
      "9930 Episode / Step: 47434 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0794\n",
      "9940 Episode / Step: 47471 / Score: 0.77 / Actor loss: 0.01 / Critic loss: 0.0723\n",
      "9950 Episode / Step: 47513 / Score: 0.77 / Actor loss: -0.06 / Critic loss: 0.0941\n",
      "9960 Episode / Step: 47539 / Score: 0.78 / Actor loss: -0.00 / Critic loss: 0.0597\n",
      "9970 Episode / Step: 47575 / Score: 0.97 / Actor loss: -0.02 / Critic loss: 0.0120\n",
      "9980 Episode / Step: 47613 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0122\n",
      "9990 Episode / Step: 47642 / Score: 0.98 / Actor loss: -0.03 / Critic loss: 0.0132\n",
      "10000 Episode / Step: 47676 / Score: 0.98 / Actor loss: -0.00 / Critic loss: 0.0114\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "10010 Episode / Step: 47703 / Score: 0.58 / Actor loss: -0.09 / Critic loss: 0.1923\n",
      "10020 Episode / Step: 47732 / Score: 0.78 / Actor loss: 0.01 / Critic loss: 0.0574\n",
      "10030 Episode / Step: 47764 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0821\n",
      "10040 Episode / Step: 47789 / Score: 0.59 / Actor loss: 0.01 / Critic loss: 0.1362\n",
      "10050 Episode / Step: 47819 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0865\n",
      "10060 Episode / Step: 47849 / Score: 0.78 / Actor loss: -0.00 / Critic loss: 0.0728\n",
      "10070 Episode / Step: 47883 / Score: 0.98 / Actor loss: 0.01 / Critic loss: 0.0168\n",
      "10080 Episode / Step: 47920 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0145\n",
      "10090 Episode / Step: 47950 / Score: 0.78 / Actor loss: -0.00 / Critic loss: 0.0689\n",
      "10100 Episode / Step: 47982 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0063\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "10110 Episode / Step: 48013 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0387\n",
      "10120 Episode / Step: 48050 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0132\n",
      "10130 Episode / Step: 48094 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0136\n",
      "10140 Episode / Step: 48123 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0111\n",
      "10150 Episode / Step: 48177 / Score: 0.96 / Actor loss: -0.04 / Critic loss: 0.0126\n",
      "10160 Episode / Step: 48197 / Score: 0.39 / Actor loss: -0.02 / Critic loss: 0.2771\n",
      "10170 Episode / Step: 48233 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0290\n",
      "10180 Episode / Step: 48256 / Score: 0.39 / Actor loss: -0.01 / Critic loss: 0.2267\n",
      "10190 Episode / Step: 48284 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0126\n",
      "10200 Episode / Step: 48316 / Score: 0.78 / Actor loss: -0.02 / Critic loss: 0.0414\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "10210 Episode / Step: 48348 / Score: 0.78 / Actor loss: -0.00 / Critic loss: 0.0853\n",
      "10220 Episode / Step: 48382 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0741\n",
      "10230 Episode / Step: 48416 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0124\n",
      "10240 Episode / Step: 48449 / Score: 0.58 / Actor loss: -0.05 / Critic loss: 0.1048\n",
      "10250 Episode / Step: 48480 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0712\n",
      "10260 Episode / Step: 48504 / Score: 0.99 / Actor loss: 0.00 / Critic loss: 0.0170\n",
      "10270 Episode / Step: 48536 / Score: 0.38 / Actor loss: -0.07 / Critic loss: 0.2312\n",
      "10280 Episode / Step: 48567 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0327\n",
      "10290 Episode / Step: 48612 / Score: 0.57 / Actor loss: -0.02 / Critic loss: 0.1441\n",
      "10300 Episode / Step: 48655 / Score: 0.77 / Actor loss: -0.03 / Critic loss: 0.0581\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "10310 Episode / Step: 48682 / Score: 0.98 / Actor loss: -0.00 / Critic loss: 0.0163\n",
      "10320 Episode / Step: 48725 / Score: 0.77 / Actor loss: -0.02 / Critic loss: 0.0463\n",
      "10330 Episode / Step: 48749 / Score: 0.79 / Actor loss: -0.03 / Critic loss: 0.0505\n",
      "10340 Episode / Step: 48781 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0245\n",
      "10350 Episode / Step: 48808 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0762\n",
      "10360 Episode / Step: 48838 / Score: 0.78 / Actor loss: -0.02 / Critic loss: 0.0637\n",
      "10370 Episode / Step: 48868 / Score: 0.58 / Actor loss: -0.01 / Critic loss: 0.1260\n",
      "10380 Episode / Step: 48904 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0577\n",
      "10390 Episode / Step: 48941 / Score: 0.97 / Actor loss: -0.01 / Critic loss: 0.0183\n",
      "10400 Episode / Step: 48970 / Score: 0.78 / Actor loss: -0.00 / Critic loss: 0.0619\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "10410 Episode / Step: 49008 / Score: 0.57 / Actor loss: -0.05 / Critic loss: 0.1052\n",
      "10420 Episode / Step: 49049 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0383\n",
      "10430 Episode / Step: 49079 / Score: 0.58 / Actor loss: -0.00 / Critic loss: 0.0825\n",
      "10440 Episode / Step: 49111 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0234\n",
      "10450 Episode / Step: 49143 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0107\n",
      "10460 Episode / Step: 49167 / Score: 0.59 / Actor loss: -0.00 / Critic loss: 0.1170\n",
      "10470 Episode / Step: 49196 / Score: 0.58 / Actor loss: -0.05 / Critic loss: 0.1582\n",
      "10480 Episode / Step: 49226 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0305\n",
      "10490 Episode / Step: 49254 / Score: 0.98 / Actor loss: -0.00 / Critic loss: 0.0194\n",
      "10500 Episode / Step: 49283 / Score: 0.78 / Actor loss: -0.31 / Critic loss: 0.1558\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "10510 Episode / Step: 49321 / Score: 0.57 / Actor loss: -0.10 / Critic loss: 0.1498\n",
      "10520 Episode / Step: 49350 / Score: 0.98 / Actor loss: -0.00 / Critic loss: 0.0149\n",
      "10530 Episode / Step: 49398 / Score: 0.76 / Actor loss: -0.01 / Critic loss: 0.0516\n",
      "10540 Episode / Step: 49423 / Score: 0.59 / Actor loss: -0.01 / Critic loss: 0.1038\n",
      "10550 Episode / Step: 49450 / Score: 0.98 / Actor loss: -0.00 / Critic loss: 0.0212\n",
      "10560 Episode / Step: 49484 / Score: 0.98 / Actor loss: -0.01 / Critic loss: 0.0094\n",
      "10570 Episode / Step: 49509 / Score: 0.39 / Actor loss: -0.01 / Critic loss: 0.1970\n",
      "10580 Episode / Step: 49547 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0317\n",
      "10590 Episode / Step: 49578 / Score: 0.58 / Actor loss: -0.07 / Critic loss: 0.1493\n",
      "10600 Episode / Step: 49614 / Score: 0.77 / Actor loss: -0.02 / Critic loss: 0.0583\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "10610 Episode / Step: 49653 / Score: 0.97 / Actor loss: -0.00 / Critic loss: 0.0255\n",
      "10620 Episode / Step: 49691 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0482\n",
      "10630 Episode / Step: 49721 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0952\n",
      "10640 Episode / Step: 49753 / Score: 0.98 / Actor loss: 0.01 / Critic loss: 0.0244\n",
      "10650 Episode / Step: 49792 / Score: 0.97 / Actor loss: -0.02 / Critic loss: 0.0240\n",
      "10660 Episode / Step: 49821 / Score: 0.78 / Actor loss: -0.01 / Critic loss: 0.0896\n",
      "10670 Episode / Step: 49851 / Score: 0.98 / Actor loss: -0.00 / Critic loss: 0.0181\n",
      "10680 Episode / Step: 49887 / Score: 0.77 / Actor loss: -0.01 / Critic loss: 0.0544\n",
      "10690 Episode / Step: 49916 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0113\n",
      "10700 Episode / Step: 49950 / Score: 0.98 / Actor loss: -0.02 / Critic loss: 0.0113\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "10710 Episode / Step: 49983 / Score: 0.98 / Actor loss: 0.01 / Critic loss: 0.0098\n",
      "... Save Model to ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "TEST START\n",
      "10720 Episode / Step: 50031 / Score: 0.96 / Actor loss: -0.01 / Critic loss: 0.0061\n",
      "10730 Episode / Step: 50063 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "10740 Episode / Step: 50098 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "10750 Episode / Step: 50129 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "10760 Episode / Step: 50162 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "10770 Episode / Step: 50186 / Score: 0.99 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "10780 Episode / Step: 50213 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "10790 Episode / Step: 50248 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "10800 Episode / Step: 50268 / Score: 0.39 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "10810 Episode / Step: 50308 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "10820 Episode / Step: 50345 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "10830 Episode / Step: 50369 / Score: 0.79 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "10840 Episode / Step: 50406 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "10850 Episode / Step: 50437 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "10860 Episode / Step: 50468 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "10870 Episode / Step: 50493 / Score: 0.79 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "10880 Episode / Step: 50528 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "10890 Episode / Step: 50559 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "10900 Episode / Step: 50591 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "10910 Episode / Step: 50623 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "10920 Episode / Step: 50655 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "10930 Episode / Step: 50691 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "10940 Episode / Step: 50723 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "10950 Episode / Step: 50750 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "10960 Episode / Step: 50781 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "10970 Episode / Step: 50806 / Score: 0.99 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "10980 Episode / Step: 50847 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "10990 Episode / Step: 50880 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11000 Episode / Step: 50903 / Score: 0.99 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11010 Episode / Step: 50930 / Score: 0.38 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11020 Episode / Step: 50965 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11030 Episode / Step: 51001 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11040 Episode / Step: 51034 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11050 Episode / Step: 51065 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11060 Episode / Step: 51093 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11070 Episode / Step: 51123 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11080 Episode / Step: 51155 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11090 Episode / Step: 51186 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11100 Episode / Step: 51218 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11110 Episode / Step: 51261 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11120 Episode / Step: 51290 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11130 Episode / Step: 51319 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11140 Episode / Step: 51350 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11150 Episode / Step: 51387 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11160 Episode / Step: 51416 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11170 Episode / Step: 51444 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11180 Episode / Step: 51475 / Score: 0.38 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11190 Episode / Step: 51508 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11200 Episode / Step: 51536 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11210 Episode / Step: 51557 / Score: 0.59 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11220 Episode / Step: 51593 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11230 Episode / Step: 51623 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11240 Episode / Step: 51659 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11250 Episode / Step: 51698 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11260 Episode / Step: 51730 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11270 Episode / Step: 51774 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11280 Episode / Step: 51800 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11290 Episode / Step: 51835 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11300 Episode / Step: 51869 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11310 Episode / Step: 51906 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11320 Episode / Step: 51941 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11330 Episode / Step: 51971 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11340 Episode / Step: 52007 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11350 Episode / Step: 52047 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11360 Episode / Step: 52082 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11370 Episode / Step: 52113 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11380 Episode / Step: 52146 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11390 Episode / Step: 52183 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11400 Episode / Step: 52218 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11410 Episode / Step: 52251 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11420 Episode / Step: 52283 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11430 Episode / Step: 52313 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11440 Episode / Step: 52341 / Score: 0.18 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11450 Episode / Step: 52369 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11460 Episode / Step: 52401 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11470 Episode / Step: 52431 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11480 Episode / Step: 52472 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11490 Episode / Step: 52498 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11500 Episode / Step: 52533 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11510 Episode / Step: 52562 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11520 Episode / Step: 52597 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11530 Episode / Step: 52638 / Score: 0.57 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11540 Episode / Step: 52673 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11550 Episode / Step: 52699 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11560 Episode / Step: 52728 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11570 Episode / Step: 52763 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11580 Episode / Step: 52798 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11590 Episode / Step: 52834 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11600 Episode / Step: 52863 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11610 Episode / Step: 52904 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11620 Episode / Step: 52926 / Score: 0.79 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11630 Episode / Step: 52953 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11640 Episode / Step: 52984 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11650 Episode / Step: 53010 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11660 Episode / Step: 53046 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11670 Episode / Step: 53083 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11680 Episode / Step: 53114 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11690 Episode / Step: 53139 / Score: 0.99 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11700 Episode / Step: 53167 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11710 Episode / Step: 53208 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11720 Episode / Step: 53255 / Score: 0.96 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11730 Episode / Step: 53281 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11740 Episode / Step: 53308 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11750 Episode / Step: 53339 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11760 Episode / Step: 53377 / Score: 0.57 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11770 Episode / Step: 53412 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11780 Episode / Step: 53443 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11790 Episode / Step: 53475 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11800 Episode / Step: 53508 / Score: 0.38 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11810 Episode / Step: 53535 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11820 Episode / Step: 53556 / Score: 0.79 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11830 Episode / Step: 53594 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11840 Episode / Step: 53624 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11850 Episode / Step: 53663 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11860 Episode / Step: 53704 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11870 Episode / Step: 53730 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11880 Episode / Step: 53766 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11890 Episode / Step: 53806 / Score: 0.57 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11900 Episode / Step: 53840 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11910 Episode / Step: 53868 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11920 Episode / Step: 53903 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11930 Episode / Step: 53936 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11940 Episode / Step: 53958 / Score: 0.79 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11950 Episode / Step: 53986 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11960 Episode / Step: 54027 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11970 Episode / Step: 54063 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11980 Episode / Step: 54094 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "11990 Episode / Step: 54136 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "12000 Episode / Step: 54175 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "12010 Episode / Step: 54205 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "12020 Episode / Step: 54240 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "12030 Episode / Step: 54285 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "12040 Episode / Step: 54319 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "12050 Episode / Step: 54355 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "12060 Episode / Step: 54395 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "12070 Episode / Step: 54420 / Score: 0.99 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "12080 Episode / Step: 54459 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "12090 Episode / Step: 54492 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "12100 Episode / Step: 54526 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "12110 Episode / Step: 54554 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "12120 Episode / Step: 54573 / Score: 0.59 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "12130 Episode / Step: 54604 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "12140 Episode / Step: 54638 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "12150 Episode / Step: 54662 / Score: 0.79 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "12160 Episode / Step: 54687 / Score: 0.59 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "12170 Episode / Step: 54736 / Score: 0.96 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "12180 Episode / Step: 54775 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "12190 Episode / Step: 54818 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "12200 Episode / Step: 54848 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "12210 Episode / Step: 54878 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "12220 Episode / Step: 54913 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "12230 Episode / Step: 54943 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "12240 Episode / Step: 54973 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 불러오기\n",
    "import numpy as np\n",
    "import datetime\n",
    "import platform\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from mlagents_envs.environment import UnityEnvironment, ActionTuple\n",
    "from mlagents_envs.side_channel.engine_configuration_channel\\\n",
    "                             import EngineConfigurationChannel\n",
    "#파라미터 값 세팅 \n",
    "state_size = 6*2\n",
    "action_size = 4 \n",
    "\n",
    "load_model = False\n",
    "train_mode = True\n",
    "\n",
    "discount_factor = 0.9\n",
    "learning_rate = 0.00025\n",
    "\n",
    "run_step = 50000 if train_mode else 0\n",
    "test_step = 5000\n",
    "\n",
    "print_interval = 10\n",
    "save_interval = 100\n",
    "\n",
    "VISUAL_OBS = 0\n",
    "GOAL_OBS = 1\n",
    "VECTOR_OBS = 2\n",
    "OBS = VECTOR_OBS\n",
    "\n",
    "# 유니티 환경 경로 \n",
    "game = \"GridWorld\"\n",
    "env_name = \"./Unity_practice/GridWorld/GridWorld\"\n",
    "\n",
    "# 모델 저장 및 불러오기 경로\n",
    "date_time = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "save_path = f\"./Unity_practice/saved_models/{game}/A2C/{date_time}\"\n",
    "load_path = f\"./Unity_practice/saved_models/{game}/A2C/20210217000848\"\n",
    "\n",
    "# 연산 장치\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# A2C 클래스 -> Actor Network, Critic Network 정의 \n",
    "class A2C(torch.nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(A2C, self).__init__(**kwargs)\n",
    "        self.d1 = torch.nn.Linear(state_size, 128)\n",
    "        self.d2 = torch.nn.Linear(128, 128)\n",
    "        self.pi = torch.nn.Linear(128, action_size)\n",
    "        self.v = torch.nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.d1(x))\n",
    "        x = F.relu(self.d2(x))\n",
    "        return F.softmax(self.pi(x), dim=1), self.v(x)\n",
    "\n",
    "# A2CAgent 클래스 -> A2C 알고리즘을 위한 다양한 함수 정의 \n",
    "class A2CAgent:\n",
    "    def __init__(self):\n",
    "        self.a2c = A2C().to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.a2c.parameters(), lr=learning_rate)\n",
    "        self.writer = SummaryWriter(save_path)\n",
    "\n",
    "        if load_model == True:\n",
    "            print(f\"... Load Model from {load_path}/ckpt ...\")\n",
    "            checkpoint = torch.load(load_path+'/ckpt', map_location=device)\n",
    "            self.a2c.load_state_dict(checkpoint[\"network\"])\n",
    "            self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    # 정책을 통해 행동 결정 \n",
    "    def get_action(self, state, training=True):\n",
    "        #  네트워크 모드 설정\n",
    "        self.a2c.train(training)\n",
    "\n",
    "        # 네트워크 연산에 따라 행동 결정\n",
    "        pi, _ = self.a2c(torch.FloatTensor(state).to(device))\n",
    "        action = torch.multinomial(pi, num_samples=1).cpu().numpy()\n",
    "        return action\n",
    "\n",
    "    # 학습 수행\n",
    "    def train_model(self, state, action, reward, next_state, done):\n",
    "        state, action, reward, next_state, done = map(lambda x: torch.FloatTensor(x).to(device),\n",
    "                                                        [state, action, reward, next_state, done])\n",
    "        pi, value = self.a2c(state)\n",
    "\n",
    "        #가치신경망\n",
    "        with torch.no_grad():\n",
    "            _, next_value = self.a2c(next_state)\n",
    "            target_value  = reward + (1-done) * discount_factor * next_value\n",
    "        critic_loss = F.mse_loss(target_value, value)\n",
    "\n",
    "        #정책신경망\n",
    "        eye = torch.eye(action_size).to(device)\n",
    "        one_hot_action = eye[action.view(-1).long()]\n",
    "        advantage = (target_value - value).detach()\n",
    "        actor_loss = -(torch.log((one_hot_action * pi).sum(1))*advantage).mean()\n",
    "        total_loss = critic_loss + actor_loss\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return actor_loss.item(), critic_loss.item()\n",
    "\n",
    "    # 네트워크 모델 저장\n",
    "    def save_model(self):\n",
    "        print(f\"... Save Model to {save_path}/ckpt ...\")\n",
    "        torch.save({\n",
    "            \"network\" : self.a2c.state_dict(),\n",
    "            \"optimizer\" : self.optimizer.state_dict(),\n",
    "        }, save_path+'/ckpt')\n",
    "\n",
    "        # 학습 기록 \n",
    "    def write_summray(self, score, actor_loss, critic_loss, step):\n",
    "        self.writer.add_scalar(\"run/score\", score, step)\n",
    "        self.writer.add_scalar(\"model/actor_loss\", actor_loss, step)\n",
    "        self.writer.add_scalar(\"model/critic_loss\", critic_loss, step)\n",
    "\n",
    "# Main 함수 -> 전체적으로 A2C 알고리즘을 진행 \n",
    "if __name__ == '__main__':\n",
    "    # 유니티 환경 경로 설정 (file_name)\n",
    "    engine_configuration_channel = EngineConfigurationChannel()\n",
    "    env = UnityEnvironment(file_name=env_name,\n",
    "                           side_channels=[engine_configuration_channel],\n",
    "                           worker_id=103)\n",
    "    env.reset()\n",
    "\n",
    "    # 유니티 브레인 설정 \n",
    "    behavior_name = list(env.behavior_specs.keys())[0]\n",
    "    spec = env.behavior_specs[behavior_name]\n",
    "    engine_configuration_channel.set_configuration_parameters(time_scale=12.0)\n",
    "    dec, term = env.get_steps(behavior_name)\n",
    "\n",
    "    # A2C 클래스를 agent로 정의 \n",
    "    agent = A2CAgent()\n",
    "    actor_losses, critic_losses, scores, episode, score = [], [], [], 0, 0\n",
    "    for step in range(run_step + test_step):\n",
    "        if step == run_step:\n",
    "            if train_mode:\n",
    "                agent.save_model()\n",
    "            print(\"TEST START\")\n",
    "            train_mode = False\n",
    "            engine_configuration_channel.set_configuration_parameters(time_scale=1.0)\n",
    "\n",
    "        preprocess = lambda obs, goal: np.concatenate((obs*goal[0][0], obs*goal[0][1]), axis=-1) \n",
    "        state = preprocess(dec.obs[OBS],dec.obs[GOAL_OBS])\n",
    "        action = agent.get_action(state, train_mode)\n",
    "        real_action = action + 1\n",
    "        action_tuple = ActionTuple()\n",
    "        action_tuple.add_discrete(real_action)\n",
    "        env.set_actions(behavior_name, action_tuple)\n",
    "        env.step()\n",
    "\n",
    "        #환경으로부터 얻는 정보\n",
    "        dec, term = env.get_steps(behavior_name)\n",
    "        done = len(term.agent_id) > 0\n",
    "        reward = term.reward if done else dec.reward\n",
    "        next_state = preprocess(term.obs[OBS], term.obs[GOAL_OBS]) if done\\\n",
    "                     else preprocess(dec.obs[OBS], dec.obs[GOAL_OBS])\n",
    "        score += reward[0]\n",
    "\n",
    "        if train_mode:\n",
    "            #학습수행\n",
    "            actor_loss, critic_loss = agent.train_model(state, action[0], [reward], next_state, [done])\n",
    "            actor_losses.append(actor_loss)\n",
    "            critic_losses.append(critic_loss)\n",
    "\n",
    "        if done:\n",
    "            episode +=1\n",
    "            scores.append(score)\n",
    "            score = 0\n",
    "\n",
    "          # 게임 진행 상황 출력 및 텐서 보드에 보상과 손실함수 값 기록 \n",
    "            if episode % print_interval == 0:\n",
    "                mean_score = np.mean(scores)\n",
    "                mean_actor_loss = np.mean(actor_losses) if len(actor_losses) > 0 else 0\n",
    "                mean_critic_loss = np.mean(critic_losses)  if len(critic_losses) > 0 else 0\n",
    "                agent.write_summray(mean_score, mean_actor_loss, mean_critic_loss, step)\n",
    "                actor_losses, critic_losses, scores = [], [], []\n",
    "\n",
    "                print(f\"{episode} Episode / Step: {step} / Score: {mean_score:.2f} / \" +\\\n",
    "                      f\"Actor loss: {mean_actor_loss:.2f} / Critic loss: {mean_critic_loss:.4f}\")\n",
    "\n",
    "            # 네트워크 모델 저장 \n",
    "            if train_mode and episode % save_interval == 0:\n",
    "                agent.save_model()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A2C Testing\n",
    "========="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Load Model from ./Unity_practice/saved_models/GridWorld/A2C/20240221183157/ckpt ...\n",
      "TEST START\n",
      "10 Episode / Step: 31 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "20 Episode / Step: 66 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "30 Episode / Step: 103 / Score: 0.57 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "40 Episode / Step: 137 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "50 Episode / Step: 168 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "60 Episode / Step: 196 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "70 Episode / Step: 219 / Score: 0.79 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "80 Episode / Step: 248 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "90 Episode / Step: 279 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "100 Episode / Step: 311 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "110 Episode / Step: 347 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "120 Episode / Step: 384 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "130 Episode / Step: 423 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "140 Episode / Step: 447 / Score: 0.59 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "150 Episode / Step: 471 / Score: 0.59 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "160 Episode / Step: 509 / Score: 0.37 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "170 Episode / Step: 545 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "180 Episode / Step: 569 / Score: 0.79 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "190 Episode / Step: 596 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "200 Episode / Step: 629 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "210 Episode / Step: 663 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "220 Episode / Step: 700 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "230 Episode / Step: 727 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "240 Episode / Step: 768 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "250 Episode / Step: 802 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "260 Episode / Step: 832 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "270 Episode / Step: 861 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "280 Episode / Step: 900 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "290 Episode / Step: 918 / Score: 0.79 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "300 Episode / Step: 959 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "310 Episode / Step: 995 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "320 Episode / Step: 1027 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "330 Episode / Step: 1058 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "340 Episode / Step: 1088 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "350 Episode / Step: 1120 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "360 Episode / Step: 1166 / Score: 0.56 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "370 Episode / Step: 1206 / Score: 0.57 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "380 Episode / Step: 1235 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "390 Episode / Step: 1270 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "400 Episode / Step: 1301 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "410 Episode / Step: 1337 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "420 Episode / Step: 1367 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "430 Episode / Step: 1393 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "440 Episode / Step: 1422 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "450 Episode / Step: 1451 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "460 Episode / Step: 1485 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "470 Episode / Step: 1521 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "480 Episode / Step: 1545 / Score: 0.79 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "490 Episode / Step: 1582 / Score: 0.37 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "500 Episode / Step: 1611 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "510 Episode / Step: 1648 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "520 Episode / Step: 1680 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "530 Episode / Step: 1713 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "540 Episode / Step: 1750 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "550 Episode / Step: 1784 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "560 Episode / Step: 1810 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "570 Episode / Step: 1841 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "580 Episode / Step: 1878 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "590 Episode / Step: 1910 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "600 Episode / Step: 1943 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "610 Episode / Step: 1974 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "620 Episode / Step: 2004 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "630 Episode / Step: 2033 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "640 Episode / Step: 2067 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "650 Episode / Step: 2089 / Score: 0.99 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "660 Episode / Step: 2112 / Score: 0.59 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "670 Episode / Step: 2147 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "680 Episode / Step: 2177 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "690 Episode / Step: 2210 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "700 Episode / Step: 2248 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "710 Episode / Step: 2281 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "720 Episode / Step: 2306 / Score: 0.79 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "730 Episode / Step: 2332 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "740 Episode / Step: 2371 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "750 Episode / Step: 2406 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "760 Episode / Step: 2434 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "770 Episode / Step: 2454 / Score: 0.39 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "780 Episode / Step: 2481 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "790 Episode / Step: 2517 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "800 Episode / Step: 2549 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "810 Episode / Step: 2582 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "820 Episode / Step: 2612 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "830 Episode / Step: 2649 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "840 Episode / Step: 2682 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "850 Episode / Step: 2708 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "860 Episode / Step: 2744 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "870 Episode / Step: 2774 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "880 Episode / Step: 2816 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "890 Episode / Step: 2860 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "900 Episode / Step: 2889 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "910 Episode / Step: 2927 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "920 Episode / Step: 2960 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "930 Episode / Step: 2993 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "940 Episode / Step: 3029 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "950 Episode / Step: 3059 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "960 Episode / Step: 3090 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "970 Episode / Step: 3123 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "980 Episode / Step: 3160 / Score: 0.57 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "990 Episode / Step: 3190 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1000 Episode / Step: 3217 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1010 Episode / Step: 3245 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1020 Episode / Step: 3277 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1030 Episode / Step: 3315 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1040 Episode / Step: 3352 / Score: 0.57 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1050 Episode / Step: 3381 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1060 Episode / Step: 3414 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1070 Episode / Step: 3448 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1080 Episode / Step: 3481 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1090 Episode / Step: 3524 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1100 Episode / Step: 3559 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1110 Episode / Step: 3590 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1120 Episode / Step: 3627 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1130 Episode / Step: 3662 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1140 Episode / Step: 3694 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1150 Episode / Step: 3730 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1160 Episode / Step: 3764 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1170 Episode / Step: 3798 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1180 Episode / Step: 3828 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1190 Episode / Step: 3861 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1200 Episode / Step: 3894 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1210 Episode / Step: 3919 / Score: 0.79 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1220 Episode / Step: 3953 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1230 Episode / Step: 3976 / Score: 0.79 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1240 Episode / Step: 4009 / Score: 0.38 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1250 Episode / Step: 4036 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1260 Episode / Step: 4067 / Score: 0.38 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1270 Episode / Step: 4096 / Score: 0.38 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1280 Episode / Step: 4118 / Score: 0.79 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1290 Episode / Step: 4159 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1300 Episode / Step: 4199 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1310 Episode / Step: 4238 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1320 Episode / Step: 4271 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1330 Episode / Step: 4307 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1340 Episode / Step: 4338 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1350 Episode / Step: 4378 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1360 Episode / Step: 4416 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1370 Episode / Step: 4448 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1380 Episode / Step: 4477 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1390 Episode / Step: 4504 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1400 Episode / Step: 4546 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1410 Episode / Step: 4590 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1420 Episode / Step: 4626 / Score: 0.77 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1430 Episode / Step: 4658 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1440 Episode / Step: 4694 / Score: 0.57 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1450 Episode / Step: 4726 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1460 Episode / Step: 4765 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1470 Episode / Step: 4800 / Score: 0.58 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1480 Episode / Step: 4831 / Score: 0.78 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1490 Episode / Step: 4860 / Score: 0.38 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1500 Episode / Step: 4890 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1510 Episode / Step: 4921 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1520 Episode / Step: 4953 / Score: 0.98 / Actor loss: 0.00 / Critic loss: 0.0000\n",
      "1530 Episode / Step: 4993 / Score: 0.97 / Actor loss: 0.00 / Critic loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 불러오기\n",
    "import numpy as np\n",
    "import datetime\n",
    "import platform\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from mlagents_envs.environment import UnityEnvironment, ActionTuple\n",
    "from mlagents_envs.side_channel.engine_configuration_channel\\\n",
    "                             import EngineConfigurationChannel\n",
    "#파라미터 값 세팅 \n",
    "state_size = 6*2\n",
    "action_size = 4 \n",
    "\n",
    "load_model = True\n",
    "train_mode = False\n",
    "\n",
    "discount_factor = 0.9\n",
    "learning_rate = 0.00025\n",
    "\n",
    "run_step = 50000 if train_mode else 0\n",
    "test_step = 5000\n",
    "\n",
    "print_interval = 10\n",
    "save_interval = 100\n",
    "\n",
    "VISUAL_OBS = 0\n",
    "GOAL_OBS = 1\n",
    "VECTOR_OBS = 2\n",
    "OBS = VECTOR_OBS\n",
    "\n",
    "# 유니티 환경 경로 \n",
    "game = \"GridWorld\"\n",
    "env_name = \"./Unity_practice/GridWorld/GridWorld\"\n",
    "\n",
    "# 모델 저장 및 불러오기 경로\n",
    "date_time = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "save_path = f\"./Unity_practice/saved_models/{game}/A2C/{date_time}\"\n",
    "load_path = f\"./Unity_practice/saved_models/{game}/A2C/20240221183157\"\n",
    "\n",
    "# 연산 장치\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# A2C 클래스 -> Actor Network, Critic Network 정의 \n",
    "class A2C(torch.nn.Module):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(A2C, self).__init__(**kwargs)\n",
    "        self.d1 = torch.nn.Linear(state_size, 128)\n",
    "        self.d2 = torch.nn.Linear(128, 128)\n",
    "        self.pi = torch.nn.Linear(128, action_size)\n",
    "        self.v = torch.nn.Linear(128, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.d1(x))\n",
    "        x = F.relu(self.d2(x))\n",
    "        return F.softmax(self.pi(x), dim=1), self.v(x)\n",
    "\n",
    "# A2CAgent 클래스 -> A2C 알고리즘을 위한 다양한 함수 정의 \n",
    "class A2CAgent:\n",
    "    def __init__(self):\n",
    "        self.a2c = A2C().to(device)\n",
    "        self.optimizer = torch.optim.Adam(self.a2c.parameters(), lr=learning_rate)\n",
    "        self.writer = SummaryWriter(save_path)\n",
    "\n",
    "        if load_model == True:\n",
    "            print(f\"... Load Model from {load_path}/ckpt ...\")\n",
    "            checkpoint = torch.load(load_path+'/ckpt', map_location=device)\n",
    "            self.a2c.load_state_dict(checkpoint[\"network\"])\n",
    "            self.optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "\n",
    "    # 정책을 통해 행동 결정 \n",
    "    def get_action(self, state, training=True):\n",
    "        #  네트워크 모드 설정\n",
    "        self.a2c.train(training)\n",
    "\n",
    "        # 네트워크 연산에 따라 행동 결정\n",
    "        pi, _ = self.a2c(torch.FloatTensor(state).to(device))\n",
    "        action = torch.multinomial(pi, num_samples=1).cpu().numpy()\n",
    "        return action\n",
    "\n",
    "    # 학습 수행\n",
    "    def train_model(self, state, action, reward, next_state, done):\n",
    "        state, action, reward, next_state, done = map(lambda x: torch.FloatTensor(x).to(device),\n",
    "                                                        [state, action, reward, next_state, done])\n",
    "        pi, value = self.a2c(state)\n",
    "\n",
    "        #가치신경망\n",
    "        with torch.no_grad():\n",
    "            _, next_value = self.a2c(next_state)\n",
    "            target_value  = reward + (1-done) * discount_factor * next_value\n",
    "        critic_loss = F.mse_loss(target_value, value)\n",
    "\n",
    "        #정책신경망\n",
    "        eye = torch.eye(action_size).to(device)\n",
    "        one_hot_action = eye[action.view(-1).long()]\n",
    "        advantage = (target_value - value).detach()\n",
    "        actor_loss = -(torch.log((one_hot_action * pi).sum(1))*advantage).mean()\n",
    "        total_loss = critic_loss + actor_loss\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        total_loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return actor_loss.item(), critic_loss.item()\n",
    "\n",
    "    # 네트워크 모델 저장\n",
    "    def save_model(self):\n",
    "        print(f\"... Save Model to {save_path}/ckpt ...\")\n",
    "        torch.save({\n",
    "            \"network\" : self.a2c.state_dict(),\n",
    "            \"optimizer\" : self.optimizer.state_dict(),\n",
    "        }, save_path+'/ckpt')\n",
    "\n",
    "        # 학습 기록 \n",
    "    def write_summray(self, score, actor_loss, critic_loss, step):\n",
    "        self.writer.add_scalar(\"run/score\", score, step)\n",
    "        self.writer.add_scalar(\"model/actor_loss\", actor_loss, step)\n",
    "        self.writer.add_scalar(\"model/critic_loss\", critic_loss, step)\n",
    "\n",
    "# Main 함수 -> 전체적으로 A2C 알고리즘을 진행 \n",
    "if __name__ == '__main__':\n",
    "    # 유니티 환경 경로 설정 (file_name)\n",
    "    engine_configuration_channel = EngineConfigurationChannel()\n",
    "    env = UnityEnvironment(file_name=env_name,\n",
    "                           side_channels=[engine_configuration_channel],\n",
    "                           worker_id=103)\n",
    "    env.reset()\n",
    "\n",
    "    # 유니티 브레인 설정 \n",
    "    behavior_name = list(env.behavior_specs.keys())[0]\n",
    "    spec = env.behavior_specs[behavior_name]\n",
    "    engine_configuration_channel.set_configuration_parameters(time_scale=12.0)\n",
    "    dec, term = env.get_steps(behavior_name)\n",
    "\n",
    "    # A2C 클래스를 agent로 정의 \n",
    "    agent = A2CAgent()\n",
    "    actor_losses, critic_losses, scores, episode, score = [], [], [], 0, 0\n",
    "    for step in range(run_step + test_step):\n",
    "        if step == run_step:\n",
    "            if train_mode:\n",
    "                agent.save_model()\n",
    "            print(\"TEST START\")\n",
    "            train_mode = False\n",
    "            engine_configuration_channel.set_configuration_parameters(time_scale=1.0)\n",
    "\n",
    "        preprocess = lambda obs, goal: np.concatenate((obs*goal[0][0], obs*goal[0][1]), axis=-1) \n",
    "        state = preprocess(dec.obs[OBS],dec.obs[GOAL_OBS])\n",
    "        action = agent.get_action(state, train_mode)\n",
    "        real_action = action + 1\n",
    "        action_tuple = ActionTuple()\n",
    "        action_tuple.add_discrete(real_action)\n",
    "        env.set_actions(behavior_name, action_tuple)\n",
    "        env.step()\n",
    "\n",
    "        #환경으로부터 얻는 정보\n",
    "        dec, term = env.get_steps(behavior_name)\n",
    "        done = len(term.agent_id) > 0\n",
    "        reward = term.reward if done else dec.reward\n",
    "        next_state = preprocess(term.obs[OBS], term.obs[GOAL_OBS]) if done\\\n",
    "                     else preprocess(dec.obs[OBS], dec.obs[GOAL_OBS])\n",
    "        score += reward[0]\n",
    "\n",
    "        if train_mode:\n",
    "            #학습수행\n",
    "            actor_loss, critic_loss = agent.train_model(state, action[0], [reward], next_state, [done])\n",
    "            actor_losses.append(actor_loss)\n",
    "            critic_losses.append(critic_loss)\n",
    "\n",
    "        if done:\n",
    "            episode +=1\n",
    "            scores.append(score)\n",
    "            score = 0\n",
    "\n",
    "          # 게임 진행 상황 출력 및 텐서 보드에 보상과 손실함수 값 기록 \n",
    "            if episode % print_interval == 0:\n",
    "                mean_score = np.mean(scores)\n",
    "                mean_actor_loss = np.mean(actor_losses) if len(actor_losses) > 0 else 0\n",
    "                mean_critic_loss = np.mean(critic_losses)  if len(critic_losses) > 0 else 0\n",
    "                agent.write_summray(mean_score, mean_actor_loss, mean_critic_loss, step)\n",
    "                actor_losses, critic_losses, scores = [], [], []\n",
    "\n",
    "                print(f\"{episode} Episode / Step: {step} / Score: {mean_score:.2f} / \" +\\\n",
    "                      f\"Actor loss: {mean_actor_loss:.2f} / Critic loss: {mean_critic_loss:.4f}\")\n",
    "\n",
    "            # 네트워크 모델 저장 \n",
    "            if train_mode and episode % save_interval == 0:\n",
    "                agent.save_model()\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDPG Training\n",
    "======"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeongminlee/miniforge3/envs/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/jeongminlee/miniforge3/envs/py37/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Episode / Step: 1929 / Score: -8.71 / Actor loss: nan / Critic loss: nan\n",
      "20 Episode / Step: 3985 / Score: -7.99 / Actor loss: nan / Critic loss: nan\n",
      "30 Episode / Step: 5263 / Score: -7.89 / Actor loss: 0.09 / Critic loss: 0.0022\n",
      "40 Episode / Step: 5772 / Score: -6.63 / Actor loss: 0.07 / Critic loss: 0.0028\n",
      "50 Episode / Step: 6311 / Score: -6.68 / Actor loss: 0.08 / Critic loss: 0.0024\n",
      "60 Episode / Step: 6803 / Score: -6.98 / Actor loss: 0.10 / Critic loss: 0.0022\n",
      "70 Episode / Step: 7287 / Score: -7.07 / Actor loss: 0.14 / Critic loss: 0.0018\n",
      "80 Episode / Step: 7795 / Score: -7.30 / Actor loss: 0.18 / Critic loss: 0.0012\n",
      "90 Episode / Step: 8322 / Score: -6.34 / Actor loss: 0.22 / Critic loss: 0.0010\n",
      "100 Episode / Step: 8783 / Score: -6.36 / Actor loss: 0.25 / Critic loss: 0.0009\n",
      "... Save Model to ./Unity_practice/saved_models/Drone/DDPG/20240222153829/ckpt ...\n",
      "110 Episode / Step: 9255 / Score: -6.40 / Actor loss: 0.29 / Critic loss: 0.0009\n",
      "120 Episode / Step: 9823 / Score: -7.26 / Actor loss: 0.33 / Critic loss: 0.0007\n",
      "130 Episode / Step: 10263 / Score: -6.97 / Actor loss: 0.37 / Critic loss: 0.0008\n",
      "140 Episode / Step: 10773 / Score: -5.94 / Actor loss: 0.40 / Critic loss: 0.0008\n",
      "150 Episode / Step: 11235 / Score: -5.31 / Actor loss: 0.39 / Critic loss: 0.0009\n",
      "160 Episode / Step: 11715 / Score: -4.89 / Actor loss: 0.38 / Critic loss: 0.0011\n",
      "170 Episode / Step: 12252 / Score: -6.67 / Actor loss: 0.37 / Critic loss: 0.0011\n",
      "180 Episode / Step: 12839 / Score: -6.72 / Actor loss: 0.38 / Critic loss: 0.0009\n",
      "190 Episode / Step: 13313 / Score: -7.16 / Actor loss: 0.38 / Critic loss: 0.0008\n",
      "200 Episode / Step: 14005 / Score: -7.92 / Actor loss: 0.39 / Critic loss: 0.0007\n",
      "... Save Model to ./Unity_practice/saved_models/Drone/DDPG/20240222153829/ckpt ...\n",
      "210 Episode / Step: 14795 / Score: -6.37 / Actor loss: 0.40 / Critic loss: 0.0006\n",
      "220 Episode / Step: 15965 / Score: -2.91 / Actor loss: 0.39 / Critic loss: 0.0005\n",
      "230 Episode / Step: 18418 / Score: -3.47 / Actor loss: 0.33 / Critic loss: 0.0006\n",
      "240 Episode / Step: 20523 / Score: 3.48 / Actor loss: 0.19 / Critic loss: 0.0009\n",
      "250 Episode / Step: 21308 / Score: 4.06 / Actor loss: 0.10 / Critic loss: 0.0009\n",
      "260 Episode / Step: 22440 / Score: 3.62 / Actor loss: 0.05 / Critic loss: 0.0013\n",
      "270 Episode / Step: 23703 / Score: 3.88 / Actor loss: -0.00 / Critic loss: 0.0014\n",
      "280 Episode / Step: 24535 / Score: 4.84 / Actor loss: -0.04 / Critic loss: 0.0014\n",
      "290 Episode / Step: 25050 / Score: 4.78 / Actor loss: -0.08 / Critic loss: 0.0016\n",
      "300 Episode / Step: 25591 / Score: 5.02 / Actor loss: -0.11 / Critic loss: 0.0017\n",
      "... Save Model to ./Unity_practice/saved_models/Drone/DDPG/20240222153829/ckpt ...\n",
      "310 Episode / Step: 26060 / Score: 5.69 / Actor loss: -0.13 / Critic loss: 0.0018\n",
      "320 Episode / Step: 26523 / Score: 4.42 / Actor loss: -0.16 / Critic loss: 0.0019\n",
      "330 Episode / Step: 27054 / Score: 4.58 / Actor loss: -0.19 / Critic loss: 0.0020\n",
      "340 Episode / Step: 27496 / Score: 4.49 / Actor loss: -0.22 / Critic loss: 0.0020\n",
      "350 Episode / Step: 27982 / Score: 4.34 / Actor loss: -0.25 / Critic loss: 0.0020\n",
      "360 Episode / Step: 28598 / Score: 4.79 / Actor loss: -0.28 / Critic loss: 0.0021\n",
      "370 Episode / Step: 29270 / Score: 4.46 / Actor loss: -0.31 / Critic loss: 0.0019\n",
      "380 Episode / Step: 29834 / Score: 4.36 / Actor loss: -0.33 / Critic loss: 0.0021\n",
      "390 Episode / Step: 30413 / Score: 5.32 / Actor loss: -0.36 / Critic loss: 0.0022\n",
      "400 Episode / Step: 30855 / Score: 5.12 / Actor loss: -0.38 / Critic loss: 0.0021\n",
      "... Save Model to ./Unity_practice/saved_models/Drone/DDPG/20240222153829/ckpt ...\n",
      "410 Episode / Step: 31347 / Score: 4.90 / Actor loss: -0.40 / Critic loss: 0.0021\n",
      "420 Episode / Step: 31770 / Score: 4.40 / Actor loss: -0.42 / Critic loss: 0.0021\n",
      "430 Episode / Step: 33003 / Score: 3.90 / Actor loss: -0.44 / Critic loss: 0.0020\n",
      "440 Episode / Step: 33454 / Score: 5.09 / Actor loss: -0.46 / Critic loss: 0.0019\n",
      "450 Episode / Step: 33902 / Score: 5.05 / Actor loss: -0.48 / Critic loss: 0.0019\n",
      "460 Episode / Step: 34334 / Score: 5.36 / Actor loss: -0.49 / Critic loss: 0.0019\n",
      "470 Episode / Step: 34748 / Score: 5.05 / Actor loss: -0.51 / Critic loss: 0.0018\n",
      "480 Episode / Step: 35160 / Score: 5.16 / Actor loss: -0.52 / Critic loss: 0.0017\n",
      "490 Episode / Step: 35564 / Score: 5.31 / Actor loss: -0.54 / Critic loss: 0.0017\n",
      "500 Episode / Step: 35918 / Score: 4.79 / Actor loss: -0.56 / Critic loss: 0.0016\n",
      "... Save Model to ./Unity_practice/saved_models/Drone/DDPG/20240222153829/ckpt ...\n",
      "510 Episode / Step: 36289 / Score: 4.57 / Actor loss: -0.58 / Critic loss: 0.0017\n",
      "520 Episode / Step: 36680 / Score: 5.25 / Actor loss: -0.59 / Critic loss: 0.0015\n",
      "530 Episode / Step: 37090 / Score: 5.47 / Actor loss: -0.61 / Critic loss: 0.0015\n",
      "540 Episode / Step: 37483 / Score: 4.95 / Actor loss: -0.63 / Critic loss: 0.0014\n",
      "550 Episode / Step: 37873 / Score: 4.82 / Actor loss: -0.65 / Critic loss: 0.0016\n",
      "560 Episode / Step: 38260 / Score: 4.73 / Actor loss: -0.67 / Critic loss: 0.0012\n",
      "570 Episode / Step: 38694 / Score: 5.43 / Actor loss: -0.68 / Critic loss: 0.0016\n",
      "580 Episode / Step: 39114 / Score: 5.26 / Actor loss: -0.71 / Critic loss: 0.0013\n",
      "590 Episode / Step: 39527 / Score: 5.25 / Actor loss: -0.72 / Critic loss: 0.0013\n",
      "600 Episode / Step: 39955 / Score: 5.47 / Actor loss: -0.74 / Critic loss: 0.0012\n",
      "... Save Model to ./Unity_practice/saved_models/Drone/DDPG/20240222153829/ckpt ...\n",
      "610 Episode / Step: 40290 / Score: 4.15 / Actor loss: -0.76 / Critic loss: 0.0012\n",
      "620 Episode / Step: 40673 / Score: 5.03 / Actor loss: -0.78 / Critic loss: 0.0013\n",
      "630 Episode / Step: 41010 / Score: 4.18 / Actor loss: -0.79 / Critic loss: 0.0011\n",
      "640 Episode / Step: 41394 / Score: 4.84 / Actor loss: -0.80 / Critic loss: 0.0011\n",
      "650 Episode / Step: 41754 / Score: 4.22 / Actor loss: -0.82 / Critic loss: 0.0011\n",
      "660 Episode / Step: 42140 / Score: 4.66 / Actor loss: -0.83 / Critic loss: 0.0010\n",
      "670 Episode / Step: 42527 / Score: 4.75 / Actor loss: -0.85 / Critic loss: 0.0011\n",
      "680 Episode / Step: 42911 / Score: 5.15 / Actor loss: -0.87 / Critic loss: 0.0011\n",
      "690 Episode / Step: 43258 / Score: 4.38 / Actor loss: -0.88 / Critic loss: 0.0010\n",
      "700 Episode / Step: 43679 / Score: 5.62 / Actor loss: -0.90 / Critic loss: 0.0010\n",
      "... Save Model to ./Unity_practice/saved_models/Drone/DDPG/20240222153829/ckpt ...\n",
      "710 Episode / Step: 44010 / Score: 3.91 / Actor loss: -0.91 / Critic loss: 0.0011\n",
      "720 Episode / Step: 44389 / Score: 4.79 / Actor loss: -0.92 / Critic loss: 0.0009\n",
      "730 Episode / Step: 44692 / Score: 4.29 / Actor loss: -0.93 / Critic loss: 0.0009\n",
      "740 Episode / Step: 45081 / Score: 4.93 / Actor loss: -0.94 / Critic loss: 0.0010\n",
      "750 Episode / Step: 45393 / Score: 4.34 / Actor loss: -0.95 / Critic loss: 0.0009\n",
      "760 Episode / Step: 45749 / Score: 5.01 / Actor loss: -0.96 / Critic loss: 0.0008\n",
      "770 Episode / Step: 46043 / Score: 4.29 / Actor loss: -0.97 / Critic loss: 0.0007\n",
      "780 Episode / Step: 46406 / Score: 4.71 / Actor loss: -0.97 / Critic loss: 0.0009\n",
      "790 Episode / Step: 46770 / Score: 4.67 / Actor loss: -0.98 / Critic loss: 0.0007\n",
      "800 Episode / Step: 47227 / Score: 5.66 / Actor loss: -0.99 / Critic loss: 0.0008\n",
      "... Save Model to ./Unity_practice/saved_models/Drone/DDPG/20240222153829/ckpt ...\n",
      "810 Episode / Step: 47594 / Score: 5.00 / Actor loss: -1.00 / Critic loss: 0.0006\n",
      "820 Episode / Step: 47935 / Score: 4.56 / Actor loss: -1.00 / Critic loss: 0.0006\n",
      "830 Episode / Step: 48341 / Score: 5.26 / Actor loss: -1.01 / Critic loss: 0.0005\n",
      "840 Episode / Step: 48740 / Score: 5.71 / Actor loss: -1.02 / Critic loss: 0.0005\n",
      "850 Episode / Step: 49120 / Score: 5.09 / Actor loss: -1.02 / Critic loss: 0.0005\n",
      "860 Episode / Step: 49500 / Score: 5.24 / Actor loss: -1.03 / Critic loss: 0.0005\n",
      "870 Episode / Step: 49870 / Score: 4.83 / Actor loss: -1.03 / Critic loss: 0.0005\n",
      "... Save Model to ./Unity_practice/saved_models/Drone/DDPG/20240222153829/ckpt ...\n",
      "TEST START\n",
      "880 Episode / Step: 50202 / Score: 4.51 / Actor loss: -1.03 / Critic loss: 0.0004\n",
      "890 Episode / Step: 50600 / Score: 5.22 / Actor loss: nan / Critic loss: nan\n",
      "900 Episode / Step: 50972 / Score: 4.50 / Actor loss: nan / Critic loss: nan\n",
      "910 Episode / Step: 51336 / Score: 5.11 / Actor loss: nan / Critic loss: nan\n",
      "920 Episode / Step: 51725 / Score: 5.30 / Actor loss: nan / Critic loss: nan\n",
      "930 Episode / Step: 52059 / Score: 4.48 / Actor loss: nan / Critic loss: nan\n",
      "940 Episode / Step: 52436 / Score: 4.49 / Actor loss: nan / Critic loss: nan\n",
      "950 Episode / Step: 52765 / Score: 4.46 / Actor loss: nan / Critic loss: nan\n",
      "960 Episode / Step: 53113 / Score: 5.42 / Actor loss: nan / Critic loss: nan\n",
      "970 Episode / Step: 53529 / Score: 5.41 / Actor loss: nan / Critic loss: nan\n",
      "980 Episode / Step: 53899 / Score: 5.02 / Actor loss: nan / Critic loss: nan\n",
      "990 Episode / Step: 54235 / Score: 4.67 / Actor loss: nan / Critic loss: nan\n",
      "1000 Episode / Step: 54504 / Score: 3.94 / Actor loss: nan / Critic loss: nan\n",
      "1010 Episode / Step: 54856 / Score: 5.08 / Actor loss: nan / Critic loss: nan\n",
      "1020 Episode / Step: 55250 / Score: 5.24 / Actor loss: nan / Critic loss: nan\n",
      "1030 Episode / Step: 55632 / Score: 5.49 / Actor loss: nan / Critic loss: nan\n",
      "1040 Episode / Step: 56021 / Score: 5.08 / Actor loss: nan / Critic loss: nan\n",
      "1050 Episode / Step: 56347 / Score: 4.40 / Actor loss: nan / Critic loss: nan\n",
      "1060 Episode / Step: 56716 / Score: 5.34 / Actor loss: nan / Critic loss: nan\n",
      "1070 Episode / Step: 57083 / Score: 4.95 / Actor loss: nan / Critic loss: nan\n",
      "1080 Episode / Step: 57438 / Score: 4.84 / Actor loss: nan / Critic loss: nan\n",
      "1090 Episode / Step: 57857 / Score: 5.91 / Actor loss: nan / Critic loss: nan\n",
      "1100 Episode / Step: 58213 / Score: 4.46 / Actor loss: nan / Critic loss: nan\n",
      "1110 Episode / Step: 58580 / Score: 5.02 / Actor loss: nan / Critic loss: nan\n",
      "1120 Episode / Step: 58949 / Score: 4.80 / Actor loss: nan / Critic loss: nan\n",
      "1130 Episode / Step: 59300 / Score: 4.71 / Actor loss: nan / Critic loss: nan\n",
      "1140 Episode / Step: 59651 / Score: 4.73 / Actor loss: nan / Critic loss: nan\n",
      "1150 Episode / Step: 59991 / Score: 4.41 / Actor loss: nan / Critic loss: nan\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import datetime\n",
    "import platform\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque\n",
    "from mlagents_envs.environment import UnityEnvironment, ActionTuple\n",
    "from mlagents_envs.side_channel.engine_configuration_channel\\\n",
    "                             import EngineConfigurationChannel\n",
    "\n",
    "# DDPG를 위한 파라미터 값 세팅\n",
    "state_size = 9\n",
    "action_size = 3\n",
    "\n",
    "load_model = False\n",
    "train_mode = True\n",
    "\n",
    "batch_size = 128\n",
    "mem_maxlen = 30000\n",
    "discount_factor = 0.9\n",
    "actor_lr = 1e-4\n",
    "critic_lr = 5e-4\n",
    "tau = 1e-3\n",
    "\n",
    "# OU noise 파라미터\n",
    "mu = 0\n",
    "theta = 1e-3\n",
    "sigma = 2e-3\n",
    "\n",
    "run_step = 50000 if train_mode else 0\n",
    "test_step = 10000\n",
    "train_start_step = 5000\n",
    "\n",
    "print_interval = 10\n",
    "save_interval = 100\n",
    "\n",
    "# 유니티 환경 경로\n",
    "game = \"Drone\"\n",
    "env_name = \"./Unity_practice/Drone/Drone\"\n",
    "\n",
    "# 모델 저장 및 불러오기 경로\n",
    "date_time = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "save_path = f\"./Unity_practice/saved_models/{game}/DDPG/{date_time}\"\n",
    "load_path = f\"./Unity_practice/saved_models/{game}/DDPG/20210709235643\"\n",
    "\n",
    "# 연산 장치\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# OU_noise 클래스 -> ou noise 정의 및 파라미터 결정\n",
    "class OU_noise:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.X = np.ones((1, action_size), dtype=np.float32) * mu\n",
    "\n",
    "    def sample(self):\n",
    "        dx = theta * (mu - self.X) + sigma * np.random.randn(len(self.X))\n",
    "        self.X += dx\n",
    "        return self.X\n",
    "\n",
    "# Actor 클래스 -> DDPG Actor 클래스 정의\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_size, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 128)\n",
    "        self.mu = torch.nn.Linear(128, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.tanh(self.mu(x))\n",
    "\n",
    "# Critic 클래스 -> DDPG Critic 클래스 정의\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(state_size, 128)\n",
    "        self.fc2 = torch.nn.Linear(128+action_size, 128)\n",
    "        self.q = torch.nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.cat((x, action), dim=-1)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.q(x)\n",
    "\n",
    "# DDPGAgent 클래스 -> DDPG 알고리즘을 위한 다양한 함수 정의\n",
    "class DDPGAgent():\n",
    "    def __init__(self):\n",
    "        self.actor = Actor().to(device)\n",
    "        self.target_actor = copy.deepcopy(self.actor)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic = Critic().to(device)\n",
    "        self.target_critic = copy.deepcopy(self.critic)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        self.OU = OU_noise()\n",
    "        self.memory = deque(maxlen=mem_maxlen)\n",
    "        self.writer = SummaryWriter(save_path)\n",
    "\n",
    "        if load_model == True:\n",
    "            print(f\"... Load Model from {load_path}/ckpt ...\")\n",
    "            checkpoint = torch.load(load_path+'/ckpt', map_location=device)\n",
    "            self.actor.load_state_dict(checkpoint[\"actor\"])\n",
    "            self.target_actor.load_state_dict(checkpoint[\"actor\"])\n",
    "            self.actor_optimizer.load_state_dict(checkpoint[\"actor_optimizer\"])\n",
    "            self.critic.load_state_dict(checkpoint[\"critic\"])\n",
    "            self.target_critic.load_state_dict(checkpoint[\"critic\"])\n",
    "            self.critic_optimizer.load_state_dict(checkpoint[\"critic_optimizer\"])\n",
    "\n",
    "    # OU noise 기법에 따라 행동 결정\n",
    "    def get_action(self, state, training=True):\n",
    "        #  네트워크 모드 설정\n",
    "        self.actor.train(training)\n",
    "\n",
    "        action = self.actor(torch.FloatTensor(state).to(device)).cpu().detach().numpy()\n",
    "        return action + self.OU.sample() if training else action\n",
    "\n",
    "    # 리플레이 메모리에 데이터 추가 (상태, 행동, 보상, 다음 상태, 게임 종료 여부)\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train_model(self):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        state      = np.stack([b[0] for b in batch], axis=0)\n",
    "        action     = np.stack([b[1] for b in batch], axis=0)\n",
    "        reward     = np.stack([b[2] for b in batch], axis=0)\n",
    "        next_state = np.stack([b[3] for b in batch], axis=0)\n",
    "        done       = np.stack([b[4] for b in batch], axis=0)\n",
    "\n",
    "        state, action, reward, next_state, done = map(lambda x: torch.FloatTensor(x).to(device),\n",
    "                                                        [state, action, reward, next_state, done])\n",
    "\n",
    "        # Critic 업데이트\n",
    "        next_actions = self.target_actor(next_state)\n",
    "        next_q = self.target_critic(next_state, next_actions)\n",
    "        target_q = reward + (1 - done) * discount_factor * next_q\n",
    "        q = self.critic(state, action)\n",
    "        critic_loss = F.mse_loss(target_q, q)\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Actor 업데이트\n",
    "        action_pred = self.actor(state)\n",
    "        actor_loss = -self.critic(state, action_pred).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        return actor_loss.item(), critic_loss.item()\n",
    "\n",
    "    # 소프트 타겟 업데이트를 위한 함수\n",
    "    def soft_update_target(self):\n",
    "        for target_param, local_param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "        for target_param, local_param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "    # 네트워크 모델 저장\n",
    "    def save_model(self):\n",
    "        print(f\"... Save Model to {save_path}/ckpt ...\")\n",
    "        torch.save({\n",
    "            \"actor\" : self.actor.state_dict(),\n",
    "            \"actor_optimizer\" : self.actor_optimizer.state_dict(),\n",
    "            \"critic\" : self.critic.state_dict(),\n",
    "            \"critic_optimizer\" : self.critic_optimizer.state_dict(),\n",
    "        }, save_path+'/ckpt')\n",
    "\n",
    "    # 학습 기록\n",
    "    def write_summray(self, score, actor_loss, critic_loss, step):\n",
    "        self.writer.add_scalar(\"run/score\", score, step)\n",
    "        self.writer.add_scalar(\"model/actor_loss\", actor_loss, step)\n",
    "        self.writer.add_scalar(\"model/critic_loss\", critic_loss, step)\n",
    "\n",
    "\n",
    "# Main 함수 -> 전체적으로 DDPG 알고리즘을 진행\n",
    "if __name__ == '__main__':\n",
    "    # 유니티 환경 경로 설정 (file_name)\n",
    "    engine_configuration_channel = EngineConfigurationChannel()\n",
    "    env = UnityEnvironment(file_name=env_name,\n",
    "                           side_channels=[engine_configuration_channel],\n",
    "                           worker_id=107)\n",
    "    env.reset()\n",
    "\n",
    "    # 유니티 브레인 설정\n",
    "    behavior_name = list(env.behavior_specs.keys())[0]\n",
    "    spec = env.behavior_specs[behavior_name]\n",
    "    engine_configuration_channel.set_configuration_parameters(time_scale=12.0)\n",
    "    dec, term = env.get_steps(behavior_name)\n",
    "\n",
    "    # DDPGAgent 클래스를 agent로 정의\n",
    "    agent = DDPGAgent()\n",
    "\n",
    "    actor_losses, critic_losses, scores, episode, score = [], [], [], 0, 0\n",
    "    for step in range(run_step + test_step):\n",
    "        if step == run_step:\n",
    "            if train_mode:\n",
    "                agent.save_model()\n",
    "            print(\"TEST START\")\n",
    "            train_mode = False\n",
    "            engine_configuration_channel.set_configuration_parameters(time_scale=1.0)\n",
    "\n",
    "        state = dec.obs[0]\n",
    "        action = agent.get_action(state, train_mode)\n",
    "        action_tuple = ActionTuple()\n",
    "        action_tuple.add_continuous(action)\n",
    "        env.set_actions(behavior_name, action_tuple)\n",
    "        env.step()\n",
    "\n",
    "        dec, term = env.get_steps(behavior_name)\n",
    "        done = len(term.agent_id) > 0\n",
    "        reward = term.reward if done else dec.reward\n",
    "        next_state = term.obs[0] if done else dec.obs[0]\n",
    "        score += reward[0]\n",
    "\n",
    "        if train_mode:\n",
    "            agent.append_sample(state[0], action[0], reward, next_state[0], [done])\n",
    "\n",
    "        if train_mode and step > max(batch_size, train_start_step):\n",
    "            # 학습 수행\n",
    "            actor_loss, critic_loss = agent.train_model()\n",
    "            actor_losses.append(actor_loss)\n",
    "            critic_losses.append(critic_loss)\n",
    "\n",
    "            # 타겟 네트워크 소프트 업데이트\n",
    "            agent.soft_update_target()\n",
    "\n",
    "        if done:\n",
    "            episode += 1\n",
    "            scores.append(score)\n",
    "            score = 0\n",
    "\n",
    "            # 게임 진행 상황 출력 및 텐서 보드에 보상과 손실함수 값 기록\n",
    "            if episode % print_interval == 0:\n",
    "                mean_score = np.mean(scores)\n",
    "                mean_actor_loss = np.mean(actor_losses)\n",
    "                mean_critic_loss = np.mean(critic_losses)\n",
    "                agent.write_summray(mean_score, mean_actor_loss, mean_critic_loss, step)\n",
    "                actor_losses, critic_losses, scores = [], [], []\n",
    "\n",
    "                print(f\"{episode} Episode / Step: {step} / Score: {mean_score:.2f} / \" +\\\n",
    "                      f\"Actor loss: {mean_actor_loss:.2f} / Critic loss: {mean_critic_loss:.4f}\")\n",
    "\n",
    "            # 네트워크 모델 저장\n",
    "            if train_mode and episode % save_interval == 0:\n",
    "                agent.save_model()\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DDPG Testing\n",
    "======="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Load Model from ./Unity_practice/saved_models/Drone/DDPG/20240222153829/ckpt ...\n",
      "TEST START\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jeongminlee/miniforge3/envs/py37/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3441: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/jeongminlee/miniforge3/envs/py37/lib/python3.7/site-packages/numpy/core/_methods.py:189: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 Episode / Step: 296 / Score: 4.33 / Actor loss: nan / Critic loss: nan\n",
      "20 Episode / Step: 669 / Score: 5.12 / Actor loss: nan / Critic loss: nan\n",
      "30 Episode / Step: 1002 / Score: 4.43 / Actor loss: nan / Critic loss: nan\n",
      "40 Episode / Step: 1350 / Score: 4.88 / Actor loss: nan / Critic loss: nan\n",
      "50 Episode / Step: 1713 / Score: 4.82 / Actor loss: nan / Critic loss: nan\n",
      "60 Episode / Step: 2041 / Score: 4.52 / Actor loss: nan / Critic loss: nan\n",
      "70 Episode / Step: 2380 / Score: 4.43 / Actor loss: nan / Critic loss: nan\n",
      "80 Episode / Step: 2712 / Score: 4.27 / Actor loss: nan / Critic loss: nan\n",
      "90 Episode / Step: 3109 / Score: 5.15 / Actor loss: nan / Critic loss: nan\n",
      "100 Episode / Step: 3503 / Score: 5.01 / Actor loss: nan / Critic loss: nan\n",
      "110 Episode / Step: 3880 / Score: 5.06 / Actor loss: nan / Critic loss: nan\n",
      "120 Episode / Step: 4202 / Score: 4.33 / Actor loss: nan / Critic loss: nan\n",
      "130 Episode / Step: 4551 / Score: 4.48 / Actor loss: nan / Critic loss: nan\n",
      "140 Episode / Step: 4926 / Score: 5.58 / Actor loss: nan / Critic loss: nan\n",
      "150 Episode / Step: 5262 / Score: 5.01 / Actor loss: nan / Critic loss: nan\n",
      "160 Episode / Step: 5657 / Score: 5.39 / Actor loss: nan / Critic loss: nan\n",
      "170 Episode / Step: 6040 / Score: 4.83 / Actor loss: nan / Critic loss: nan\n",
      "180 Episode / Step: 6413 / Score: 4.88 / Actor loss: nan / Critic loss: nan\n",
      "190 Episode / Step: 6739 / Score: 4.37 / Actor loss: nan / Critic loss: nan\n",
      "200 Episode / Step: 7027 / Score: 3.84 / Actor loss: nan / Critic loss: nan\n",
      "210 Episode / Step: 7429 / Score: 5.42 / Actor loss: nan / Critic loss: nan\n",
      "220 Episode / Step: 7766 / Score: 4.75 / Actor loss: nan / Critic loss: nan\n",
      "230 Episode / Step: 8113 / Score: 4.34 / Actor loss: nan / Critic loss: nan\n",
      "240 Episode / Step: 8479 / Score: 5.19 / Actor loss: nan / Critic loss: nan\n",
      "250 Episode / Step: 8794 / Score: 4.50 / Actor loss: nan / Critic loss: nan\n",
      "260 Episode / Step: 9121 / Score: 4.39 / Actor loss: nan / Critic loss: nan\n",
      "270 Episode / Step: 9493 / Score: 4.75 / Actor loss: nan / Critic loss: nan\n",
      "280 Episode / Step: 9883 / Score: 5.25 / Actor loss: nan / Critic loss: nan\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import datetime\n",
    "import platform\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import deque\n",
    "from mlagents_envs.environment import UnityEnvironment, ActionTuple\n",
    "from mlagents_envs.side_channel.engine_configuration_channel\\\n",
    "                             import EngineConfigurationChannel\n",
    "\n",
    "# DDPG를 위한 파라미터 값 세팅\n",
    "state_size = 9\n",
    "action_size = 3\n",
    "\n",
    "load_model = True\n",
    "train_mode = False\n",
    "\n",
    "batch_size = 128\n",
    "mem_maxlen = 30000\n",
    "discount_factor = 0.9\n",
    "actor_lr = 1e-4\n",
    "critic_lr = 5e-4\n",
    "tau = 1e-3\n",
    "\n",
    "# OU noise 파라미터\n",
    "mu = 0\n",
    "theta = 1e-3\n",
    "sigma = 2e-3\n",
    "\n",
    "run_step = 50000 if train_mode else 0\n",
    "test_step = 10000\n",
    "train_start_step = 5000\n",
    "\n",
    "print_interval = 10\n",
    "save_interval = 100\n",
    "\n",
    "# 유니티 환경 경로\n",
    "game = \"Drone\"\n",
    "env_name = \"./Unity_practice/Drone/Drone\"\n",
    "\n",
    "# 모델 저장 및 불러오기 경로\n",
    "date_time = datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\")\n",
    "save_path = f\"./Unity_practice/saved_models/{game}/DDPG/{date_time}\"\n",
    "load_path = f\"./Unity_practice/saved_models/{game}/DDPG/20240222153829\"\n",
    "\n",
    "# 연산 장치\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# OU_noise 클래스 -> ou noise 정의 및 파라미터 결정\n",
    "class OU_noise:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.X = np.ones((1, action_size), dtype=np.float32) * mu\n",
    "\n",
    "    def sample(self):\n",
    "        dx = theta * (mu - self.X) + sigma * np.random.randn(len(self.X))\n",
    "        self.X += dx\n",
    "        return self.X\n",
    "\n",
    "# Actor 클래스 -> DDPG Actor 클래스 정의\n",
    "class Actor(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Actor, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(state_size, 128)\n",
    "        self.fc2 = torch.nn.Linear(128, 128)\n",
    "        self.mu = torch.nn.Linear(128, action_size)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.tanh(self.mu(x))\n",
    "\n",
    "# Critic 클래스 -> DDPG Critic 클래스 정의\n",
    "class Critic(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Critic, self).__init__()\n",
    "\n",
    "        self.fc1 = torch.nn.Linear(state_size, 128)\n",
    "        self.fc2 = torch.nn.Linear(128+action_size, 128)\n",
    "        self.q = torch.nn.Linear(128, 1)\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.cat((x, action), dim=-1)\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.q(x)\n",
    "\n",
    "# DDPGAgent 클래스 -> DDPG 알고리즘을 위한 다양한 함수 정의\n",
    "class DDPGAgent():\n",
    "    def __init__(self):\n",
    "        self.actor = Actor().to(device)\n",
    "        self.target_actor = copy.deepcopy(self.actor)\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=actor_lr)\n",
    "        self.critic = Critic().to(device)\n",
    "        self.target_critic = copy.deepcopy(self.critic)\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=critic_lr)\n",
    "        self.OU = OU_noise()\n",
    "        self.memory = deque(maxlen=mem_maxlen)\n",
    "        self.writer = SummaryWriter(save_path)\n",
    "\n",
    "        if load_model == True:\n",
    "            print(f\"... Load Model from {load_path}/ckpt ...\")\n",
    "            checkpoint = torch.load(load_path+'/ckpt', map_location=device)\n",
    "            self.actor.load_state_dict(checkpoint[\"actor\"])\n",
    "            self.target_actor.load_state_dict(checkpoint[\"actor\"])\n",
    "            self.actor_optimizer.load_state_dict(checkpoint[\"actor_optimizer\"])\n",
    "            self.critic.load_state_dict(checkpoint[\"critic\"])\n",
    "            self.target_critic.load_state_dict(checkpoint[\"critic\"])\n",
    "            self.critic_optimizer.load_state_dict(checkpoint[\"critic_optimizer\"])\n",
    "\n",
    "    # OU noise 기법에 따라 행동 결정\n",
    "    def get_action(self, state, training=True):\n",
    "        #  네트워크 모드 설정\n",
    "        self.actor.train(training)\n",
    "\n",
    "        action = self.actor(torch.FloatTensor(state).to(device)).cpu().detach().numpy()\n",
    "        return action + self.OU.sample() if training else action\n",
    "\n",
    "    # 리플레이 메모리에 데이터 추가 (상태, 행동, 보상, 다음 상태, 게임 종료 여부)\n",
    "    def append_sample(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def train_model(self):\n",
    "        batch = random.sample(self.memory, batch_size)\n",
    "        state      = np.stack([b[0] for b in batch], axis=0)\n",
    "        action     = np.stack([b[1] for b in batch], axis=0)\n",
    "        reward     = np.stack([b[2] for b in batch], axis=0)\n",
    "        next_state = np.stack([b[3] for b in batch], axis=0)\n",
    "        done       = np.stack([b[4] for b in batch], axis=0)\n",
    "\n",
    "        state, action, reward, next_state, done = map(lambda x: torch.FloatTensor(x).to(device),\n",
    "                                                        [state, action, reward, next_state, done])\n",
    "\n",
    "        # Critic 업데이트\n",
    "        next_actions = self.target_actor(next_state)\n",
    "        next_q = self.target_critic(next_state, next_actions)\n",
    "        target_q = reward + (1 - done) * discount_factor * next_q\n",
    "        q = self.critic(state, action)\n",
    "        critic_loss = F.mse_loss(target_q, q)\n",
    "\n",
    "        self.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        self.critic_optimizer.step()\n",
    "\n",
    "        # Actor 업데이트\n",
    "        action_pred = self.actor(state)\n",
    "        actor_loss = -self.critic(state, action_pred).mean()\n",
    "\n",
    "        self.actor_optimizer.zero_grad()\n",
    "        actor_loss.backward()\n",
    "        self.actor_optimizer.step()\n",
    "\n",
    "        return actor_loss.item(), critic_loss.item()\n",
    "\n",
    "    # 소프트 타겟 업데이트를 위한 함수\n",
    "    def soft_update_target(self):\n",
    "        for target_param, local_param in zip(self.target_actor.parameters(), self.actor.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "        for target_param, local_param in zip(self.target_critic.parameters(), self.critic.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "\n",
    "    # 네트워크 모델 저장\n",
    "    def save_model(self):\n",
    "        print(f\"... Save Model to {save_path}/ckpt ...\")\n",
    "        torch.save({\n",
    "            \"actor\" : self.actor.state_dict(),\n",
    "            \"actor_optimizer\" : self.actor_optimizer.state_dict(),\n",
    "            \"critic\" : self.critic.state_dict(),\n",
    "            \"critic_optimizer\" : self.critic_optimizer.state_dict(),\n",
    "        }, save_path+'/ckpt')\n",
    "\n",
    "    # 학습 기록\n",
    "    def write_summray(self, score, actor_loss, critic_loss, step):\n",
    "        self.writer.add_scalar(\"run/score\", score, step)\n",
    "        self.writer.add_scalar(\"model/actor_loss\", actor_loss, step)\n",
    "        self.writer.add_scalar(\"model/critic_loss\", critic_loss, step)\n",
    "\n",
    "\n",
    "# Main 함수 -> 전체적으로 DDPG 알고리즘을 진행\n",
    "if __name__ == '__main__':\n",
    "    # 유니티 환경 경로 설정 (file_name)\n",
    "    engine_configuration_channel = EngineConfigurationChannel()\n",
    "    env = UnityEnvironment(file_name=env_name,\n",
    "                           side_channels=[engine_configuration_channel],\n",
    "                           worker_id=107)\n",
    "    env.reset()\n",
    "\n",
    "    # 유니티 브레인 설정\n",
    "    behavior_name = list(env.behavior_specs.keys())[0]\n",
    "    spec = env.behavior_specs[behavior_name]\n",
    "    engine_configuration_channel.set_configuration_parameters(time_scale=12.0)\n",
    "    dec, term = env.get_steps(behavior_name)\n",
    "\n",
    "    # DDPGAgent 클래스를 agent로 정의\n",
    "    agent = DDPGAgent()\n",
    "\n",
    "    actor_losses, critic_losses, scores, episode, score = [], [], [], 0, 0\n",
    "    for step in range(run_step + test_step):\n",
    "        if step == run_step:\n",
    "            if train_mode:\n",
    "                agent.save_model()\n",
    "            print(\"TEST START\")\n",
    "            train_mode = False\n",
    "            engine_configuration_channel.set_configuration_parameters(time_scale=1.0)\n",
    "\n",
    "        state = dec.obs[0]\n",
    "        action = agent.get_action(state, train_mode)\n",
    "        action_tuple = ActionTuple()\n",
    "        action_tuple.add_continuous(action)\n",
    "        env.set_actions(behavior_name, action_tuple)\n",
    "        env.step()\n",
    "\n",
    "        dec, term = env.get_steps(behavior_name)\n",
    "        done = len(term.agent_id) > 0\n",
    "        reward = term.reward if done else dec.reward\n",
    "        next_state = term.obs[0] if done else dec.obs[0]\n",
    "        score += reward[0]\n",
    "\n",
    "        if train_mode:\n",
    "            agent.append_sample(state[0], action[0], reward, next_state[0], [done])\n",
    "\n",
    "        if train_mode and step > max(batch_size, train_start_step):\n",
    "            # 학습 수행\n",
    "            actor_loss, critic_loss = agent.train_model()\n",
    "            actor_losses.append(actor_loss)\n",
    "            critic_losses.append(critic_loss)\n",
    "\n",
    "            # 타겟 네트워크 소프트 업데이트\n",
    "            agent.soft_update_target()\n",
    "\n",
    "        if done:\n",
    "            episode += 1\n",
    "            scores.append(score)\n",
    "            score = 0\n",
    "\n",
    "            # 게임 진행 상황 출력 및 텐서 보드에 보상과 손실함수 값 기록\n",
    "            if episode % print_interval == 0:\n",
    "                mean_score = np.mean(scores)\n",
    "                mean_actor_loss = np.mean(actor_losses)\n",
    "                mean_critic_loss = np.mean(critic_losses)\n",
    "                agent.write_summray(mean_score, mean_actor_loss, mean_critic_loss, step)\n",
    "                actor_losses, critic_losses, scores = [], [], []\n",
    "\n",
    "                print(f\"{episode} Episode / Step: {step} / Score: {mean_score:.2f} / \" +\\\n",
    "                      f\"Actor loss: {mean_actor_loss:.2f} / Critic loss: {mean_critic_loss:.4f}\")\n",
    "\n",
    "            # 네트워크 모델 저장\n",
    "            if train_mode and episode % save_interval == 0:\n",
    "                agent.save_model()\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
